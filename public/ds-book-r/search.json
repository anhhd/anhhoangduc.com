[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Khoa học dữ liệu với R",
    "section": "",
    "text": "1 Khoa học dữ liệu và nghề phân tích dữ liệu\nKhoa học dữ liệu là nhóm ngành ứng dụng việc tổ chức và khai thác dữ liệu để hỗ trợ hoạt động ra quyết định. Phạm vi của khoa học dữ liệu rất rộng, từ việc tổ chức, quản lý, quản trị dữ liệu cho đến khai thác dữ liệu dưới dạng các báo cáo đơn giản cho đến các ứng dụng của Machine Learning, AI.\nTrong quá trình sử dụng, dữ liệu được đi qua nhiều bước trong vòng đời dữ liệu, bao gồm:\nTrong các bước trên, việc phân tích dữ liệu (hay còn gọi là phân tích dữ liệu) được nhấn mạnh và chú trọng nhiều nhất trong vòng đời của dữ liệu.\nDo đó, yếu tố cốt tủy và nghệ thuật phân tích dữ liệu là đặt ra câu hỏi và biết cách trả lời câu hỏi một cách khoa học. Với cách tiếp cận như trên, dù là một báo cáo (report) thông thường cũng ẩn tàng các hypothesis mà người đưa ra yêu cầu muốn giải quyết.\nVới sự phát triển của ngành, vai trò phân tích dữ liệu ngày càng được mở rộng và là vùng giao thoa giữa vị trí báo cáo phân tích truyền thống (Business Intelligence) và khoa học dữ liệu (Data Science)."
  },
  {
    "objectID": "index.html#khoa-học-dữ-liệu",
    "href": "index.html#khoa-học-dữ-liệu",
    "title": "Khoa học dữ liệu với R",
    "section": "1.1 Khoa học dữ liệu",
    "text": "1.1 Khoa học dữ liệu\nKhoa học dữ liệu là nhóm ngành ứng dụng việc tổ chức và khai thác dữ liệu để hỗ trợ hoạt động ra quyết định. Phạm vi của khoa học dữ liệu rất rộng, từ việc tổ chức, quản lý, quản trị dữ liệu cho đến khai thác dữ liệu dưới dạng các báo cáo đơn giản cho đến các ứng dụng của Machine Learning, AI.\n\n§\n\nTrong quá trình sử dụng, dữ liệu được đi qua nhiều bước trong vòng đời dữ liệu, bao gồm:\n\nThu thập (Capture): Thu thập dữ liệu từ các hệ thống nguồn\nLưu trữ (Store): Lưu trữ dữ liệu được thu thập\nXử lý (Process): Xử lý, làm sạch dữ liệu\nChia sẻ (Share): Tích hợp dữ liệu từ các nguồn, chia sẻ dưới dạng API, etc.\nSử dụng (Use): Báo cáo, phân tích\nNgừng sử dụng (End of life): Đưa vào vùng đóng băng, hạn chế sử dụng\n\nTrong các bước trên, việc sử dụng dữ liệu (hay còn gọi là phân tích dữ liệu) được nhấn mạnh và chú trọng nhiều nhất trong vòng đời của dữ liệu.\n\nPhân tích dữ liệu là việc chứng minh các giả thuyết (hypothesis) là đúng hoặc sai dựa trên cở sở sử dụng dữ liệu."
  },
  {
    "objectID": "index.html#tại-sao-phân-tích-dữ-liệu-là-nghề-khó",
    "href": "index.html#tại-sao-phân-tích-dữ-liệu-là-nghề-khó",
    "title": "Khoa học dữ liệu với R",
    "section": "1.1 Tại sao phân tích dữ liệu là nghề khó?",
    "text": "1.1 Tại sao phân tích dữ liệu là nghề khó?\nTheo quan sát cá nhân, phần lớn, các bạn làm trong ngành phân tích dữ liệu rơi vào một trong hai nhóm sau:\n\nNhóm một, chưa có nền tảng về phân tích thống kê. Ở nhóm này, các bạn thường không được đào tạo bài bản hoặc không có đủ điều kiện (phần lớn là về thời gian) để học các kiến thức về phân tích thống kê. Do nhu cầu của công việc, các bạn có thành thạo các kỹ năng về xây dựng báo cáo và phân tích khám phá dữ liệu đơn giản với SQL và Excel. Các bạn này thường thuộc các nhóm phân tích báo cáo (Business Intelligence) tại các tổ chức lớn hoặc làm trong startup. Điểm mạnh của nhóm này là làm việc sát với các bộ phận kinh doanh, hiểu rõ nghiệp vụ và nhu cầu nghiệp vụ. Tuy nhiên, điểm yếu của các bạn lại là không thể ứng dụng hoặc tự học và không biết cách triển khai các ứng dụng của khoa học dữ liệu ở mức độ cao vào công việc thực tế.\nNhóm hai, có nền tảng vững vàng về kiến thức thống kê, dự báo nhưng lại quá chú trọng vào các yếu tố kỹ thuật. Ở nhóm này, các bạn đều có nền tảng kiến thức về toán, thống kê rất tốt. Một số bạn được học và đào tạo cơ bản về khoa học dữ liệu, học máy và các ứng dụng của khoa học dữ liệu. Các bạn này có thiên hướng thích xây dựng mô hình dự báo, thích làm các bài toán lớn trong kinh doanh. Điểm mạnh của nhóm này là rất thông minh, chịu khó học hỏi và có thể áp dụng những kỹ thuật phân tích mới vào thực tế một cách nhanh chóng. Nhưng ngược lại, nhóm này lại có nhược điểm chết người là có thói quen chỉ tập trung vào việc phân tích dữ liệu mà thiếu đi cái nhìn tổng quát trong việc giải quyết bài toán thực tế. Không chỉ thế, nhóm này không có thế mạnh trong việc trình bày và giao tiếp, dẫn đến các kết quả thực tế không được các đơn vị kinh doanh nghiệp vụ đón nhận và sử dụng.\n\nĐối với một tổ chức muốn phát triển dựa vào dữ liệu và muốn biến các quyết định của tổ chức dựa vào phân tích dữ liệu, cả hai nhóm trên đều là các trạng thái nên tránh và phải cân bằng được cả hai. Cuốn sách này sẽ phân tích và giúp các bạn làm trong lĩnh vực phân tích dữ liệu hiểu rõ hơn các ưu nhược điểm của chính mình.\n\n§\n\nPhân tích dữ liệu đòi hỏi cùng lúc thực hiện ba nhóm công việc sau:\n\nHiểu biết về vấn đề kinh doanh. Nghe có vẻ đơn giản nhưng qua quá trình làm việc thực tế, đây có lẽ là phần dễ bị bỏ qua nhất bỏi lẽ mấy nguyên nhân sau.\n\nSự khác biệt của vận hành kinh doanh so với hoạt động phân tích dữ liệu.\nTư duy và thái độ của nhóm phân tích dữ liệu: Các bạn phân tích kinh doanh (hoặc đôi khi được gọi là phân tích kinh doanh) thường tự coi mình là đơn vị hỗ trợ và cung cấp dữ liệu theo yêu cầu. Với lối tư duy thụ động này, các bạn sẽ không có nhu cầu tìm hiểu cặn kẽ các hoạt động kinh doanh, dẫn đến không hiểu hoạt động kinh doanh đủ sâu để có thể tư vấn và thuyết phục các bên kinh doanh trong việc triển khai các dự án phân tích dữ liệu mới.\nNghiệp vụ kinh doanh rất phức tạp và thiếu hệ thống tài liệu ghi chép dưới góc độ khái quát cho hoạt động phân tích dữ liệu. Đây là vấn đề phần lớn các nhóm phân tích dữ liệu gặp phải. Để giải quyết vấn đề này, trong phần sau tác giả sẽ đưa ra phương pháp tìm hiểu hoạt động kinh doanh theo 6 nhóm vấn đề.\n\nKhai thác và phân tích dữ liệu: Đây là quá trình đòi hỏi phải có sự đầu tư phù hợp về thời gian đào tạo. Với nhóm có nền tảng hiểu biết về hoạt động kinh doanh tốt lại không được đào tạo hoặc không có thời gian để trau dồi thêm các kiến thức cơ bản về phân tích khám phá dữ liệu.\nTrình bày, thuyết phục và tư vấn cho các bên kinh doanh về kết quả phân tích dữ liệu, đòi hỏi kỹ năng kể chuyện (story telling) dựa trên số liệu.\n\n\n§\n\nKhi ứng dụng phân tích dữ liệu trong hoạt động kinh doanh, giá trị của phân tích chỉ là số 0 - sự phối hợp giữa kinh doanh và phân tích dữ liệu có thể so sánh tương ứng như số 1 và số 0 như sau:\n\n0-1 - Số không đứng trước, số 1 đứng sau. Trong trường hợp này, đội phân tích dữ liệu thường dễ bị tự mãn, cho rằng các phân tích mình đưa ra phải được đơn vị kinh doanh triển khai và sử dụng. Do đó, kết quả phân tích thường không được sử dụng do sự thiếu hiểu biết về hoạt động kinh doanh, thiếu thuyết phục trong quá trình triển khai. Trong trường hợp này, giá trị của phân tích dữ liệu hoàn toàn chỉ là số 0.\n\\(1^0\\) - Số 1 đứng dưới, số 0 đứng trên. Trong trường hợp này, đơn vị kinh doanh quá đề cao giá trị và quá dựa vào hoạt động phân tích số liệu. Điều này sẽ dẫn đến sự thất vọng nếu các dự án phân tích số liệu triển khai chậm, không đưa ra được kết quả như kỳ vọng (dù có rất nhiều điều phân tích số liệu không thể làm được). Trong trường hợp này, giá trị của phân tích cũng chỉ là số 0.\n1-0 - Số 1 đứng trước, số 0 đứng sau. Trong trường hợp này, phân tích dữ liệu đứng ngang hàng và đi sau hoạt động kinh doanh. Trong trường hợp này, sự phối hợp giữa kinh doanh và phân tích rất nhuần nhuyễn và khéo léo - tư duy của phân tích số liệu được triển khai đơn giản nhưng mạch lạc, sắc sảo và khiêm tốn trong quá trình thúc đẩy kinh doanh. Trong trường hợp này, giá trị của phân tích dữ liệu có thể giúp giá trị của kinh doanh tăng lên 10 lần.\n\nNhư vậy, trong 3 trường hợp trên, chỉ có 1 trường hợp duy nhất phân tích dữ liệu có thể đem lại giá trị cho kinh doanh thực tiễn. Các tổ chức lớn gặp khó khăn trong việc biến ứng dụng phân tích số liệu vào thực tiễn do đặt sai vị trí của số 0 (phân tích số liệu) trong cả tổ chức.\n\n§\n\nBa cấp độ của viết code: Trong quá trình làm việc trong lĩnh vực phân tích dữ liệu, chắc chắn chúng ta sẽ phải học và sử dụng các ngôn ngữ truy vấn/phân tích dữ liệu và sẽ trải qua 3 cấp độ như sau:\n\nViết thứ đơn giản. Ở cấp độ này, các bạn thường mới nhập môn phân tích dữ liệu, đầy lo lắng và thiếu tự tin ở bản thân và bắt đầu với những bài phân tích đơn giản để áp dụng các kiến thức mới học.\nViết càng nguy hiểm càng tốt. Ở giai đoạn này, các bạn đã có một lượng kiến thức nền tương đối vững và bắt đầu đi vào các phương pháp nâng cao. Do đó, các bạn thường có xu hướng khiến mọi thứ trở nên nguy hiểm, tô vẽ và đưa ra nhiều yếu tố không thực sự cần thiết. Với các bạn có xu hướng trực quan hóa, sẽ là đưa ra các biểu đồ đầy màu sắc và cực kỳ nguy hiểm. Với các bạn có xu hướng xây dựng mô hình dự báo, sẽ là dùng mô hình dự báo, học máy hoặc deep-learning mọi lúc, mọi nơi. Kết quả sẽ khiến người đọc ấn tượng nhưng có thể chưa thực sự có tính ứng dụng cao.\nChỉ viết những thứ có khả năng tái sử dụng, giải quyết vấn đề bằng phương pháp đơn giản nhất có thể có. Ở giai đoạn này, các bạn đã dung hòa được rất nhiều kiến thức của nghành phân tích dữ liệu với nhau và tiếp cận các vấn đề một cách mạch lạc, logic và rất chặt chẽ. Các bạn nắm rất vững khi nào nên dùng các phương pháp phân tích khám phá dữ liệu đơn giản thay cho các thuật toán phức tạp trong việc giải quyết các vấn đề kinh doanh.\n\nCông thức tổng quát khi viết code:\nKhi phân tích dữ liệu (hay bất cứ hoạt động nào cần phải viết code), công thức tổng quát cần phải ghi nhớ là:\n\n\\[e = mc^2 \\Leftrightarrow error = ({more\\;code})^2\\]\n\nDo đó, càng viết đơn giản, khả năng chúng ta giảm thiểu được các sai sót trong quá trình phân tích dữ liệu càng nhiều."
  },
  {
    "objectID": "index.html#phân-loại-hoạt-động-phân-tích-dữ-liệu",
    "href": "index.html#phân-loại-hoạt-động-phân-tích-dữ-liệu",
    "title": "Khoa học dữ liệu với R",
    "section": "1.2 Phân loại hoạt động phân tích dữ liệu",
    "text": "1.2 Phân loại hoạt động phân tích dữ liệu\n\n1.2.1 Dựa theo cách thức tác động đến khách hàng\nKhi làm việc thực tế, có nhiều nhánh trong hoạt động phân tích dữ liệu dẫn tới việc gây hiểu nhầm giữa các khía cạnh khác nhau. Khái quát hóa có thể chia làm hai nhánh lớn sau.\n\nPhân tích dữ liệu để tác động lên khách hàng một cách trực tiếp. Với cách tiếp cận này, để có thể tác động trực tiếp lên khách hàng, kết quả của hoạt động phân tích dữ liệu phải được đưa vào thực tế dưới dạng API và tác động trực tiếp lên UI mà người dùng sử dụng. Các hệ thống recommendation engine trên các trang thương mại điện tử thuộc loại này\nPhân tích dữ liệu để tác động lên khách hàng thông qua những người ra quyết định. Đây là nhóm được sử dụng đặc biệt nhiều trong thực tế nhưng lại bị đánh giá thấp hơn so với nhóm đầu tiên. Nhánh ứng dụng phân tích dữ liệu này được dùng đặc biệt nhiều trong các tổ chức lớn như ngân hàng, bảo hiểm… Trong nhóm này lại tách thành hai nhóm nhỏ:\n\nKết quả phân tích được thể hiện dưới dạng trình chiếu - công cụ thường hay được sử dụng nhất là powerpoint\nKết quả được sử dụng dưới dạng phần mềm (data tools). Với dạng này, người ra quyết định sẽ sử dụng các kết quả từ các công cụ này để đánh giá và ra quyết định thay đổi chính sách, sản phẩm cho phù hợp với khách hàng. Các công cụ như Google Analytics, Tableau (BI tools), hệ thống CRM, Customer 360 thuộc dạng này.\n\n\nLưu ý:\n\nĐối với mô hình nâng cao hơn, kết quả phân loại, dự báo có thể được hiển thị trên các công cụ này và khiến người sử dụng dễ dàng hơn trong việc ra quyết định.\n\n\n§\n\nCăn cứ theo nguồn dữ liệu sử dụng để phân tích, có thể tách ra thành hai nhóm lớn:\n\nDữ liệu trong nội bộ doanh nghiệp.\n\nNhóm dữ liệu nền tảng: Nguồn dữ liệu này thể hiện phần lớn hành vi, thói quen của khách hàng đối với sản phẩm, dịch vụ do doanh nghiệp nắm giữ. Tổ chức càng lớn, nguồn dữ liệu này càng khó được khai thác hết nếu không có hệ thống Data Warehouse tốt do gặp phải vấn đề phân tán và chất lượng dữ liệu. Các hệ thống như T24, W4, EBank, LOS của ngân hàng.. thuộc vào nhóm này\nNhóm dữ liệu hành vi nền tảng kỹ thuật số: Dữ liệu chứa nhiều các thông tin event, activity của khách hàng trên web, app - thường được để dạng ẩn danh hoặc đòi hỏi sự hỗ trợ từ mảng công nghệ để có thể sử dụng và tracking. Nguồn dữ liệu từ Google Analytics, Insider, Firebase… thuộc nhóm này\n\nDữ liệu bên ngoài doanh nghiệp. Nguồn dữ liệu không do doanh nghiệp quản lý, phản ánh hành vi của khách hàng nói chung. Ví dụ - social network, blog, giao dịch trên các trang thương mại điện tử, etc.\n\n\n§\n\nNgoài ra, theo định dạng, cấu trúc của dữ liệu có thể chia làm 3 nhóm lớn sau:\n\nDữ liệu có cấu trúc (Structred data): Dữ liệu định dạng bảng với các hàng và cột (excel, csv, database)\nDữ liệu bán phi cấu trúc (Semi-Unstructred data): Dữ liệu không theo trật tự bảng nhưng vẫn có thể chuyển đổi thành dữ liệu có cấu trúc (json, html)\nDữ liệu phi cấu trúc (Unstructured data): Dữ liệu định dạng âm thanh, hình ảnh (image), video\n\nPhần đa các bài toán triển khai trong doanh nghiệp sử dụng đến dữ liệu có cấu trúc hoặc bán phi cấu trúc. Trong phạm vi các note trong tài liệu này tập trung vào các kỹ thuật phân tích dữ liệu có cấu trúc.\n\n§\n\nMục tiêu toàn bộ của hoạt động phân tích dữ liệu là sử dụng dữ liệu, thông qua phân tích, dự báo để tác động lên hành vi khách hàng nhằm gia tăng giá trị của sản phẩm, dịch vụ của doanh nghiệp đối với khách hàng.\nKhái quát hóa hoạt động phân tích dữ liệu có thể thể hiện qua sơ đồ sau.\n\n\n\n\n\n1.2.2 Dựa theo đặc tính của nghề phân tích dữ liệu\n\n\n\nNếu bám sát vào hoạt động của sơ đồ phía trên, ta có thể chia các hoạt động có liên quan đến dữ liệu thành 4 nhóm lớn.\n\nData Governance - Quản trị dữ liệu: Quản trị các hoạt động ghi nhận dữ liệu vào hệ thống của tổ chức, đưa ra các quy định, quy trình để đảm bảo dữ liệu được đưa vào hệ thống một cách chính xác và đảm bảo yêu cầu của hoạt động kinh doanh. Ví dụ, dữ liệu địa chỉ của khách hàng phải để dạng dropdown theo các đơn vị hành chính để tránh sai sót (quận/huyện, phường/xã, tỉnh, thành phố)\nData Management - Quản lý dữ liệu: Quản lý dữ liệu sau khi dữ liệu được đưa vào hệ thống của tổ chức, bao gổm làm sạch, tái cấu trúc, sắp xếp dữ liệu theo hệ thống để đảm bảo sẵn sàng sử dụng dữ liệu. Nhóm công việc liên quan đến Quản lý dữ liệu bao gồm các vị trí:\n\nData Engineer: Kỹ sư dữ liệu, chịu trách nhiệm thực hiện việc dịch chuyển và biến đổi dữ liệu sang các vùng khác nhau\nBusiness Analyst: Phân tích nghiệp vụ, là cầu nối giữa kỹ sư dữ liệu với các hệ thống IT và nhu cầu nghiệp vụ. Với các tổ chức quy mô nhỏ, vai trò này nằm luôn cùng với Data Engineer nhưng với các tổ chức lớn, nghiệp vụ phức tạp, đây là vị trí đóng vai trò rất quan trọng\nDBA (Data Base Administrator): Quản lý, vận hành server hệ thống\n\nBusiness Intelligence: Khai thác dữ liệu để hỗ trợ hoạt động kinh doanh và quá trình ra quyết định. Đối với hoạt động BI, có thể chia thành ba cấp độ.\n\nData provider - cung cấp dữ liệu: Đơn vị kinh doanh/ nghiệp vụ yêu cầu như thế nào, cung cấp chính xác như vậy\nInformation & basic insight provider - cung cấp thông tin & các hiểu biết đơn giản về khách hàng và hoạt động kinh doanh: Đưa ra các nhận định, thông tin ở mức độ đơn giản nhưng rất hữu dụng cho hoạt động kinh doanh. Công cụ sử dụng cho cấp độ này phần lớn là Excel với pivot table đi kèm với các chỉ số thống kê cơ bản. Ví dụ, top 10% khách hàng chiếm đến 90% doanh thu của cả công ty.\nAdvanced insights provider - cung cấp các hiểu biết sâu sắc về khách hàng và hoạt động kinh doanh. Để đưa ra các hiểu biết này, phải sử dụng nhiều đến các công cụ và mô hình thuộc nhóm inference nhằm tác động đến hoạt động kinh doanh. Ví dụ, khách hàng nếu có ít nhất 3 loại giao dịch trong 2 tháng đầu tiên có khả năng ở lại với ngân hàng cao hơn 20% so với các khách hàng khác.\n\nPredictive Modelling/ Optimization - Phân tích dự báo, tối ưu hóa. Ở mức độ này, công việc phân tích dữ liệu sẽ tập trung nhiều vào việc dự báo và tối ưu hóa. Ví dụ: Khách hàng nào là khách hàng có nhiều khả năng bán chéo được sản phẩm thấu chi? Các công cụ sử dụng trong cấp độ này là các phần mềm chuyên biệt về phân tích thống kê, dự báo như R/Python/SAS. Các vị trí thuộc nhóm này bao gồm Data Scientist & ML/AI Engineer\n\nData Scientist: Xây dựng các mô hình dự báo\nML/AI Engineer: Đưa mô hình dự báo vào môi trường thực (production) dưới dạng package, API realtime.\n\n\n\n\n1.2.3 Dựa theo chu kỳ của khách hàng đối với sản phẩm\n\n\n\nXét về khía cạnh vòng đời sản phẩm, chu kỳ khách hàng, bất cứ hoạt động kinh doanh nào cũng có thể chia thành chu kỳ như sau.\n\nTim kiếm khách hàng - acquisition: Giai đoạn thu hút khách hàng mới, mục tiêu của giai đoạn này là đưa khách hàng mới về doanh nghiệp. Giai đoạn này, khách hàng chưa đem lại doanh số mà phần lớn chỉ là ở mức đăng ký dịch vụ, sản phẩm và trải nghiệm.\nKích hoạt khách hàng - activation: Sau khi một người đã trở thành khách hàng của doanh nghiệp, không có nghĩa người đó sẽ sử dụng sản phẩm dịch vụ mà thường phải có quá trình on-boarding. Mục tiêu của giai đoạn này không chỉ là để khách hàng có những trải nghiệm đầu tiên với sản phẩm dịch vụ mà còn khiến các sản phẩm đó trở nên quen thuộc và khách hàng bắt đầu thực sự sử dụng sản phẩm dịch vụ. Đây là giai đoạn rất nhiều doanh nghiệp lớn quên không sử dụng và đưa vào vận hành, khiến khách hàng vừa ở giai đoạn acquisition đã chuyển sang giai đoạn churn\nKhai thác khách hàng - deep farming: Chỉ sau khi khách hàng thực sự sử dụng sản phẩm, dịch vụ, lúc này doanh nghiệp mới có thể tiếp tục khai thác khách hàng. Thông thường có hai nhóm là bán chéo (cross-selling) và bán thêm (up-selling).\nGiữ chân khách hàng - retaining customers: Sau một thời gian sử dụng sản phẩm, dịch vụ, khách hàng lúc này sẽ có xu hướng rời bỏ doanh nghiệp để sử dụng các sản phẩm khác cạnh tranh hơn. Do đó, mục tiêu phân tích trong giai đoạn này là dự báo, tìm kiếm và hỗ trợ các hoạt động kinh doanh giúp giảm churn\nThu hồi nợ - collection/bad bank: Giai đoạn này thường chỉ sử dụng đối với các doanh nghiệp có hoạt động cho vay như ngân hàng. Mục tiêu phân tích là giảm tỷ lệ nợ, tỷ lệ nợ xấu\n\nNgoài các giai đoạn lớn trên, hoạt động phân tích dữ liệu còn tập trung vào 2 mảng lớn:\n\nGiữ chân khách hàng chủ động - proactively retaining customers: Phân tích để giữ chân khách hàng chủ động, ngay từ khi khách hàng chuyển qua giai đoạn có thể khai thác được. Nhiều doanh nghiệp lớn chưa chú trọng đến vấn đề này mà thường để đến khi khách hàng sắp rời bỏ doanh nghiệp mới tìm cách giữ chân. Khi đó, phần lớn mọi việc đã trở nên quá muộn.\nPhân nhóm khách hàng - customers segmentation: Phân nhóm khách hàng theo các đặc trưng về hành vi, tính chất để có thể đưa ra các sản phẩm phù hợp\n\n\n\n1.2.4 Dựa theo nhu cầu về hoạt động trong doanh nghiệp\nNếu chia theo ứng dụng trong hoạt động doanh nghiệp, ta có thể chia ứng dụng của hoạt động phân tích dữ liệu như sau"
  },
  {
    "objectID": "index.html#cách-xây-dựng-nhóm-phân-tích-dữ-liệu",
    "href": "index.html#cách-xây-dựng-nhóm-phân-tích-dữ-liệu",
    "title": "Khoa học dữ liệu với R",
    "section": "1.3 Cách xây dựng nhóm phân tích dữ liệu",
    "text": "1.3 Cách xây dựng nhóm phân tích dữ liệu\nĐể xây dựng nhóm phân tích dữ liệu, cần đảm bảo 3 nguyên tắc lớn sau:\n\nNắm vững kỹ năng phân tích khám phá dữ liệu\nBám sát hoạt động kinh doanh\nĐẩy mạnh tương tác nhóm & phân nhóm kỹ năng dựa trên sở thích và tính cách cá nhân.\n\nThứ nhất, nền tảng của toàn bộ hoạt động phân tích dữ liệu (Analytics) phải giải quyết được bài toán kinh doanh dưới góc độ phân tích dữ liệu. Do đó, không nhất thiết phải xây dựng các mô hình phức tạp để giải quyết bài toán. Ngược lại, trong nhiều tình huống, ta nên sử dụng các phương pháp đơn giản bởi hai lý do: thứ nhất - có thể giải thích cho kinh doanh và thứ hai - rất nhiều vấn đề không cần xây mô hình cũng có thể giải quyết ngay được bởi các nguyên nhân từ vận hành, con người và sản phẩm. Chỉ khi nắm vững được kỹ năng phân tích khám phá dữ liệu ta mới có thể đưa ra các nhận định và kết quả một cách nhanh chóng và chính xác hỗ trợ hoạt động kinh doanh.\nThứ hai, hoạt động phân tích được sinh ra nhằm phục vụ kinh doanh. Do đó, nếu không nắm vững đến một mức độ nhất định hoạt động kinh doanh, ta không thể đưa ra các kết luận thực tế và có thể triển khai được. Rất nhiều trường hợp, nhóm phân tích dữ liệu quá chú trọng đến mặt kỹ thuật và thuật toán, đưa ra những kết luận phi thực tế. Điều này cần tránh dối với các tổ chức hướng đến việc ứng dụng phân tích dữ liệu thưc tiễn hỗ trợ kinh doanh.\nThứ ba, các bạn làm việc trong lĩnh vực phân tích dữ liệu thường khá thông minh và thích làm việc tương đối độc lập. Tuy nhiên, bên cạnh đó còn có rất nhiều các khía cạnh khác nhau trong hoạt động phân tích dữ liệu. Có người ưa thích thuật toán và kiên trì và có thể tự học tốt nhưng lại yếu trong khâu giao tiếp và tương tác. Nhóm này nên khuyến khích phát triển các bài toán và vấn đề dự báo. Ngược lại, có nhóm rất thông minh, nhanh nhạy, thích thể hiện trước đông người nhưng lại thường xuyên cả thèm chóng chán. Nhóm này nên phát triển các kỹ năng khám phá dữ liệu, tìm kiếm insights và nên giao cho nhiệm vụ tương tác chính với kinh doanh. Ngoài ra, còn có nhóm trunh bình, việc gì cũng có thể làm ở mức độ khá nhưng không có việc gì thực sự nổi trội. Với nhóm này nên cố gắng bồi dưỡng để họ phát huy kiến thức tổng thể và hướng họ có thể back-up hai nhóm trên.\nThêm vào đó, việc phân rã kỹ năng còn giúp cho kiến thức không bị quá tập trung vào 1-2 thành viên, điều này có thể khiến cho hoạt động phân tích dữ liệu bị chậm lại do bị phụ thuộc vào những thành viên đó."
  },
  {
    "objectID": "index.html#lưu-ý-khi-đọc-tài-liệu",
    "href": "index.html#lưu-ý-khi-đọc-tài-liệu",
    "title": "Khoa học dữ liệu với R",
    "section": "1.4 Lưu ý khi đọc tài liệu",
    "text": "1.4 Lưu ý khi đọc tài liệu\nCác chương tiếp theo được trình bày với những kiến thức lõi về phân tích số liệu với R. Các chương này sẽ được viết theo hướng ứng dụng của khoa học dữ liệu trong hoạt động kinh doanh. Do đó, ngôn ngữ cũng như cách tiếp cận trong cuốn sách này được cố gắng viết một cách đơn giản, dễ hiểu để trình bày các kiến thức, thuật ngữ khó hiểu của khoa học dữ liệu thành các ngôn ngữ bình dân.\nCuốn sách này sẽ không đi sâu vào lý thuyết của các thuật toán, mô hình thống kê mà sẽ cố gắng trả lời các khía cạnh sau.\n\nThuật toán, mô hình đó là gì?\nMô hình đó được sử dụng như thế nào?\nKhi giải thích cho các đơn vị kinh doanh, ta cần phải giải thích điều gì?\nỨng dụng của mô hình trong thực tế như thế nào?\n\nBên cạnh đó, có nhiều phần còn tổng hợp thêm các kỹ thuật bổ trợ cần thiết cho hoạt động phân tích số liệu như git, cmd - đây là các kiến thức cơ bản cần thiết để có thể phát triển và làm việc trong lĩnh vực phân tích dữ liệu hiệu quả\n\n\n\n\n\n\nDisclaimer\n\n\n\nCác nội dung trong các chương tiếp theo là tài liệu tổng hợp từ nhiều nguồn và được sử dụng với mục đích cá nhân như tài liệu tra cứu tổng hợp nên không thể tránh khỏi việc thiếu sót. Tài liệu này chỉ nên dùng như tài liệu tham khảo.\n\n\n\n\n\n\n\n\nLưu ý\n\n\n\nĐối với các bạn muốn tập trung phát triển Insights, nên tập trung đọc các chương sau:\nCấp độ 1 - Phân tích khám phá - bắt buộc phải nắm vững (chương 1-7)\n\nGiới thiệu cơ bản về R (chương 1)\nXây dựng báo cáo với Rmarkdown\nBiến đổi dữ liệu với DPLYR\nPhân rã & xoay chiều dữ liệu\nNgữ pháp biểu đồ với GGPLOT2\nCác chỉ số thống kê cơ bản\n\nCấp độ 2 - Phân tích khám phá - Kỹ thuật nâng cao - nhưng vẫn cần bắt buộc phải nắm vững (chương 8-10)\n\nXử lý dữ liệu text\nLập trình với purrr\nLập trình hàm\n\nCấp độ 3 - cơ bản về các thuật toán & nguyên lý thường sử dụng:\n\nNguyên lý dự báo & cơ bản học máy (chương 18)\nMô hình hồi quy tuyến tính (chương 19)\nMô hình logistics (chương 20)\nCây quyết định & họ cây quyết định (chương 21-22)\nPhân cụm với kmeans (chương 38)\nBasket Analysis (chương 39)\n\nNâng cao\n\nSurvival Analysis (chương 54)\nQuantile Regression (chương 59)\nChuỗi thời gian (chương 41)\n\nCác chương và nội dung khác dùng để tham khảo đọc thêm nếu cần. Tuy nhiên, với việc nắm vững các kỹ thuật trên đã giải quyết 95%+ các bài toàn thực tế trên các lĩnh vực khác nhau và giúp các bạn có thể dịch chuyển nhanh chóng sang các tầng insights nâng cao"
  },
  {
    "objectID": "p02-01-gioi-thieu-co-ban-ve-r.html#giới-thiệu-về-r-và-hệ-sinh-thái-của-r",
    "href": "p02-01-gioi-thieu-co-ban-ve-r.html#giới-thiệu-về-r-và-hệ-sinh-thái-của-r",
    "title": "2  Giới thiệu cơ bản về R",
    "section": "2.1 Giới thiệu về R và hệ sinh thái của R",
    "text": "2.1 Giới thiệu về R và hệ sinh thái của R\nToàn bộ hệ sinh thái của R bao gồm các thành phần sau:\n\nR base: thành phần core của R: https://www.r-project.org/.\nR IDE (Integrated Development Environment): RStudio là phần mềm được thiết kế để việc sử dụng R trở nên dễ dàng hơn. Download tại website: https://www.rstudio.com/\nR packages: Tập hợp các lênh, dữ liệu, tài liệu để giải quyết 1 hoặc 1 nhóm các vấn đề trong phân tích. Hiện có hơn 5000 packagé khác nhau trên R. Truy cập website: https://cran.r-project.org/web/packages/\n\n\n§\n\nCách cài đặt R & Rstudio:\n\nCài đặt R-base: https://cran.r-project.org/bin/windows/base/\nCài đặt R-studio: https://www.rstudio.com/products/rstudio/download/\nGiới thiệu về giao diện R Studio:\n\nScript: Xem, chỉnh sửa và thực hiện các dòng lệnh trong R\nConsole: Hiển thị kết quả của câu lệnh từ script hoặc cho phép nhập lệnh và trả kết quả như R-base\nEnvironment & History: Xem lịch sử và các đối tượng (oject) trong R\nPlot, packges & Help: Xem kết quả plot, packages và help trong R\n\n\n\n\nCách tạo một script trong R:\n\nCtrl + Shift + N\nFile &gt;&gt; New File &gt;&gt; R Script\n\nMột số lưu ý khi viết lệnh trong R:\n\nComment: Viết sau ký tự “#”\nThực hiện dòng lệnh: Ctrl + Enter"
  },
  {
    "objectID": "p02-01-gioi-thieu-co-ban-ve-r.html#giới-thiệu-đối-tượng-cơ-bản-trong-r",
    "href": "p02-01-gioi-thieu-co-ban-ve-r.html#giới-thiệu-đối-tượng-cơ-bản-trong-r",
    "title": "2  Giới thiệu cơ bản về R",
    "section": "2.2 Giới thiệu đối tượng cơ bản trong R",
    "text": "2.2 Giới thiệu đối tượng cơ bản trong R\n\nTrong R, TẤT CẢ mọi thứ đều được lưu dưới dạng đối tượng (object), ví dụ: biến, mô hình, kết quả, biểu đồ. Tất cả được gọi là đối tượng\n\n\n#Ví dụ về đối tượng\n#Đối tượng là 1 số\nx&lt;-3\n#Đối tượng là 1 vector có phân phối chuẩn\ny&lt;-rnorm(1000,3,1)\n#Đối tượng là 1 biểu đồ\nhist(y, col=\"violet\", main = \"Histogram of normal distribution\")\n\n\n\n#Xem thư mục đang làm việc\n#Get working directory\ngetwd()\n\n[1] \"/cloud/project\"\n\n#Thay đổi thư mục làm việc\n#setwd(\"Thư mục làm việc\") - lưu ý thay \"/\" bằng \"\\\"\n\n#Xem các đối tượng đang làm việc\nls()\n\n[1] \"has_annotations\" \"x\"               \"y\"              \n\n#Xóa tất cả các đối tượng\n#rm = remove\nrm(list=ls())\n\n\n2.2.1 Câu lệnh\nCâu lệnh: Là một yêu cầu do người dùng tương tác với R để yêu cầu R thực hiện một hoặc một vài yêu cầu. Một câu lệnh sẽ có 2 thành phần: - Tên câu lệnh (function) - Biến (Variable)\nCấu trúc của câu lệnh như sau: f(argument1, argument2...) - f: câu lệnh/hàm (function) - argument: Tham số trong hàm\nXem các ví dụ dưới đây\n\n#VÍ DỤ VỀ CÂU LỆNH TRONG R\n#Các biến trong toán học\nexp(1)\n\n[1] 2.718282\n\ncos(-3.14)\n\n[1] -0.9999987\n\n#Các câu lệnh nâng cao khác\n#Tóm tắt các số liệu\nsummary(mtcars) #\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n#Xem tên các biến trong 1 bản ghi dữ liệu\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n#Xem các dòng đầu tiên của 1 bản dữ liệu\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n#Xem các dòng cuối cùng của 1 bản dữ liệu\ntail(mtcars)\n\n                mpg cyl  disp  hp drat    wt qsec vs am gear carb\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.5  0  1    5    6\nMaserati Bora  15.0   8 301.0 335 3.54 3.570 14.6  0  1    5    8\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.6  1  1    4    2\n\n\n\nĐể tìm thông tin và đọc tài liệu về một hàm, dùng lệnh ? Tên hàm hoặc help(tên hàm). Xem ví dụ:\n\n\n?mean\nhelp(mean)\n\n\n\n2.2.2 Packages trong R\n\nPackages (thư viện) trong R là một gói các câu lệnh, dữ liệu và tài liệu để xử lý một nhóm các vấn đề về phân tích thống kê. Các packages trong R thường được upload tại hai nguồn:\n\nCRAN: [https://cran.r-project.org/web/packages/] - lưu trữ các packages đã được phát triển và kiểm tra sự ổn định\nGithub: [github.com] - lưu trữ các packages đang được phát triển\n\nMột số các packages thường dùng: reshape2, `ggplot2, dplyr, forecast, rpart, car, zoo…\nCách cài đặt packages trong R:\n\n\n#Cài đặt 1 thư viên\n# Bước 1: Cài đặt với các packages trong CRAN\ninstall.packages(\"ggplot2\")\n# Bước 2: Load dữ liệu\nlibrary(ggplot2)\n\n# Cài đặt với github\ndevtools::install_github(\"rstudio/rmarkdown\")\n\n\n#Cài đặt nhiều thư viện\n#Bước 1: Tạo 1 list các thư viện cần dùng\npackages&lt;-c(\"car\", \"zoo\")\n#Bước 2: Cài đặt nhiều thư viên\ninstall.packages(packages)\n#Bước 3: Load nhiều thư viện 1 lúc\nlapply(packages, library, character.only = TRUE)"
  },
  {
    "objectID": "p02-01-gioi-thieu-co-ban-ve-r.html#cấu-trúc-dữ-liệu-cơ-bản",
    "href": "p02-01-gioi-thieu-co-ban-ve-r.html#cấu-trúc-dữ-liệu-cơ-bản",
    "title": "2  Giới thiệu cơ bản về R",
    "section": "2.3 Cấu trúc dữ liệu cơ bản",
    "text": "2.3 Cấu trúc dữ liệu cơ bản\n\n2.3.1 Véc-tơ\nVéc-tơ là mảng một chiều, trong đó có thể chứa dữ liệu kiểu số, ký tự, …\n\n#Vector chứa dữ liệu kiểu số. c~combine\na &lt;- c(1, 2, 3, 4, 5)\n#Vector chứa dữ liệu kiểu ký tự\nb &lt;- c(\"One\", \"two\", \"three\")\n#Phép tính logic\nc &lt;- c(TRUE, FALSE, FALSE)\n#Kiểu dữ liệu trong 1 vector phải đồng nhất\na &lt;- c(1, 2, 3, 4, 5, \"x\")\nclass(a)\n\n[1] \"character\"\n\n\nFactors: Factor là trường hợp đặc biệt của vector, là loại vector thể hiện nhóm dạng nominal (không sắp xếp được như: màu sắc, giới tính) hay ordinal (có thể sắp xếp được như: trình độ - cao đẳng, đại học, thạc sĩ, sở thích - ghét, bình thường, thích)\n\n#class là data frame\nclass(mtcars$vs)\n\n[1] \"numeric\"\n\ndata &lt;- mtcars\n#class là factor\ndata$vs &lt;- as.factor(data$vs)\nclass(data$vs)\n\n[1] \"factor\"\n\nsummary(data)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec       vs           am        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   0:18   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1:14   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71          Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85          Mean   :0.4062  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90          3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90          Max.   :1.0000  \n      gear            carb      \n Min.   :3.000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:2.000  \n Median :4.000   Median :2.000  \n Mean   :3.688   Mean   :2.812  \n 3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :5.000   Max.   :8.000  \n\n#Class là của các đối tượng không phải là số liệu\nclass(class)\n\n[1] \"function\"\n\n#class là mô hình\nlmfit &lt;- lm(mpg ~ wt, data = mtcars)\nclass(lmfit)\n\n[1] \"lm\"\n\n\n\n\n2.3.2 Ma trận\nMa trận là 1 mảng có 2 chiều, gồm dòng và cột\n\ny&lt;-matrix(1:20, nrow=5, ncol=4)\ny\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\n\n\n\n2.3.3 Data frames\nData frames là một bản ghi dữ liệu trong R gồm dòng và cột, dòng là biến, cột là quan sát (observation). Trong đó, mỗi dòng (mỗi biến) chứa cùng một kiểu dữ liệu (numeric, character, date,…)\n\n#Tạo vector của biến\npatientID &lt;- c(1, 2, 3, 4)\nage &lt;- c(25, 34, 28, 52)\ndiabetes &lt;- c(\"Type1\", \"Type2\", \"Type1\", \"Type1\")\nstatus &lt;- c(\"Poor\", \"Improved\", \"Excellent\", \"Poor\")\n#Tạo data frame. Lưu ý: các vector trong data frame cần có cùng độ dài\npatientdata &lt;- data.frame(patientID, age, diabetes, status)\npatientdata\n\n  patientID age diabetes    status\n1         1  25    Type1      Poor\n2         2  34    Type2  Improved\n3         3  28    Type1 Excellent\n4         4  52    Type1      Poor\n\n#MỘT SỐ LỆNH CƠ BẢN THƯỜNG SỬ DỤNG TRONG DATA FRAME\n\n#Xem tên các biến trong data frame\nnames(patientdata)\n\n[1] \"patientID\" \"age\"       \"diabetes\"  \"status\"   \n\n#Xem biến trong data frame\npatientdata$age\n\n[1] 25 34 28 52\n\n#Lựa chọn giá trị của cột 3, dòng 2\npatientdata[3, 2]\n\n[1] 28\n\n#Lựa chọn tập dữ liệu trong đó age&gt;28\npatientdata[patientdata$age &gt; 28, ]\n\n  patientID age diabetes   status\n2         2  34    Type2 Improved\n4         4  52    Type1     Poor\n\n#Lựa chọn dữ liệu chỉ có age, status: Cách 1\npatientdata[c(\"age\", \"status\")]\n\n  age    status\n1  25      Poor\n2  34  Improved\n3  28 Excellent\n4  52      Poor\n\n#Lựa chọn dữ liệu chỉ có age, status: Cách 2\npatientdata[c(2, 4)]\n\n  age    status\n1  25      Poor\n2  34  Improved\n3  28 Excellent\n4  52      Poor\n\n\nAttach, detach đối với data frame\n\nAttach: Lưu lại data frame với chế độ mặc định. Các câu lệnh tiếp theo sẽ mặc định sử dụng dữ liệu đã được lưu\nDetach: Loại bỏ data frame mặc định\nWith: cách thức thay thế attach\n\n\n#Khi chưa attach\ndata(mtcars)\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n#Các biến trong data frame được đi sau ký tự $\nsummary(mtcars$mpg) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\n\n\n2.3.4 Lists\nList là một tập hợp các đối tượng. Một list có thể chứa vector, ma trận,… Cấu trúc: mylist &lt;- list(name1=object1, name2=object2, …)\n\ng &lt;- \"My First List\"\nh &lt;- c(25, 26, 18, 39)\nj &lt;- matrix(1:10, nrow=5)\nk &lt;- c(\"one\", \"two\", \"three\")\nmylist &lt;- list(title=g, ages=h, ducanh = j, Object_4 = k)"
  },
  {
    "objectID": "p02-01-gioi-thieu-co-ban-ve-r.html#một-vài-lỗi-cơ-bản-khi-sử-dụng-r",
    "href": "p02-01-gioi-thieu-co-ban-ve-r.html#một-vài-lỗi-cơ-bản-khi-sử-dụng-r",
    "title": "2  Giới thiệu cơ bản về R",
    "section": "2.4 Một vài lỗi cơ bản khi sử dụng R",
    "text": "2.4 Một vài lỗi cơ bản khi sử dụng R\n\n#Lỗi viết hoa, viết thường: R phân biệt viết hoa, viết thường\nhelp() vs. HELP() vs. Help()\n#Không có dấu ngoặc kép khi cài đặt packages\ninstall.packages(\"car\") vs. install.packages(car)\n#Sử dụng dấy gạch \"\\\" trong windows cho đường dẫn thay vì \"/\"\nsetwd(\"D:/mydata\") vs. setwd(\"D:\\mydata\")"
  },
  {
    "objectID": "p02-01-gioi-thieu-co-ban-ve-r.html#tài-liệu-tham-khảo",
    "href": "p02-01-gioi-thieu-co-ban-ve-r.html#tài-liệu-tham-khảo",
    "title": "2  Giới thiệu cơ bản về R",
    "section": "2.5 Tài liệu tham khảo",
    "text": "2.5 Tài liệu tham khảo\n\nR in Action\nR in Nutshell\nR Cookbook"
  },
  {
    "objectID": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#giới-thiệu",
    "href": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#giới-thiệu",
    "title": "3  Xây dựng báo cáo với Rmarkdown",
    "section": "3.1 Giới thiệu",
    "text": "3.1 Giới thiệu\nKhi xây dựng các project phân tích dữ liêu, việc ghi chép lại quá trình và các bước phân tích đóng một vai trò rất quan trọng. Với R, ta có thể sử dụng rmarkdown để thực hiện các công việc sau:\n\nXây dựng báo cáo, giúp cho người ra quyết địnnắm được insights và ra quyết định\nGhi chép lại toàn bộ quá trình phân tích chi tiết\n\nrmarkdown cho phép ta làm được các công việc sau:\n\nCho phép viết toàn bộ phân tích trong 1 báo cáo\nCho phép chèn code, vẽ biểu đồ, hiển thị đính kèm báo cáo\nCho phép comment các kết quả phân tích\nHỗ trợ các biểu đồ/ bảng động trong phân tích\nCho phép tái phân tích toàn bộ dữ liệu nhanh chóng"
  },
  {
    "objectID": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#sử-dụng-rmarkdown",
    "href": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#sử-dụng-rmarkdown",
    "title": "3  Xây dựng báo cáo với Rmarkdown",
    "section": "3.2 Sử dụng Rmarkdown",
    "text": "3.2 Sử dụng Rmarkdown\n\n3.2.1 Tạo file mới\nĐể có thể xây dựng báo cáo, ta cần cài đặt package rmarkdown như sau\n\ninstall.packages(\"rmarkdown\")\nlibrary(rmarkdown)\n\nTạo file markdown mới\nFile &gt;&gt; New file &gt;&gt; rmarkdown &gt;&gt; OK\n\nFile rmarkdown mới tạo có định dạng như sau\n\n\n\n3.2.2 Các thành phần cơ bản của 1 file markdown\n\n3.2.2.1 Header\nPhần header (hay metadata) của file markdown được ghi sau hai dấu --- như sau.\n---\ntitle: \"Untitled\"\noutput: html_document\n---\nTrong thực tế, khi xây dựng báo cáo, ta có thể khai báo metadata trong phần header như sau.\n---\ntitle: \"Nội dung phân tích\"\noutput: \n  html_document: \n    highlight: haddock\n    number_sections: yes\n    theme: readable\n    toc: yes\n    toc_float: true\n    code_folding: hide\n---\nÝ nghĩa các tham số như sau:\n\nhighlight: Theme màu của code trong file rmarkdown\nnumber_sections: Đánh số tự động các phần heading\ntheme: Theme của cả file rmarkdown\ntoc: Hiển thị mục lục, yes hoặc no\ntoc_float: Chế độ cho phép mục lục treo ở phần bên trái, true hoặc false\ncode_folding: Chế độ cho phép ẩn hoặc hiện code trong báo cáo\n\n\n\n3.2.2.2 Text trong markdown\nmarkdown cho phép người dùng tập trung vào content. Do đó, cấu trúc viết text trên markdown sẽ ưu tiên vào sự đơn giản. Cấu trúc của markdown như sau.\n# Heading 1\n## Heading 2\n\nNội dung phân tích\nCác cú pháp hay dùng trong markdown:\n\n\n\nNội dung\nCú pháp\n\n\n\n\nTiêu đề 1\n# Tiêu đề 1\n\n\nTiêu đề 2\n## Tiêu đề 2\n\n\nTiêu đề 3\n### Tiêu đề 3\n\n\nIn đậm\n**In đậm**\n\n\nIn nghiêng\n*In nghiêng*\n\n\nCông thức toán\n$$E = mc^2$$\n\n\nGạch đầu dòng\n-\n\n\n\nCác cấu trúc khác, xem thêm tại Rmarkdown cheatsheet\n\n\n3.2.2.3 R code\nTrong file rmarkdown, ta có thể chèn các câu lệnh với R. Một đoạn code chứa trong rmarkdown được gọi là code chuink,\n\nCách chèn R code trong Rmarkdown: Ctrl + Shift + I\nCấu trúc R code chunk\n\n```{r, r code options}\nmtcars %&gt;% summary\nCác options thường dùng:\n\n\n\nOptions\nÝ nghĩa\n\n\n\n\neval = F\nKhông chạy code, chỉ hiển thị code\n\n\neho = F\nChạy code, không hiển thị code\n\n\nwarning = F\nKhông hiển thị cảnh báo\n\n\nmessage = F\nKhông hiển thị thông báo\n\n\nfig.height = 8\nĐể biểu đồ cao 8 inches\n\n\nfig.width = 6\nĐể biểu đồ cao 6 inches\n\n\n\n\n\n3.2.2.4 Thực hiện báo cáo\n\nCách 1: Click vào Knit\nCách 2: Ctrl + Shift + K"
  },
  {
    "objectID": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#tài-liệu-tham-khảo",
    "href": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#tài-liệu-tham-khảo",
    "title": "3  Xây dựng báo cáo với Rmarkdown",
    "section": "3.4 Tài liệu tham khảo",
    "text": "3.4 Tài liệu tham khảo\n\nRmarkdown cheatsheet tiếng Việt\nRstudio document\nQuarto document"
  },
  {
    "objectID": "p02-02-bien-doi-du-lieu-dplyr.html#giới-thiệu-về-pipe-operator",
    "href": "p02-02-bien-doi-du-lieu-dplyr.html#giới-thiệu-về-pipe-operator",
    "title": "4  Ngữ pháp của biến đổi dữ liệu với DPLYR",
    "section": "4.1 Giới thiệu về pipe operator",
    "text": "4.1 Giới thiệu về pipe operator\nKhi viết các câu lệnh, thông thường ta có 2 cách viết phổ biến sau.\n\nCách 1: Viết với các câu lệnh lồng vào nhau (nested). Với cách viết này, các hàm sẽ được viết lồng vào nhau và kết quả của hàm sẽ được tính toán theo thứ tự từ trong ra ngoài.\nCách 2: Viết lưu dưới dạng các đối tượng trung gian. Với cách viết này, từng đối tượng sẽ được tính toán từng phần và kết quả sẽ được hiển thị một cách mạch lạc hơn. Tuy nhiên, nhược điểm của phương pháp này là sẽ tạo ra rất nhiều đối tượng trung gian, gây ra khó khăn trong việc theo dõi và quản lý.\n\nGiả sử ta cần tính toán độ lệch chuẩn của véc-tơ x, công thức tính độ lệch chuẩn sẽ là\n\\[\\sigma = \\frac{\\sum_{i=1}^n(x_i-\\overline{x})^2}{n-1}\\]\nVới hai cách viết code khác nhau, ta có thể tính độ lệch chuẩn theo hai cách.\n\n# Cách 1\n# Tạo vector x\nx &lt;- seq(2, 100, 2)  \n# Tính độ lệch chuẩn\nsqrt(sum((x-mean(x))^2)/(length(x)-1))\n\n[1] 29.15476\n\nsd(x)\n\n[1] 29.15476\n\n\n\n# Cách 2\n# Tạo vector x\nx &lt;- seq(2, 100, 2)\n# Tính tổng bình phương \nsum_sqr &lt;- sum((x-mean(x))^2)\nlen_x &lt;- length(x)\nvar &lt;- sum_sqr/(len_x - 1)\nsd &lt;- var^(1/2)\nsd\n\n[1] 29.15476\n\n\nTa thấy kết quả ở hai cách tính là như nhau. Tuy nhiên, cách viết hai sẽ tạo ra nhiều đối tượng trung gian hơn cách viết 1 rất nhiều. Khi phân tích dữ liệu thực tế, ta sẽ phải áp dụng cả 2 cách viết code để có thể vận dụng linh hoạt trong từng trường hợp cụ thể.\nTrong R, có cách viết code thứ ba, được gọi là cách suwr dụng pipe operator (%&gt;%). Toán tử Pipe cho phép viết code theo cách đơn giản và dễ theo dõi giúp cho người đọc và người viết code trên R có thể theo dõi được code một cách dễ dàng nhất. Câu trúc của pipe như sau\n\nf(x, y) = x %&gt;% f(., y) \n\nVí dụ của pipe.\n\n# Cách 1\nmean(x)\n# Cách 2\nx %&gt;% mean\n\nTa có thể xem xét ví dụ phức tạp hơn.\n\n# Cách 1 - dùng cách viết thường\nsummary(head(iris))\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width    \n Min.   :4.600   Min.   :3.000   Min.   :1.300   Min.   :0.2000  \n 1st Qu.:4.750   1st Qu.:3.125   1st Qu.:1.400   1st Qu.:0.2000  \n Median :4.950   Median :3.350   Median :1.400   Median :0.2000  \n Mean   :4.950   Mean   :3.383   Mean   :1.450   Mean   :0.2333  \n 3rd Qu.:5.075   3rd Qu.:3.575   3rd Qu.:1.475   3rd Qu.:0.2000  \n Max.   :5.400   Max.   :3.900   Max.   :1.700   Max.   :0.4000  \n       Species \n setosa    :6  \n versicolor:0  \n virginica :0  \n               \n               \n               \n\n# Cách 2 - dùng pipe\niris %&gt;% head %&gt;% summary\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width    \n Min.   :4.600   Min.   :3.000   Min.   :1.300   Min.   :0.2000  \n 1st Qu.:4.750   1st Qu.:3.125   1st Qu.:1.400   1st Qu.:0.2000  \n Median :4.950   Median :3.350   Median :1.400   Median :0.2000  \n Mean   :4.950   Mean   :3.383   Mean   :1.450   Mean   :0.2333  \n 3rd Qu.:5.075   3rd Qu.:3.575   3rd Qu.:1.475   3rd Qu.:0.2000  \n Max.   :5.400   Max.   :3.900   Max.   :1.700   Max.   :0.4000  \n       Species \n setosa    :6  \n versicolor:0  \n virginica :0  \n               \n               \n               \n\n\nCả hai cách đều cho ra kết quả giống nhau. Tuy nhiên, cách hai sẽ dễ theo dõi, dễ đọc hơn cách 1 rất nhiều. Cách đọc hiểu quá trình thực hiện pipe như sau:\n\nGọi tập dữ liệu iris để phân tích\nThực hiện hàm head trên tập dữ liệu này, được kết quả bao nhiêu…\n… tiếp tục thực hiện hàm summary\n\nNhư ta thấy, cách viết theo phong cách của pipe operator (%&gt;%) cho phép ta thực hiện các phép tính theo đúng mạch tư duy logic của bản thân. Điều này là một điểm rất mạnh mà hiện tại, mới chỉ ở R có toán tử %&gt;% áp dụng được cho mọi hàm.\nMột số đặc tính cơ bản của toán tử pipe:\n\nTheo mặc định, Phía tay trái (LHS) sẽ được chuyển tiếp thành yếu tố đầu tiên của hàm được sử dụng phía tay phải (RHS), ví dụ:\n\n\nmean(x) \n\n[1] 51\n\n# Tương đương với:\nx %&gt;% mean\n\n[1] 51\n\n\n\nKhi LHS không còn là yếu tố đầu tiên của một hàm RHS, thì dấu “.” được sử dụng để định vị cho LHS, ví dụ:\n\n\nlibrary(dplyr)\n# Cách 1\nsummary(lm(mpg ~ cyl, data = mtcars))\n\n# Cách 2\nmtcars %&gt;% \n  lm(mpg ~ cyl, data = .) %&gt;% \n  summary\n\nTrong tình huống trên, tham số về dữ liệu trong hàm lm không phải là ở đầu, mà sau phần công thức, nên chúng ta sẽ dùng dấu “.” như là đại diện của thực thể mtcars ở bên ngoài (LHS) của hàm lm.\n\n§"
  },
  {
    "objectID": "p02-02-bien-doi-du-lieu-dplyr.html#các-hàm-cơ-bản-trong-dplyr",
    "href": "p02-02-bien-doi-du-lieu-dplyr.html#các-hàm-cơ-bản-trong-dplyr",
    "title": "4  Ngữ pháp của biến đổi dữ liệu với DPLYR",
    "section": "4.3 Các hàm cơ bản trong dplyr",
    "text": "4.3 Các hàm cơ bản trong dplyr\nTrong công việc biến đổi dữ liệu, bất kỳ ngôn ngữ phân tích nào cũng có 3 nhóm hàm lớn.\n\nNhóm 1 - các hàm truy vấn dữ liệu: Lấy dữ liệu theo dòng, theo cột và theo điều kiện. Trong dplyr sẽ là các hàm select, filter và slice\nNhóm 2 - Các hàm tổng hợp dữ liệu: Tính toán tổng hợp dữ liệu theo chiều. Trong dplyr sẽ là các hàm group_by, summarise\nNhóm 3 - Các hàm biến đổi dữ liệu: Tạo mới, biến đổi các dữ liệu cũ thành các dữ liệu mới. Trong dplyr sẽ là các hàm thuộc nhóm mutate, join, bind\n\nTrong phần này, chúng ta sẽ giới thiệu nhanh các nhóm câu lệnh cơ bản trên.\n\n4.3.1 Nhóm câu lệnh truy vấn dữ liệu\nKhi truy vấn dữ liệu, ta thường phải thực hiện 3 nhóm công việc sau.\n\nLấy theo cột\nLấy theo dòng\nLấy theo điều kiện\n\nXét về mặt bản chất, lấy theo điều kiện là một trường hợp đặc biệt của việc lấy theo dòng. Đối với các ngôn ngữ như SQL, sẽ không phân biệt hai loại này. Tuy nhiên, vì R lưu thứ tự của từng quan sát trong dataframe, nên việc phân biệt được hai loại truy vấn trên là cần thiết.\n\n4.3.1.1 Lấy các cột trong dataframe với select\n\ndata %&gt;% select(var1, var2, ...)\n\nTrong đó, var1, var2 là tên các cột cần truy vấn. Đặc biệt, R rất linh hoạt trong việc lọc theo cột. Ta có thể truy vấn theo tên, theo thứ tự hoặc thậm chí theo các khoảng thứ tự các biến. Xem ví dụ sau.\n\nlibrary(dplyr)\n# Xêm tên các biến trong mtcars\nmtcars %&gt;% names\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n# Chọn cột mpg và cyl\nmtcars %&gt;% select(mpg, cyl) %&gt;% head\n\n                   mpg cyl\nMazda RX4         21.0   6\nMazda RX4 Wag     21.0   6\nDatsun 710        22.8   4\nHornet 4 Drive    21.4   6\nHornet Sportabout 18.7   8\nValiant           18.1   6\n\n# Chọn cột thứ nhất và thứ hai\nmtcars %&gt;% select(1,2) %&gt;% head\n\n                   mpg cyl\nMazda RX4         21.0   6\nMazda RX4 Wag     21.0   6\nDatsun 710        22.8   4\nHornet 4 Drive    21.4   6\nHornet Sportabout 18.7   8\nValiant           18.1   6\n\n# Chọn cột thứ 3 đến cột thứ 6\nmtcars %&gt;% select(3:6) %&gt;% head\n\n                  disp  hp drat    wt\nMazda RX4          160 110 3.90 2.620\nMazda RX4 Wag      160 110 3.90 2.875\nDatsun 710         108  93 3.85 2.320\nHornet 4 Drive     258 110 3.08 3.215\nHornet Sportabout  360 175 3.15 3.440\nValiant            225 105 2.76 3.460\n\n\n \nNgoài ra, khi lấy chi tiết các cột (liệt kê từng cột) khi lấy dữ liệu trên 1 bảng, bạn có thể dùng một số hàm sau để hỗ trợ việc lấy trường dữ liệu được nhanh hơn:\n\nstarts_with(\"Ký tự là thông tin mong muốn\"): các cột dữ liệu ccó tên hứa các ký tự mong muốn đứng ở đầu của tên, ví dụ:\n\n\niris %&gt;%\n  select(starts_with(\"Petal\")) %&gt;%\n  head\n\n  Petal.Length Petal.Width\n1          1.4         0.2\n2          1.4         0.2\n3          1.3         0.2\n4          1.5         0.2\n5          1.4         0.2\n6          1.7         0.4\n\n\n\nends_with(\"Ký tự là thông tin mong muốn\"): các cột dữ liệu có tên chứa các ký tự mong muốn ở cuối của tên, ví dụ:\n\n\niris %&gt;%\n  select(ends_with(\"Length\")) %&gt;%\n  head\n\n  Sepal.Length Petal.Length\n1          5.1          1.4\n2          4.9          1.4\n3          4.7          1.3\n4          4.6          1.5\n5          5.0          1.4\n6          5.4          1.7\n\n\n\ncontains(\"Ký tự là thông tin mong muốn\"): các cột dữ liệu có tên chứa chính xác các ký tự mong muốn ở bất kỳ vị trí nào của tên, ví dụ:\n\n\niris %&gt;%\n  select(contains(\"etal\")) %&gt;%\n  head\n\n  Petal.Length Petal.Width\n1          1.4         0.2\n2          1.4         0.2\n3          1.3         0.2\n4          1.5         0.2\n5          1.4         0.2\n6          1.7         0.4\n\n\n\nmatches(“Dạng ký tự là thông tin mong muốn”): các cột dữ liệu có tên chứa các ký tự có dạng ký tự mong muốn ở bất kỳ vị trí nào của tên, ví dụ:\n\n\niris %&gt;%\n  select(matches(\".t.\")) %&gt;% \n  head\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n4          4.6         3.1          1.5         0.2\n5          5.0         3.6          1.4         0.2\n6          5.4         3.9          1.7         0.4\n\n\nTrong ví dụ trên, R sẽ lấy tất cả các cột có tên chứa chữ t và có ký tự khác ở trước và sau (các ký tự chỉ chứa chữ t mà chữ t ở đâu hoặc cuối tên sẽ không được tính vào)\nThêm vào đó, ta có thể đổi tên biến ngay trong khi lựa chọn các biến với select như sau:\n\nmtcars %&gt;%\n  select(`miles per gallon` = mpg\n         , cylinder = cyl\n         , weight = wt) %&gt;%\n  head\n\n                  miles per gallon cylinder weight\nMazda RX4                     21.0        6  2.620\nMazda RX4 Wag                 21.0        6  2.875\nDatsun 710                    22.8        4  2.320\nHornet 4 Drive                21.4        6  3.215\nHornet Sportabout             18.7        8  3.440\nValiant                       18.1        6  3.460\n\n\n\n\n4.3.1.2 Lấy các dòng trong dataframe với slice\n\ndata %&gt;% slice(observation)\n\nTương tự như lấy theo cột, ta có thể lấy các dòng trong một dataframe. Tuy nhiên, lưu ý hàm slice chỉ cho phép điều kiện lấy quan sát là một véc-tơ. Xem ví dụ sau.\n\n# Lấy dòng đầu tiên\nmtcars %&gt;% slice(1)\n\n          mpg cyl disp  hp drat   wt  qsec vs am gear carb\nMazda RX4  21   6  160 110  3.9 2.62 16.46  0  1    4    4\n\n# Lấy dòng từ 1:3\nmtcars %&gt;% slice(1:3)\n\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n\n# Lấy dòng 1:3 và 5\nmtcars %&gt;% slice(c(1:3,5))\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n\n4.3.1.3 Lọc quan sát theo điều kiện với filter\n\ndata %&gt;% filter(condition)\n\nHàm filter cho phép ta sử dụng các điều kiện phức tạp để truy xuất dữ liệu từ dataframe. Các điều kiện thường dùng bao gồm.\n\n\n\n\n\n\n\n\n   Dấu\nKý hiệu\n Ví dụ\n\n\n\n\nBằng\n==\n7==8\n\n\nKhác\n!=\n7!=8\n\n\nLớn hơn\n&gt;\na &gt; b\n\n\nLớn hơn hoặc bằng\n&gt;=\na &gt;= b\n\n\nNhỏ hơn\n&lt;\na &lt; b\n\n\nNhỏ hơn hoặc bằng\n&lt;=\na &lt;= b\n\n\nVà\n&\na &gt; 7 & b &lt; 9\n\n\n\nXem ví dụ sau.\n\n# Lọc điều kiện mpg &gt; 20\nmtcars %&gt;%\n  filter(mpg &gt; 20) %&gt;% \n  dim\n\n[1] 14 11\n\n# Lọc điều kiện mpg &gt;20 hoặc mpg &lt;18\nmtcars %&gt;% \n  filter(mpg &gt; 20 | mpg &lt; 18) %&gt;% \n  dim\n\n[1] 27 11\n\n# Lọc điều kiện mpg &gt;=20 và cyl = 6\nmtcars %&gt;% \n  filter(mpg &gt; 20 & cyl == 6)\n\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n\n\nLưu ý: Khi điều kiện hoặc là chuỗi các giá trị rời rạc áp dụng cho cùng một trường, chúng ta có thể làm ngắn gọn hơn với cấu trúc “%in%” thay vì cấu phải liệt kê tất cả các điều kiện đơn lẻ và ngăn cách nhau bởi dấu “|”:\n\nmtcars %&gt;%\n filter(carb == 4 | carb == 3 | carb == 1)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n\n\nCâu lệnh trên tương đương với:\n\nmtcars %&gt;%\n  filter(carb %in% c(1, 3, 4))\n\n\n\n4.3.1.4 Sắp xếp dữ liệu với arrange\n\ndata %&gt;% arrange(var1, var2)\n\nNgoài việc lọc dữ liệu có điều kiện, chúng ta cũng thường xuyên thực hiện việc sắp xếp dữ liệu theo một trật tự nhất định nào đó khi xem dữ liệu. Hàm arrange() hỗ trợ công việc này. Cách thức sắp xếp dữ liệu mặc định là từ nhỏ đến lớn.\n\nmtcars %&gt;%\n  arrange(mpg) %&gt;% \n  head\n\n                     mpg cyl disp  hp drat    wt  qsec vs am gear carb\nCadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4\nCamaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4\nDuster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\nChrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4\nMaserati Bora       15.0   8  301 335 3.54 3.570 14.60  0  1    5    8\n\n\nKhi có nhiều biến cần được sắp xếp, hàm arrange sẽ ưu tiên các biến theo thứ tự từ trái sang phải.\n\nmtcars %&gt;% arrange(mpg, cyl)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n\n\n\n\n4.3.1.5 Đổi tên biến với rename\n\ndata %&gt;% rename(new_var = old_var)\n\n\nmtcars %&gt;%\n  rename(displacement = disp,\n         miles_per_gallon = mpg) %&gt;% \n  names\n\n [1] \"miles_per_gallon\" \"cyl\"              \"displacement\"     \"hp\"              \n [5] \"drat\"             \"wt\"               \"qsec\"             \"vs\"              \n [9] \"am\"               \"gear\"             \"carb\"            \n\n\n\n\n\n4.3.2 Nhóm câu lệnh biến đổi dữ liệu\n\n4.3.2.1 Tạo mới trường dữ liệu với mutate\nTrong quá trình xử lý dữ liệu, ta thường xuyên phải tạo thêm các trường dữ liệu mới (trường dữ liệu phát sinh). Hàm mutate() được sử dụng để làm công việc này. Cấu trúc của hàm rất đơn giản như sau.\n\ndata %&gt;% mutate(new_var = statement)\n\nXem ví dụ sau:\n\nmtcars %&gt;%\n  select(mpg) %&gt;%\n  mutate(new_mpg = mpg * 2) %&gt;%\n  head\n\n                   mpg new_mpg\nMazda RX4         21.0    42.0\nMazda RX4 Wag     21.0    42.0\nDatsun 710        22.8    45.6\nHornet 4 Drive    21.4    42.8\nHornet Sportabout 18.7    37.4\nValiant           18.1    36.2\n\n\nTrong một số trường hợp, khi ta không muốn lấy các trường thông tin cũ mà chỉ muốn lấy các trường thông tin mới tạo thì có thể sử dụng hàm transmute() với cấu trúc giống như hàm mutate.\n\nmtcars %&gt;%\n  select(mpg) %&gt;% \n  transmute(new_mpg = mpg * 1.61) %&gt;%\n  head\n\n                  new_mpg\nMazda RX4          33.810\nMazda RX4 Wag      33.810\nDatsun 710         36.708\nHornet 4 Drive     34.454\nHornet Sportabout  30.107\nValiant            29.141\n\n\n\n\n4.3.2.2 Gộp nhiều bảng với nhóm hàm join\n\nHàm inner_join(x, y, by = \"key\"): lấy tất cả dữ liệu có trên bảng hai bảng khi trùng key, ví dụ:\n\n\nx &lt;- data.frame(student_id = seq(1, 10, 1), \n                maths = c(10, 8, 7, 6, 7.8, 4, \n                          7.7, 9, 9.5, 6.5))\ny &lt;- data.frame(student_id = seq(2, 20, 2), \n                physics = c(8, 9.5, 7.5, 6, 5.5, \n                            6.5, 7.8, 8.2, 8, 7.5))\nx\n\n   student_id maths\n1           1  10.0\n2           2   8.0\n3           3   7.0\n4           4   6.0\n5           5   7.8\n6           6   4.0\n7           7   7.7\n8           8   9.0\n9           9   9.5\n10         10   6.5\n\ny\n\n   student_id physics\n1           2     8.0\n2           4     9.5\n3           6     7.5\n4           8     6.0\n5          10     5.5\n6          12     6.5\n7          14     7.8\n8          16     8.2\n9          18     8.0\n10         20     7.5\n\n# gộp 2 bảng dữ liệu x và y theo student_id\nx %&gt;%\n  inner_join(y, by = \"student_id\") \n\n  student_id maths physics\n1          2   8.0     8.0\n2          4   6.0     9.5\n3          6   4.0     7.5\n4          8   9.0     6.0\n5         10   6.5     5.5\n\n\n\n\nfull_join: lấy tất cả dữ liệu có cả trên bảng x, y.\n\n\nfull_join(x, y, by = \"key\")\n\n\nx %&gt;%\n  full_join(y, by = \"student_id\")\n\n   student_id maths physics\n1           1  10.0      NA\n2           2   8.0     8.0\n3           3   7.0      NA\n4           4   6.0     9.5\n5           5   7.8      NA\n6           6   4.0     7.5\n7           7   7.7      NA\n8           8   9.0     6.0\n9           9   9.5      NA\n10         10   6.5     5.5\n11         12    NA     6.5\n12         14    NA     7.8\n13         16    NA     8.2\n14         18    NA     8.0\n15         20    NA     7.5\n\n\nTrong ví dụ trên, các giá trị về điểm toán (maths) sẽ trả về NA cho các student_id không tồn tại trên bảng y và ngược lại cho bảng x với các giá trị điểm vật lý (physics) của các student_id không tồn tại trên bảng x.\n\n\nHàm left_join: lấy dữ liệu chỉ có trên bảng x, ví dụ:\n\n\nleft_join(x, y, by = \"var\")\n\n\nx %&gt;%\n  left_join(y, by = \"student_id\") \n\n   student_id maths physics\n1           1  10.0      NA\n2           2   8.0     8.0\n3           3   7.0      NA\n4           4   6.0     9.5\n5           5   7.8      NA\n6           6   4.0     7.5\n7           7   7.7      NA\n8           8   9.0     6.0\n9           9   9.5      NA\n10         10   6.5     5.5\n\n\nVới các student_id không có giá trị trên bảng y, cột physics sẽ trả về giá trị NA\n\n\nHàm right_join : lấy dữ liệu chỉ có trên bảng y, ví dụ:\n\n\nright_join(x, y, by = \"var\")\n\n\nx %&gt;%\n  right_join(y, by = \"student_id\") \n\n   student_id maths physics\n1           2   8.0     8.0\n2           4   6.0     9.5\n3           6   4.0     7.5\n4           8   9.0     6.0\n5          10   6.5     5.5\n6          12    NA     6.5\n7          14    NA     7.8\n8          16    NA     8.2\n9          18    NA     8.0\n10         20    NA     7.5\n\n\nVới các student_id không có giá trị trên bảng x, cột maths sẽ trả về giá trị NA\nLưu ý: Trong trường hợp cột dữ liệu dùng để nối các bảng có tên khác nhau, ta có thể sử dụng cấu trúc sau:\n\nleft_join(x, y, by = c(\"key_x\" = \"key_y\"))\n\nXem ví dụ sau:\n\nnames(x)[1] &lt;- \"student_id1\"\nnames(y)[1] &lt;- \"student_id2\"\n\nx %&gt;%\n  inner_join(y, by = c(\"student_id1\" = \"student_id2\")) \n\n  student_id1 maths physics\n1           2   8.0     8.0\n2           4   6.0     9.5\n3           6   4.0     7.5\n4           8   9.0     6.0\n5          10   6.5     5.5\n\n\n\nLưu ý: Khi dữ liệu có NA, dplyr sẽ cho phép join NA với NA, cần phải sử dụng option na_matches\n\ndf1 &lt;- data.frame(id = c(1, NA, NA, 3), x = c(1:4))\ndf2 &lt;- data.frame(id = c(1, NA, NA), y = c(11:13))\n\n# Kết quả không mong muốn\ndf1 %&gt;% left_join(df2)\n\n  id x  y\n1  1 1 11\n2 NA 2 12\n3 NA 2 13\n4 NA 3 12\n5 NA 3 13\n6  3 4 NA\n\n# Kết quả đúng\ndf1 %&gt;% left_join(df2, na_matches = \"never\")\n\n  id x  y\n1  1 1 11\n2 NA 2 NA\n3 NA 3 NA\n4  3 4 NA\n\n\n\n\n4.3.2.3 Ghép nhiều bảng theo dòng hoặc cột với nhóm hàm bind\nBên cạnh các hàm join, khi xử lý dữ liệu trong thực tiễn, ta có thể phải ghép các bảng dữ liệu theo hàng hoặc cột. Trong dplyr, có hai hàm rất hữu dụng trong hai trường hợp trên là bind_col và bind_rows\n\nbind_cols(data1, data2)\nbind_rows(data1, data2)\n\nXem hai ví dụ sau.\n\ndf1 &lt;- data.frame(id = 1:3,\n                  income = 8:10)\ndf2 &lt;- data.frame(id = 4:9,\n                  income = 8:13)\ndf3 &lt;- data.frame(id = 1:3,\n                  gender = c(\"F\", \"F\", \"M\"))\n\n# Nối theo dòng\ndf1 %&gt;% bind_rows(df2)\n\n  id income\n1  1      8\n2  2      9\n3  3     10\n4  4      8\n5  5      9\n6  6     10\n7  7     11\n8  8     12\n9  9     13\n\n# Nối theo cột\ndf1 %&gt;% bind_cols(df3)\n\n  id...1 income id...3 gender\n1      1      8      1      F\n2      2      9      2      F\n3      3     10      3      M\n\n\n\n\n\n4.3.3 Nhóm hàm tổng hợp dữ liệu với summarise\nTrong quá trình xử lý dữ liệu, ta thường xuyên phải tổng hợp dữ liệu theo các cách như: tính tổng, tính số dư bình quân, phương sai, tổng số lượng quan sát… Với dplyr, ta có thể sử dụng hàm summarise() để thực hiện công việc này.\n\ndata %&gt;% \n  summarise(var_name = calculate_stats(var))\n\n\nmtcars %&gt;% \n  summarise(mean_mpg = mean(mpg),\n            sd_mpg = sd(mpg))\n\n  mean_mpg   sd_mpg\n1 20.09062 6.026948\n\n\nĐây là ví dụ đơn giản nhất với summarise mà ta có thể thay thế bằng summary() trên R base. Tuy nhiên, kết hợp giữa hàm summarise() và hàm group_by() trên dplyr sẽ cho chúng ta có cái nhìn về dữ liệu tổng hợp một cách đa chiều hơn. Hàm group_by() cho phép dữ liệu tổng hợp được gộp lại theo một hoặc nhiều trường thông tin khác nhau, giúp người phân tích có thể nhìn dữ liệu theo từ chiều riêng biệt hoặc gộp các chiều thông tin với nhau.\n\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarise(mean_mpg = mean(mpg),\n            mean_disp = mean(disp))\n\n# A tibble: 3 × 3\n    cyl mean_mpg mean_disp\n  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     4     26.7      105.\n2     6     19.7      183.\n3     8     15.1      353."
  },
  {
    "objectID": "p02-02-bien-doi-du-lieu-dplyr.html#các-hàm-nâng-cao-trong-dplyr",
    "href": "p02-02-bien-doi-du-lieu-dplyr.html#các-hàm-nâng-cao-trong-dplyr",
    "title": "4  Ngữ pháp của biến đổi dữ liệu với DPLYR",
    "section": "4.3 Các hàm nâng cao trong dplyr",
    "text": "4.3 Các hàm nâng cao trong dplyr\nBên cạnh các nhóm hàm cơ bản đã trình bày ở phần trên, dplyr còn có một số hàm nâng cao khác đặc biệt hữu dụng trong quá trình biến đổi, tổng hợp dữ liệu, bao gồm case_when, mutate_at & summarise_at\n\n4.3.1 Điều kiện phân nhóm với case_when\nTrong quá trình phân tích và xử lý dữ liệu, chúng ta thường phải tạo thêm các trường mới hoặc tính toán dữ liệu dựa vào từng điều kiện khác nhau để đưa ra giá trị của trường hoặc cách tính cho dữ liệu. Ví dụ, khi ta muốn tính thưởng cho KH thì sẽ phải dùng nhiều công thức khác nhau như KH thuộc VIP sẽ nhân 1 tỷ lệ, KH thuộc nhóm trung bình sẽ có 1 tỷ lệ khác, hay KH thông thường thì sẽ 1 tỷ lệ khác….\nTrong dplyr, hàm case_when() xử lý các trường hợp trên rất nhanh chóng.\n\ndata %>% mutate(new_var = case_when(\n                   condition_1 ~ \"value_1\",\n                   condition_2 ~ \"value_2\",...,\n                   TRUE ~ \"value_n\"\n                 ))\n\nTa xem ví dụ sau:\n\ndf <- data.frame(number = 1:10) \n\ndf %>% mutate(nhom = case_when(\n  number <= 5 ~ \"nhom_1\", # nhóm 1: số từ 1 đến 5\n  number > 5 & number <= 8 ~ \"nhom_2\", # nhóm 2: số từ 6 đến 8\n  TRUE ~ \"nhom_3\" # các số còn lại\n  ))\n\n   number   nhom\n1       1 nhom_1\n2       2 nhom_1\n3       3 nhom_1\n4       4 nhom_1\n5       5 nhom_1\n6       6 nhom_2\n7       7 nhom_2\n8       8 nhom_2\n9       9 nhom_3\n10     10 nhom_3\n\n\n\n\n4.3.2 Tạo thêm biến mới theo điều kiện với mutate_if & mutate_at\nKhi phân tích, ta có thể tạo thêm biến mới khi các biến trong dataframe thỏa mãn điều kiện nào đó.\n\ndata %>% mutate_if(condition, function)\n\n\ndf <- data.frame(\n  id = 1:5,\n  gender = c(\"F\", \"M\", \"M\", \"F\", \"F\"),\n  income = c(4,5,3,6,7)\n)\ndf %>% summary\n\n       id       gender              income \n Min.   :1   Length:5           Min.   :3  \n 1st Qu.:2   Class :character   1st Qu.:4  \n Median :3   Mode  :character   Median :5  \n Mean   :3                      Mean   :5  \n 3rd Qu.:4                      3rd Qu.:6  \n Max.   :5                      Max.   :7  \n\n# Biến đổi các biến factor thành character\ndf %>% mutate_if(is.factor, \n                 as.character) %>% \n  summary\n\n       id       gender              income \n Min.   :1   Length:5           Min.   :3  \n 1st Qu.:2   Class :character   1st Qu.:4  \n Median :3   Mode  :character   Median :5  \n Mean   :3                      Mean   :5  \n 3rd Qu.:4                      3rd Qu.:6  \n Max.   :5                      Max.   :7  \n\n\nNgoài ra, ta có thể tự tạo các hàm mới và áp dụng với mutate_if. Xem ví dụ sau.\n\nmy_func <- function(x){x*100}\n# Nhân các biến numeric lên 100 lần\ndf %>% \n  mutate_if(is.numeric, my_func)\n\n   id gender income\n1 100      F    400\n2 200      M    500\n3 300      M    300\n4 400      F    600\n5 500      F    700\n\n\nĐối với mutate_at, ta cũng có thể thực hiện tương tự. Cấu trúc tổng quát của mutate_at như sau.\n\ndata %>% \n  mutate_at(vars(var1, var2, ...), function())\n\n\ndf \n\n  id gender income\n1  1      F      4\n2  2      M      5\n3  3      M      3\n4  4      F      6\n5  5      F      7\n\n# Nhân biến income lên 100 lần\ndf %>% mutate_at(vars(income), my_func)\n\n  id gender income\n1  1      F    400\n2  2      M    500\n3  3      M    300\n4  4      F    600\n5  5      F    700\n\n\n\n\n4.3.3 Tổng hợp dữ liệu theo điều kiện với summarise_at và summarise_if\nTương tự như mutate_at và mutate_if, ta có thể tổng hợp nhanh dữ liệu theo điều kiện.\nCấu trúc tổng quát của summarise_at\n\ndata %>% \n     group_by(var) (không bắt buộc)\n     summarise_at(vars(variables), funs(functions))\n\nXem ví dụ sau\n\nmtcars %>% \n  group_by(am) %>% \n  summarise_at(vars(mpg, disp),\n               funs(mean, max, median))\n\n# A tibble: 2 × 7\n     am mpg_mean disp_mean mpg_max disp_max mpg_median disp_median\n  <dbl>    <dbl>     <dbl>   <dbl>    <dbl>      <dbl>       <dbl>\n1     0     17.1      290.    24.4      472       17.3        276.\n2     1     24.4      144.    33.9      351       22.8        120.\n\n\nTương tự, ta có cấu trúc tổng quát của summarise_if\n\ndata %>% \n   group_by(var) (không bắt buộc)\n   summarise_if(condition, funs(functions))\n\n\niris %>% \n  select(Species, Sepal.Length, Sepal.Width) %>% \n  group_by(Species) %>% \n  summarise_if(is.numeric,\n               funs(mean, median))\n\n# A tibble: 3 × 5\n  Species    Sepal.Length_mean Sepal.Width_mean Sepal.Length_median\n  <fct>                  <dbl>            <dbl>               <dbl>\n1 setosa                  5.01             3.43                 5  \n2 versicolor              5.94             2.77                 5.9\n3 virginica               6.59             2.97                 6.5\n# ℹ 1 more variable: Sepal.Width_median <dbl>\n\n\n\n\n4.3.4 Tính toán theo hàng\nTrong quá trình tính toán dữ liệu, đôi khi ta phải tính toán theo hàng, không phải côt. Khi đó, cần chuyển cấu trúc theo dạng `rowwise``\n\nlibrary(dplyr)\ndf <- data.frame(x = 1:2, y = 3:4, z = 5:6)\n# Đẻ thông thường không hoạt động\ndf %>% \n  mutate(sum = sum(x, y, z))\n\n  x y z sum\n1 1 3 5  21\n2 2 4 6  21\n\n# Sử dụng rowwise\ndf %>% \n  rowwise() %>% \n  mutate(sum = sum(x, y, z),\n         mean = mean(x:z))\n\n# A tibble: 2 × 5\n# Rowwise: \n      x     y     z   sum  mean\n  <int> <int> <int> <int> <dbl>\n1     1     3     5     9     3\n2     2     4     6    12     4"
  },
  {
    "objectID": "p02-02-bien-doi-du-lieu-dplyr.html#các-tricks-khác",
    "href": "p02-02-bien-doi-du-lieu-dplyr.html#các-tricks-khác",
    "title": "4  Ngữ pháp của biến đổi dữ liệu với DPLYR",
    "section": "4.4 Các tricks khác",
    "text": "4.4 Các tricks khác\n\n4.4.1 DPLYR vs. SQL\nTrong quá trình học DPLYR hoặc SQL, có thể sử dụng các packages sau để chuyển đổi qua lại giữa các ngôn ngữ:\n\ntidyquery để chuyển đổi từ SQL sang dplyr\n\ndbplyr để chuyển đổi từ dplyr sang SQL\n\n\nChuyển đổi dplyr sang SQL\n\nlibrary(dplyr)\nlibrary(dbplyr)\n\n# Copy dataframe sang SQLite\niris_df &lt;- iris %&gt;% memdb_frame()\n\niris_df %&gt;% \n  group_by(Species) %&gt;% \n  summarise(no = n(),\n            mean_sepal = mean(Sepal.Length)) %&gt;% \n  show_query()\n\n&lt;SQL&gt;\nSELECT `Species`, COUNT(*) AS `no`, AVG(`Sepal.Length`) AS `mean_sepal`\nFROM `dbplyr_001`\nGROUP BY `Species`\n\n\n\nChuyển đổi từ code SQL sang dplyr\n\nlibrary(tidyquery)\n\ncode &lt;- \"select avg(`Sepal.Length`) as avg_sepal_length\n        , count(*) as NO\n        from iris\n        group by `Species`\"\n\ncode %&gt;% query\n\n# A tibble: 3 × 2\n  avg_sepal_length    NO\n             &lt;dbl&gt; &lt;int&gt;\n1             5.01    50\n2             5.94    50\n3             6.59    50\n\ncode %&gt;% show_dplyr()\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(mean(Sepal.Length, na.rm = TRUE), dplyr::n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(avg_sepal_length = `mean(Sepal.Length, na.rm = TRUE)`, NO = `dplyr::n()`) %&gt;%\n  select(avg_sepal_length, NO)"
  },
  {
    "objectID": "p02-03-phan-ra-va-xoay-chieu-du-lieu.html#phân-rã-và-xoay-chiều-dữ-liệu",
    "href": "p02-03-phan-ra-va-xoay-chieu-du-lieu.html#phân-rã-và-xoay-chiều-dữ-liệu",
    "title": "5  Làm sạch và biến đổi dữ liệu nâng cao",
    "section": "5.1 Phân rã và xoay chiều dữ liệu",
    "text": "5.1 Phân rã và xoay chiều dữ liệu\nKhi phân tích dữ liệu, dữ liệu sau khi được làm sạch thường cơ bản có hai dạng.\n\nDạng ngang: Mỗi dòng ứng với 1 quan sát và nhiều biến\nDạng dọc: Nhiều dòng có thể chứa cùng một quan sát nhưng với các biến khác nhau.\n\nXem hai ví dụ về dữ liệu dạng ngang và dọc ở dưới đây.\n\n\n  id Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n1  1  setosa          5.1         3.5          1.4         0.2\n2  2  setosa          4.9         3.0          1.4         0.2\n3  3  setosa          4.7         3.2          1.3         0.2\n4  4  setosa          4.6         3.1          1.5         0.2\n5  5  setosa          5.0         3.6          1.4         0.2\n6  6  setosa          5.4         3.9          1.7         0.4\n\n\n\n\n  id Species  Measurement Value\n1  1  setosa Sepal.Length   5.1\n2  1  setosa  Sepal.Width   3.5\n3  1  setosa Petal.Length   1.4\n4  1  setosa  Petal.Width   0.2\n5  2  setosa Sepal.Length   4.9\n6  2  setosa  Sepal.Width   3.0\n7  2  setosa Petal.Length   1.4\n8  2  setosa  Petal.Width   0.2\n\n\nTrong thực tế, chúng ta phải sử dụng rất linh hoạt cả hai định dạng dữ liệu này (dữ liệu ngang và dữ liệu dọc). Trong chương này, chúng ta sẽ học cách sử dụng và biến đổi dữ liệu giữa hai định dạng với tidyr.\n\n5.1.1 Phân rã dữ liệu thành dạng dọc với gather\n\nlibrary(tidyr)\ndata %>% \n  gather(key = name_of_key,\n         value = name_of_value_variable,\n         gather = c(list_of_var))\n\nXem ví dụ sau.\n\nlibrary(dplyr)\ndf <- iris %>% \n  head(2) %>% \n  mutate(id = 1:nrow(.)) %>% \n  select(6, 5, 1:4) \ndf  \n\n  id Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n1  1  setosa          5.1         3.5          1.4         0.2\n2  2  setosa          4.9         3.0          1.4         0.2\n\n# Xoay dữ liệu sang dạng dọc\ndf2 <- df %>% \n  gather(key = Measurement, \n       value = Value, \n       c(3:6)) # Các biến được phân rã\ndf2\n\n  id Species  Measurement Value\n1  1  setosa Sepal.Length   5.1\n2  2  setosa Sepal.Length   4.9\n3  1  setosa  Sepal.Width   3.5\n4  2  setosa  Sepal.Width   3.0\n5  1  setosa Petal.Length   1.4\n6  2  setosa Petal.Length   1.4\n7  1  setosa  Petal.Width   0.2\n8  2  setosa  Petal.Width   0.2\n\n\nỞ ví dụ trên, khi phân rã dữ liệu sang dạng dọc, các biến được phân rã là 4 biến ở vị trí từ 3 đến 6. Do dữ liệu gốc df chỉ có 2 quan sát, nên dữ liệu mới sau khi phân rã sẽ có 8 quan sát.\n\n\n5.1.2 Xoay chiều dữ liệu với spread\nNgược lại với phân rã dữ liệu là xoay chiều dữ liệu. Trong tidyr, ta có thể sử dụng hàm spread. Công thức tổng quát để xoay chiều dữ liệu như sau.\n\ndata %>% \n  #Biến được xoay thành cột\n  spread(key = key_variable, \n         # Biến giá trị\n         value = value_variable) \n\nTa quay trở lại ví dụ ở phần trước vói dữ liệu df2 đã được phân rã.\n\ndf2\n\n  id Species  Measurement Value\n1  1  setosa Sepal.Length   5.1\n2  2  setosa Sepal.Length   4.9\n3  1  setosa  Sepal.Width   3.5\n4  2  setosa  Sepal.Width   3.0\n5  1  setosa Petal.Length   1.4\n6  2  setosa Petal.Length   1.4\n7  1  setosa  Petal.Width   0.2\n8  2  setosa  Petal.Width   0.2\n\n\nTa có thể xoay chiều dữ liệu lại như sau.\n\ndf2 %>% \n  spread(key = Measurement, \n         value = Value)\n\n  id Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1  1  setosa          1.4         0.2          5.1         3.5\n2  2  setosa          1.4         0.2          4.9         3.0"
  },
  {
    "objectID": "p02-03-phan-ra-va-xoay-chieu-du-lieu.html#tách-một-biến-thành-nhiều-biến-với-separate",
    "href": "p02-03-phan-ra-va-xoay-chieu-du-lieu.html#tách-một-biến-thành-nhiều-biến-với-separate",
    "title": "5  Làm sạch và biến đổi dữ liệu nâng cao",
    "section": "5.2 Tách một biến thành nhiều biến với separate",
    "text": "5.2 Tách một biến thành nhiều biến với separate\nKhi phân tích dữ liệu, ta thường xuyên phải tách một biến thành nhiều biến. Khi đó, việc tách biến sẽ trở nên rất đơn giản với hàm separate. Công thức tổng quát của separate như sau:\n\ndata %>% \n  separate(var_to_spread, c(\"new_var1\", \n                            \"new_var2\", ...))\n\n\ndf <- data.frame(date = c(NA, \n                          \"2018-07-01\", \n                          \"2018-09-02\"))\ndf\n\n        date\n1       <NA>\n2 2018-07-01\n3 2018-09-02\n\n# Tách biến date thành 3 biến\ndf %>% \n  separate(date, c(\"year\", \"month\", \"date\"))\n\n  year month date\n1 <NA>  <NA> <NA>\n2 2018    07   01\n3 2018    09   02"
  },
  {
    "objectID": "p02-03-phan-ra-va-xoay-chieu-du-lieu.html#gộp-nhiều-biến-thành-một-biến-với-unite",
    "href": "p02-03-phan-ra-va-xoay-chieu-du-lieu.html#gộp-nhiều-biến-thành-một-biến-với-unite",
    "title": "5  Làm sạch và biến đổi dữ liệu nâng cao",
    "section": "5.3 Gộp nhiều biến thành một biến với unite",
    "text": "5.3 Gộp nhiều biến thành một biến với unite\nNgược lại với spread, ta có thể gộp nhiều biến thành một với unite. Công thức tổng quát của unite như sau.\n\ndata %>% \n  unite(new_var, var_1, var2,...)\n\nTrong đó, var_1, var_2 là tên các biến sẽ được gộp. new_var là tên biến mới được tạo thành.\nQuay trở lại ví dụ trên.\n\ndf <- data.frame(date = c(\"2018-07-01\", \"2018-09-02\")) %>%   separate(date, c(\"year\", \"month\", \"date\"))\ndf\n\n  year month date\n1 2018    07   01\n2 2018    09   02\n\n# Gộp nhiều biến\ndf %>% \n  unite(full_date, 1:3, \n        sep = \"/\",\n        remove = F)\n\n   full_date year month date\n1 2018/07/01 2018    07   01\n2 2018/09/02 2018    09   02"
  },
  {
    "objectID": "p02-03-phan-ra-va-xoay-chieu-du-lieu.html#các-trường-hợp-biến-đổi-nâng-cao",
    "href": "p02-03-phan-ra-va-xoay-chieu-du-lieu.html#các-trường-hợp-biến-đổi-nâng-cao",
    "title": "5  Làm sạch và biến đổi dữ liệu nâng cao",
    "section": "5.4 Các trường hợp biến đổi nâng cao",
    "text": "5.4 Các trường hợp biến đổi nâng cao\n\n5.4.1 Compete & fill trong chuỗi thời gian\nKhi làm việc với dữ liệu time series, đặc biệt với loại dữ liệu cổ phiếu, sẽ xuất hiện những ngày có dữ liệu bị thiếu.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\n\ndf <- data.frame(\n  date = 1e4 + c(1, 2, 4, 5, 8, 9, 10, 102, 105, 106, 108),\n  stock = c(rep(\"A\", 7), rep(\"B\", 4)),\n  price = c(runif(7, 10, 50), NA, 20, 35, 39)\n) %>% \n  mutate(date = as_date(date))\ndf\n\n         date stock    price\n1  1997-05-20     A 18.18917\n2  1997-05-21     A 49.13626\n3  1997-05-23     A 19.72709\n4  1997-05-24     A 33.20293\n5  1997-05-27     A 26.63366\n6  1997-05-28     A 29.81862\n7  1997-05-29     A 34.41163\n8  1997-08-29     B       NA\n9  1997-09-01     B 20.00000\n10 1997-09-02     B 35.00000\n11 1997-09-04     B 39.00000\n\n\nTrong trường hợp trên, để clean dữ liệu, ta cần làm 2 việc:\n\nBổ sung các ngày bị thiếu trong chuỗi thời gian, VD: 22/05/1997, 25-26/05/1997 (1)\nĐiền price theo giá của ngày gần nhất, thứ tự ưu tiên từ ngày liền kề gần nhất (2)\n\nĐối với việc 1, tidyr cho phép sử dụng hàm complete. Sau khi được bổ sung ngày bị thiếu, dữ liệu mới sẽ như sau:\n\ndf1 <- df %>% \n  group_by(stock) %>% \n  complete(date = seq.Date(min(date), max(date), by=\"day\"))\n\ndf1\n\n# A tibble: 17 × 3\n# Groups:   stock [2]\n   stock date       price\n   <chr> <date>     <dbl>\n 1 A     1997-05-20  18.2\n 2 A     1997-05-21  49.1\n 3 A     1997-05-22  NA  \n 4 A     1997-05-23  19.7\n 5 A     1997-05-24  33.2\n 6 A     1997-05-25  NA  \n 7 A     1997-05-26  NA  \n 8 A     1997-05-27  26.6\n 9 A     1997-05-28  29.8\n10 A     1997-05-29  34.4\n11 B     1997-08-29  NA  \n12 B     1997-08-30  NA  \n13 B     1997-08-31  NA  \n14 B     1997-09-01  20  \n15 B     1997-09-02  35  \n16 B     1997-09-03  NA  \n17 B     1997-09-04  39  \n\n\nVới nhóm việc số 2, có 2 trường hợp xảy ra:\n\nTồn tại dữ liệu của ngày trước đó - lấy dữ liệu theo ngày gần nhất trước khi bị thiếu. VD: với cổ phí A, ngày 25-26/05 có thể lấy tham chiếu theo ngày 24/05\nKhông có dữ liệu ngày trước đó - lấy dữ liệu theo ngày gần nhất sau khi bị thiêu. VD: Với cổ phiếu B, từ ngày 01/09 mới có giá cổ phiếu và bị thiếu mất 3 ngày từ 29-31/08/1997. Trong trường hợp này cần phải lấy dữ liệu theo ngày gần nhất là 01/09.\n\nĐể giải quyết cả 2 trường hợp trên, tidyr cho phép sử dụng hàm fill với option .directipn = \"downup\".\n\ndf1 %>% \n  fill(price, .direction = \"downup\")\n\n# A tibble: 17 × 3\n# Groups:   stock [2]\n   stock date       price\n   <chr> <date>     <dbl>\n 1 A     1997-05-20  18.2\n 2 A     1997-05-21  49.1\n 3 A     1997-05-22  49.1\n 4 A     1997-05-23  19.7\n 5 A     1997-05-24  33.2\n 6 A     1997-05-25  33.2\n 7 A     1997-05-26  33.2\n 8 A     1997-05-27  26.6\n 9 A     1997-05-28  29.8\n10 A     1997-05-29  34.4\n11 B     1997-08-29  20  \n12 B     1997-08-30  20  \n13 B     1997-08-31  20  \n14 B     1997-09-01  20  \n15 B     1997-09-02  35  \n16 B     1997-09-03  35  \n17 B     1997-09-04  39  \n\ndf1\n\n# A tibble: 17 × 3\n# Groups:   stock [2]\n   stock date       price\n   <chr> <date>     <dbl>\n 1 A     1997-05-20  18.2\n 2 A     1997-05-21  49.1\n 3 A     1997-05-22  NA  \n 4 A     1997-05-23  19.7\n 5 A     1997-05-24  33.2\n 6 A     1997-05-25  NA  \n 7 A     1997-05-26  NA  \n 8 A     1997-05-27  26.6\n 9 A     1997-05-28  29.8\n10 A     1997-05-29  34.4\n11 B     1997-08-29  NA  \n12 B     1997-08-30  NA  \n13 B     1997-08-31  NA  \n14 B     1997-09-01  20  \n15 B     1997-09-02  35  \n16 B     1997-09-03  NA  \n17 B     1997-09-04  39"
  },
  {
    "objectID": "p02-04-ngu-phap-bieu-do-ggplot2.html#giới-thiệu",
    "href": "p02-04-ngu-phap-bieu-do-ggplot2.html#giới-thiệu",
    "title": "6  Ngữ pháp của biểu đồ với GGPLOT2",
    "section": "6.1 Giới thiệu",
    "text": "6.1 Giới thiệu\nTrong lĩnh vực phân tích dữ liệu, bên cạnh việc biến đổi dữ liệu, kỹ năng trực quan hóa biểu đồ là kỹ năng đặc biệt quan trọng, giúp cho người đọc báo cáo, phân tích nắm được kết quả phân tích một cách đơn giản hơn.\nVới R, package đặc biệt quan trọng trong trực quan hóa là package ggplot2. Thư viện này được phát triển để thực hiện khái niệm ngữ pháp của biểu đồ và trở thành thư viện nổi tiếng nhất của R trong việc trực quan hóa dữ liệu. Một loạt thư viện khác được phát triển xung quanh ggplot2 và trở thành một tiểu hệ sinh thái giúp vẽ biểu đồ cực kỳ hiệu quả.\nKhái niệm “ngữ pháp của biểu đồ” cho rằng: Mỗi biểu đồ được tạo ra do 2 hay nhiều lớp chống lấn mà thành. Các lớp của biểu đồ bao gồm\n\nCác lớp (layer) cơ bản (bắt buộc phải có)):\n\nDữ liệu (data): Tập số liệu được sử dụng cho việc xây dựng biểu đồ.\nBiến (aesthetic attribute): Xác định các biến trong dữ liệu được sắp xếp như thế nào. VD: trục tung, trục hoành là biến nào? Màu sắc được sắp xếp theo biến nào? Độ lớn của mỗi điểm được xác định theo biến nào?\nTính chất hình học của biểu đồ (geometric object): xác định loại biểu đồ được sử dụng. VD: biểu đồ cột, biểu đồ điểm (scatter plot), biểu đồ dây (line chart)\n\nCác lớp (layer) nâng cao (không bắt buộc):\n\nFacet (tạm dịch: cách thức sắp xếp số liệu trên cùng 1 biểu đồ): Xác định cách thức sắp xếp vị trí của nhiều biểu đồ trên cùng một màn hình.\nTính toán thống kê (statistical transformation): Sử dụng các tính toán trong biểu đồ. VD: Thêm các biểu đồ xu hướng dạng trơ (geom_smooth) , số lượng bin trong histogram… Lớp này có thể nằm lồng trong ngay trong nhóm aesthetics\nVị trí (position): Xác định vị trí trong biểu đồ\nTrục tọa độ (Coordinate): Xác định hệ trục tọa độ vẽ biểu đồ. VD: Hệ tọa độ 3 trục trong không gian (hệ tọa đồ Descartes), hệ tọa độ cực\n\n\nCác lớp trong biểu đồ có thể thể hiện thành các lớp như sau \n\nViệc hiểu và nắm vững nguyên lý của ggplot2 sẽ cho phép chúng ta xây dựng gần như bất kỳ biểu đồ nào."
  },
  {
    "objectID": "p02-04-ngu-phap-bieu-do-ggplot2.html#xây-dựng-biểu-đồ",
    "href": "p02-04-ngu-phap-bieu-do-ggplot2.html#xây-dựng-biểu-đồ",
    "title": "6  Ngữ pháp của biểu đồ với GGPLOT2",
    "section": "6.2 Xây dựng biểu đồ",
    "text": "6.2 Xây dựng biểu đồ\n\n6.2.1 Biểu đồ cơ bản\nCâu lệnh cơ bản của ggplot như sau:\n\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nmpg %>% head\n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n\nggplot(data = mpg) + # Dữ liệu là tập mpt\n  geom_point(mapping = aes(x = displ, # Trục x là displ\n                           y = hwy,   # Trục y là hwy\n                           col = class)) # Color là class\n\n\n\n\nTrong ví dụ trên, ta đã vẽ biểu đồ điểm (geom_point) với tập dữ liệu mpg để phân tích quan hệ giữa hai biến displ và hwy và phân nhóm màu theo class.\nTa có thể vẽ biểu đồ với kích thước của từng điểm theo cyl như sau.\n\nggplot(data = mpg) + # Dữ liệu là tập mpt\n  geom_point(mapping = aes(x = displ, # Trục x là displ\n                           y = hwy,   # Trục y là hwy\n                           size = cyl),# Kích thước theo cyl\n             col = \"darkblue\")  # Tất cả các điểm có màu là blue\n\n\n\n\n\n\n6.2.2 Facet\nFacet cho phép chúng ta chia dữ liệu thành nhiều phần theo từng nhóm và xây dựng biểu đồ cho mỗi nhóm trong đó. Câu lệnh của facet như sau\n\nfacet_wrap( ~ group_variable, # Chia nhóm\n            nrow = 2,         # Số lượng dòng trong biểu đồ  \n            scale = \"free\")   # \"free\" cho phép biểu đồ auto scale\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, \n                           y = hwy),\n             col = \"darkblue\") + \n  facet_wrap(~ class, \n             nrow = 2,\n             scale = \"free\")\n\n\n\n\n\n\n6.2.3 Thuộc tính hình học - geom\nCác loại biểu đồ khác nhau trong ggplot2 chỉ khác nhau ở geom_ - thuộc tính hình học của biểu đồ. Mỗi geom sẽ có các tham số khác nhau cần được điều chỉnh.\nTrong ví dụ dưới đây, ta vẽ 2 loại biểu đồ chồng lên nhau gồm 1. biểu đồ điểm và 2. biểu đồ thể hiện xu hướng\n\nggplot(mpg, mapping = aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\nLưu ý: Khi có nhiều geom_object, nếu không có sự điều chỉnh khi mapping của từng geom, các lớp sau sẽ lấy các biến global để làm thuộc tính. Trong ví dụ trên, hai lớp geom_point và geom_smooth lấy giá trị mặc định là aes(displ, hwy)\n\nggplot(mpg, mapping = aes(displ, hwy)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(aes(linetype = drv, col = drv),\n              size = 1.2)\n\n\n\n\nTrong ggplot2 có rất nhiều geom phục vụ các loại biểu đồ khác nhau. Tuy nhiên, để phục vụ công viên thực tế, trong ggplot2 cần nắm vững các loại geom quan trọng nhất như sau.\n\ngeom_point: Biểu đồ điểm\ngeom_line: Biểu đồ đường\ngeom_boxplot: Biểu đồ boxplot\ngeom_col, geom_bar: Biểu đồ cột\ngeom_smooth: Biểu đồ vẽ xu hướng\ngeom_histogram, geom_density: Biểu đồ phân phối\n\n\n# geom_boxplot\nmpg %>% \n  ggplot(aes(class, hwy)) +\n  geom_boxplot()\n\n\n\n# geom_col\nmpg %>% \n  ggplot(aes(class)) +\n  geom_bar(aes(fill = drv))\n\n\n\n# geom_density\nmpg %>% \n  ggplot(aes(hwy)) +\n  geom_density()\n\n\n\n# geom_histogram\nmpg %>% \n  ggplot(aes(hwy)) +\n  geom_histogram()\n\n\n\n\n\n\n6.2.4 Tính toán thống kê - statistical transformations\nĐối với lớp tính toán thống kê (statistical transformation), biểu đồ không được vẽ trực tiếp với 2 trục tung và trục hoành mà sẽ phải trải qua bước tính toán thống kê trước.\nXem ví dụ biểu đồ cột dưới đây.\n\nggplot(diamonds) +\n  geom_bar(mapping = aes(x = cut))\n\n\n\n\nỞ ví dụ này, trục x là các nhóm của cut trong khi trục y là số lượng quan sát ở mỗi nhóm, giá trị này không phải là biến trực tiếp từ dữ liệu mà là 1 biến phái sinh. Quá trình xây dựng biểu đồ được thực hiện qua 3 bước như sau.\n\nTa cũng có thể thay đổi lớp biến đổi dữ liệu trong biểu đồ trên để hiển thị dạng tỷ lệ phần trăm như sau.\n\nggplot(diamonds) +\n  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1))\n\n\n\n\n\n\n6.2.5 Vị trí trong biểu đồ\nQuay lại ví dụ bar chart phía trên, khi xây dựng biểu đồ có 2 hoặc nhiều nhóm, biểu đồ có thể có 3 kiểu vị trí như sau:\n\nposition = \"identity\" (default): Giữ nguyên chế độ mặc định, để dạng stack bar với giá trị trục y mặc định\nposition = \"fill\": Tương tự “identity” nhưng mỗi cột sẽ có tỷ trọng bằng nhau\nposition = \"dodge\": Phân nhóm biểu đồ\n\nXem ví dụ dưới đây.\n\n# position = \"identity\"\nggplot(diamonds) +\n  geom_bar(mapping = aes(x = cut, fill = clarity))\n\n\n\n# position = \"fill\"\nggplot(diamonds) +\n  geom_bar(mapping = aes(x = cut, fill = clarity), \n           position = \"fill\")\n\n\n\n# position = \"dodge\"\nggplot(diamonds) +\n  geom_bar(mapping = aes(x = cut, fill = clarity), \n           position = \"dodge\")\n\n\n\n\nBên cạnh 3 nhóm position trên, biểu đồ điểm còn có nhóm riêng là postion = \"jitter\". Option này cho phép thay đổi giá trị của mỗi điểm một lượng nhỏ để biểu đồ điểm không bị trùng lặp nhiều quan sát (khi nhiều quan sát có chung một giá trị). Nhóm này có thể sử dụng tương đương với geom_jitter.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  labs(title = \"Without position jitter\")\n\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), \n             position = \"jitter\") +\n  labs(title = \"Position jitter\")\n\n\n\n\n\n§\n\nĐối với biểu đồ miền (area chart), ta có thể sử dụng option position = \"fill\" để thể hiện dạng phần trăm.\n\nyear <- 1875:1972\ngroup <- c(\"A\", \"B\", \"C\")\ndf <- expand.grid(year, group) %>% \n  mutate(level = rnorm(nrow(.), 50, 10) %>% abs)\n\nnames(df) <- c(\"year\", \"group\", \"level\")\n\n# Lấy theo số tuyệt đối\nggplot(df, aes(year, level)) +\n  geom_area(aes(fill = group), position = \"stack\")\n\n\n\n# Lấy theo tỷ lệ\nggplot(df, aes(year, level)) +\n  geom_area(aes(fill = group), \n            position = position_fill())\n\n\n\n\n\n\n6.2.6 Hệ trục tọa độ\nBiểu đồ thông thường được xây trên hệ trục tọa độ Đề-các với tỷ lệ 1-1. Bên cạnh hệ trục này, ggplot2 hỗ trợ các hệ trục tọa độ khác nhưng thông dụng nhất là coord_flip, cho phép đổi vị trí hai trục tọa độ cho nhau\n\niris %>% \n  ggplot(aes(Species, Sepal.Length)) +\n  geom_boxplot(aes(fill = Species)) +\n  coord_flip()\n\n\n\n\n\n\n6.2.7 Các yếu tố khác\nBên cạnh các lớp biểu đồ đã phân tích trên, khi xây dựng biểu đồ với ggplot2, còn 2 nhóm câu lệnh thường sử dụng:\n\nlabs: Đặt tên bản đồ, chú giải, tên các trục\ntheme: Sử dụng để có bản đồ đẹp và phù hợp hơn\n\n\n6.2.7.1 Labs\n\nlabs(title = \"title\",\n     subtitle = \"subtitle\",\n     x = \"Trục x\",\n     y = \"Trục y\",\n     caption = \"Footnote\")\n\n\np <- mpg %>% \n  ggplot(aes(displ, hwy)) +\n  geom_point(col = \"darkblue\", alpha = 0.5, position = \"jitter\", size = 2) +\n  labs(title = \"Highway miles per gallon by Engine displacement\",\n       subtitle = \"From mpg data set\",\n       x = \"Engine displacement\", \n       y = \"Highway miles per gallon\",\n       caption = \"Source: data science team\")\np\n\n\n\n\n\n\n6.2.7.2 Theme\nKhi xây dựng biểu đồ với ggplot2, ta có thể sử dụng rất nhiều theme có sẵn nhưng được dùng nhiều nhất là theme_bw(), theme_classic(), theme_minimal()\n\np <- iris %>% \n  ggplot(aes(Species, Sepal.Length)) +\n  geom_boxplot(aes(fill = Species), show.legend = F)\np1 <- p + labs(title = \"Default theme\")\n\np2 <- p + labs(title = \"Theme black & white\") +\n  theme_bw()\n\np3 <- p + labs(title = \"Theme classic\") +\n  theme_classic()\n\np4 <- p + theme_minimal() + labs(title = \"Theme minimal\")\n\np1 + p2 + p3 + p4 + plot_layout(ncol = 2)\n\n\n\n\nĐể tùy chỉnh các thành phần có sẵn trong biểu đồ, ta có thể sử dụng hàm theme để customize từng thành phần như sau.\n\np + theme(\n  # Tên title\n  axis.title = element_text(family = \"serif\", face = \"bold\"),\n  # Tên của trục x và y\n  axis.title.x = element_text(family = \"serif\", face = \"bold\"),\n  axis.title.y = element_text(family = \"serif\", face = \"bold\"),\n  # Chữ trên trục\n  axis.text = element_text(face = \"italic\"),\n  # Xóa phần nền trong panel\n  panel.background = element_blank()\n)\n\n\n\n\nĐể xem các yếu tố có thể tùy chỉnh trong theme, ta có thể sử dụng câu lệnh ?theme\n’### Thay đổi màu trong biểu đồ\nHệ mã màu trong ggplot2 được chia làm 2 nhóm chính:\n\ncolor: Với các nhóm biểu đồ điểm, đường (point, line)\nfill: Với các biểu đồ như biểu đồ cột, miền, boxplot\n\nBên cạnh đó, dải màu trong ggplot2 cũng được chia làm 2 nhóm\n\nBiến liên tục\nBiến rời rạc\n\nVới biến màu rời rạc\n\n# Điều chỉnh màu thủ công\np + \n  scale_fill_manual(values = c(\"836214\", \"brown\", \"#178349\"))\n\n\n\n# Điều chỉnh màu theo nhóm có sẵn với scale_fill_brewer\np + \n  scale_fill_brewer(palette = \"Set1\") + \n  theme_minimal()\n\n\n\n# Điều chỉnh scale_color\nlibrary(ggthemes)\niris %>% \n  ggplot(aes(Sepal.Length, Sepal.Width)) +\n  geom_point(aes(col = Species)) +\n  theme_classic() +\n  theme(legend.position = \"top\") +\n  scale_color_tableau()\n\n\n\n\nVới biến màu liên tục: Biến màu liên tục được thể hiện tốt nhất với dải màu từ package viridis như sau\n\nlibrary(viridis)\niris %>% \n  ggplot(aes(Sepal.Length, Sepal.Width)) +\n  geom_point(aes(col = Petal.Width)) +\n  theme_classic() +\n  theme(legend.position = \"top\") +\n  scale_color_viridis()"
  },
  {
    "objectID": "p02-04-ngu-phap-bieu-do-ggplot2.html#các-mẹo-với-ggplot2",
    "href": "p02-04-ngu-phap-bieu-do-ggplot2.html#các-mẹo-với-ggplot2",
    "title": "6  Ngữ pháp của biểu đồ với GGPLOT2",
    "section": "6.3 Các mẹo với ggplot2",
    "text": "6.3 Các mẹo với ggplot2\n\n6.3.1 Đặt global option cho theme\nKhi bắt đầu project, ta có thể đặt một theme để sử dụng với tất cả biểu đồ với hàm theme_set.\n\ntheme_set(theme_classic() +\n          theme(legend.position = \"top\"))\n\n\n\n6.3.2 Loại bỏ legend trong 1 lớp\n\n# Để cả 2 legend\niris %>% \n  ggplot(aes(Sepal.Length, Sepal.Width)) +\n  geom_point(aes(col = Species)) +\n  geom_smooth(aes(col = Species))\n\n\n\n# Loại bỏ 1 legend\n\niris %>% \n  ggplot(aes(Sepal.Length, Sepal.Width)) +\n  geom_point(aes(col = Species)) +\n  geom_smooth(aes(col = Species), show.legend = F)\n\n\n\n\n\n\n6.3.3 Format dữ liệu trong biểu đồ với scales\nVới các kiểu đơn vị khác nhau trong biểu đồ, khi xây dựng biểu đồ ta cần tùy chỉnh để dữ liệu hiển thị tốt nhất. Bên cạnh kiểu dữ liệu factor và dạng numeric thông thường, ta cần quan tâm đến 3 kiểu hiển thị dữ liệu sau.\n\nHiển thị kiểu dữ liệu tiền tệ\nHiển thị kiểu dữ liệu phần trăm\nHiển thị kiểu dữ liệu thời gian\n\n\nlibrary(tidyverse)\nlibrary(scales)\n# Sử dụng theme_bw cho tất cả các biểu đồ\ntheme_set(theme_minimal() +\n            theme(legend.position = \"top\"))\n\n# Tạo biểu đồ cơ bản\np <- ggplot(cars, aes(x = speed, y = dist)) + \n  geom_point(col = \"darkblue\", alpha = 0.5, size = 3)\np\n\n\n\n# Scale trục y với dạng percentage\np + scale_y_continuous(label = percent)\n\n\n\n# Scale trục y với dạng dollar\n\np + scale_y_continuous(labels = dollar)\n\n\n\n\nĐối với biểu đồ hiển thị dạng thời gian, ta cần kết hợp linh động hàm format với cấu trúc dữ liệu thời gian trong R strptime\n\ndf <- data.frame(\n  date = seq(12995, 13100, by = 1) %>% lubridate::as_date()) \ndf <- df %>% \n  mutate(id = 1:nrow(.)) %>% \n  mutate(price = id * 3 + 10*sin(id) + rnorm(nrow(.),100,50)) %>% \n  select(-id)\n\np <- ggplot(data=df, \n             aes(x=date, y=price)) + \n  geom_line(col = \"darkblue\", size = 0.3) \n\ndf %>% head\n\n        date     price\n1 2005-07-31 109.95362\n2 2005-08-01 185.65910\n3 2005-08-02 201.22057\n4 2005-08-03  37.19954\n5 2005-08-04  55.16117\n6 2005-08-05  88.24374\n\n# Break theo tháng, format dạng tháng hiển thị số, năm\np + scale_x_date(breaks = date_breaks(\"month\"),\n  labels = date_format(\"%m/%y\")) \n\n\n\n# Break theo tháng, format dạng tháng hiển thị chữ, năm\np + scale_x_date(breaks = date_breaks(\"month\"),\n  labels = date_format(\"%b,%y\")) \n\n\n\n\nĐể có thể tùy biến các định dạng ngày tháng trong R, ta kiểm tra các định dạng format qua câu lệnh\n\n?strptime\n\n\n\n6.3.4 Làm nổi bật biểu đồ với annotate\nTrong biểu đồ, để tạo hiệu ứng tốt nhất, ta có thể thêm các vùng highlight để làm nổi bật biểu đồ bằng cách sử dụng annotate.\nHàm annotate tương tự như geom nhưng khác biệt ở chỗ, tham số của annotate được thêm vào dưới dạng vector.\n\nggplot(cars, aes(x = speed, y = dist)) + \n    geom_point(col = \"darkblue\", alpha = 0.8, size = 3) +\n  annotate(xmin = 15, xmax = Inf, ymin = 0, ymax = Inf,\n           geom = \"rect\", fill = \"darkred\", alpha = 0.2)\n\n\n\n\n\n\n6.3.5 Hiển thị text biểu đồ ở giữa bar chart\n\nSử dụng option position = position_stack(vjust = 0.5)\n\n\ndf <- mpg %>% \n  group_by(class) %>% \n  summarise(no = n())\ndf %>% \n  ggplot(aes(class, no)) +\n  geom_bar(stat = \"identity\", fill = \"darkblue\", alpha = 0.6) +\n  geom_text(aes(label = no), position = position_stack(vjust = 0.5))"
  },
  {
    "objectID": "p02-04-ngu-phap-bieu-do-ggplot2.html#các-package-bổ-trợ-cho-ggplot2",
    "href": "p02-04-ngu-phap-bieu-do-ggplot2.html#các-package-bổ-trợ-cho-ggplot2",
    "title": "6  Ngữ pháp của biểu đồ với GGPLOT2",
    "section": "6.4 Các package bổ trợ cho ggplot2",
    "text": "6.4 Các package bổ trợ cho ggplot2\n\n6.4.1 Kết hợp nhiều biểu đồ với patchwork\nKhi trực quan hóa dữ liệu, ta có thể kết hợp nhiều biểu đồ với nhau để thành 1 biểu đồ duy nhất. Để kết hợp các biểu đồ của ggplot2, ta có thể sử dụng package patchwork.\n\nlibrary(ggthemes)\np1 <- iris %>% \n  ggplot(aes(Species, Sepal.Length)) +\n  geom_boxplot(aes(fill = Species), show.legend = F) +\n  scale_fill_tableau()\np2 <- iris %>% \n  ggplot(aes(Sepal.Length, Sepal.Width)) +\n  geom_point(aes(col = Species), show.legend = F) +\n  scale_color_tableau()\np3 <- iris %>% \n  ggplot(aes(Sepal.Length)) +\n  geom_density(aes(fill = Species), alpha = 0.7, show.legend = F) +\n  scale_fill_tableau()\n\n\n# Kết hợp 2 biểu đồ với nhau\np1 + p2 \n\n\n\n# Kết hợp 1 biểu đồ bên cạnh, 2 biểu đồ ở trên\np1 + (p2/p3)\n\n\n\n\nNgoài ra, để điều chỉnh kích thước của từng biểu đồ, ta có thể dùng hàm plot_layout.\n\np1 + coord_flip() +\n  (p2 + p3 + plot_layout(ncol = 1)) +\n  plot_layout(widths = c(2,3))\n\n\n\n\n\n\n6.4.2 Sử dụng ggtext để hiển thị dạng markdown\nggtext cho phép highlight các đoạn text trong biểu đồ theo cấu trúc ngữ pháp của markdown. Để sử dụng, ta đi qua 2 bước:\n\nÁp dụng cấu trúc markdown tại thành phần cần highlight\nSử dụng element_markdown trong bước trên\n\n\nlibrary(ggplot2)\nlibrary(ggtext)\ncars %>% \n  ggplot(aes(speed, dist)) +\n  geom_point(size = 3, col = \"darkblue\", alpha = 0.5) +\n  theme_minimal() +\n  labs(title = \"Sample **chart**\",\n       subtitle = \"Speed is *correlated* with distance\",\n       caption = \"Made by ***Analytics team***\") +\n  theme(plot.title = element_markdown(),\n        plot.caption = element_markdown(),\n        plot.subtitle = element_markdown())\n\n\n\n\n\n\n6.4.3 Highlight biểu đồ với gghighlight\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gghighlight)\n\nmy_theme <- function(){\n  list(\n      theme_bw(),\n      scale_fill_brewer(palette = \"Set1\"),\n      scale_color_brewer(palette = \"Set1\")\n    )\n}\n\n\n# Với biểu đồ bar\n\ndata <- mtcars %>% mutate(cyl = factor(cyl))\n\ndata %>% \n  group_by(cyl) %>% \n  summarise(mpg = sum(mpg)) %>% \n  ggplot(aes(cyl, mpg)) +\n  geom_bar(aes(fill = cyl), stat = \"identity\") +\n  geom_text(aes(label = mpg),\n                       hjust = -0.3) +\n  my_theme() +\n  coord_flip() +\n  scale_y_continuous(limits = c(0, 320)) +\n  gghighlight(cyl == 6) +\n  labs(title = \"Hightlight with text\")\n\n\n\n# Với boxplot\n\niris %>% \n  ggplot(aes(Species, Sepal.Length)) +\n  geom_boxplot(aes(fill = Species)) +\n  my_theme() +\n  facet_wrap(~Species) +\n  gghighlight() +\n  theme(legend.position = \"top\") +\n  labs(title = \"Highlight for box plot\")\n\n\n\n# Với biểu đồ điểm\n\ndf <- mtcars %>% mutate(name = row.names(.))\ndf %>% \n  ggplot(aes(mpg, disp)) +\n  geom_point(col = \"darkred\") +\n  my_theme() +\n  gghighlight(disp > 350 & disp <= 400,\n              unhighlighted_colour = alpha(\"darkgreen\", 0.4), # Màu với các điểm không highlight\n              use_direct_label = T,\n              label_key = name,\n              label_params = list(size = 5)) +\n  geom_point(col = \"darkred\", size = 3) +\n  labs(title = \"Example 5 - Hightlight points\")"
  },
  {
    "objectID": "p02-04-ngu-phap-bieu-do-ggplot2.html#tài-liệu-tham-khảo",
    "href": "p02-04-ngu-phap-bieu-do-ggplot2.html#tài-liệu-tham-khảo",
    "title": "6  Ngữ pháp của biểu đồ với GGPLOT2",
    "section": "6.5 Tài liệu tham khảo",
    "text": "6.5 Tài liệu tham khảo\n\nR for Data Science - Chapter 3 - Data Visualisation\n[Modern data science with R - Chapter 3 - A grammar for graphics]"
  },
  {
    "objectID": "p02-05-cac-chi-so-thong-ke.html#correlation",
    "href": "p02-05-cac-chi-so-thong-ke.html#correlation",
    "title": "7  Các chỉ số thống kê mô tả cơ bản",
    "section": "7.1 Correlation",
    "text": "7.1 Correlation\nHệ số tương quan\nHệ số tương quan (correlation coefficient) là một chỉ số thống kê, được sử dụng để đo lường mối liên hệ tương quan tuyến tính giữa 2 biến định lượng hay còn gọi là biến liên tục (quantitative/continuous variables). Hay nói một cách dễ hiểu hơn, hệ số tương quan cho ta biết rằng giữa 2 biến liên tục có mối liên hệ nào hay không.\nHệ số tương quan có giá trị từ -1 đến 1. Dấu “+” hoặc “-” của hệ số tương quan cho ta biết 2 biến có mối liên hệ tương quan tuyến tính cùng chiều hoặc ngược chiều tương ứng. Nếu hệ số tương quan của 2 biến mang dấu “+” có nghĩa là khi biến này tăng thì biến kia cũng tăng, còn nếu hệ số tương quan của 2 biến mang dấu “-” thì có nghĩa là khi biến này tăng thì biến kia sẽ giảm.\nHệ số tương quan của 2 biến bằng 0 có nghĩa là 2 biến không có mối liên hệ tương quan tuyến tính (no relationships), hệ số tương quan bằng -1 hoặc 1 có nghĩa là 2 biến có mối liên hệ tương quan tuyến tính tuyệt đối.\nTiếp tục sử dụng dữ liệu iris, chúng ta tính toán hệ số tương quan của từng cặp biến chiều dài, chiều rộng cánh hoa và đài hoa của các loài hoa để xem 4 biến trên có mối liên hệ với nhau như thế nào. Để tính hệ số tương quan trong R, chúng ta có thể sử dụng hàm cor().\n\ncor(iris %>% select(1:4))\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nKết quả cho ta thấy Sepal.Length và Petal.Length có mối liên hệ tương quan tuyến tính cùng chiều (hệ số tương quan 0.87), trong khi đó, Sepal.Length và Sepal.Width có mối liên hệ tương quan tuyến tính ngược chiều (hệ số tương quan -0.11).\nKiểm định hệ số tương quan\nNhư vậy, chúng ta đã hiểu bản chất của hệ số tương quan và biết cách tính hệ số tương quan trong R. Câu hỏi đặt ra lúc này là làm thế nào để biết được thực sự hệ số tương quan được tính toán có ý nghĩa về mặt thống kê hay không, hay nói cách khác, giữa 2 biến có thực sự có mối liên hệ hay không. Vì vậy, chúng ta cần kiểm định hệ số tương quan.\nGiả sử, chúng ta muốn biết xem 2 biến Sepal.Length và Petal.Length có mối liên hệ hay không. Như đã tính toán ở phần trước, hệ số tương quan của 2 biến này là 0.87, chúng ta sẽ kiểm định xem hệ số tương quan này có ý nghĩa về mặt thống kê hay không, tức chúng ta cần kiểm định xem hệ số tương quan của 2 biến này có khác 0 hay không. Chúng ta có cặp giả thuyết sau:\n\nHo: 2 biến không có mối liên hệ (hệ số tương quan = 0)\nH1: 2 biến có mối liên hệ (hệ số tương quan khác 0)\n\n\ncor.test(iris$Sepal.Length,iris$Petal.Length)\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Petal.Length\nt = 21.646, df = 148, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8270363 0.9055080\nsample estimates:\n      cor \n0.8717538 \n\n\nKết quả cho ta thấy p-value < 2.2e-16, như vậy, chúng ta bác bỏ giả thuyết Ho, thừa nhận H1, tức 2 biến trên có mối liên hệ với nhau, cụ thể hơn là tương quan tuyến tính cùng chiều (hệ số tương quan 0.87)."
  },
  {
    "objectID": "p02-05-cac-chi-so-thong-ke.html#anova",
    "href": "p02-05-cac-chi-so-thong-ke.html#anova",
    "title": "7  Các chỉ số thống kê mô tả cơ bản",
    "section": "7.2 ANOVA",
    "text": "7.2 ANOVA\nANOVA viết tắt của từ analysis of variance, tức phân tích phương sai. Phương pháp ANOVA được sử dụng khi chúng ta muốn so sánh giữa 2 hoặc nhiều nhóm đối tượng khác nhau dựa vào 1 tiêu chí nhất định nào đó, ví dụ như so sánh độ tuổi của nhóm khách hàng thường và khách VIP, hay so sánh thu nhập của nhóm khách hàng nam và khách hàng nữ… Để giải thích một cách dễ hiểu hơn, chúng ta sẽ làm ví dụ sau.\nGiả sử chúng ta muốn so sánh chiều dài đài hoa của 3 loài hoa trong tập dữ liệu iris.\nChúng ta sẽ vẽ biểu đồ boxplot so sánh Sepal.Length giữa các loài hoa (Species).\n\nlibrary(ggplot2)\niris %>% \n  ggplot(aes(Species, Sepal.Length, fill = Species)) + \n  geom_boxplot() +\n  labs(\n   title = \"Overview of Iris Sepal Length by Species\", \n       x = \"Species\",                                          \n       y = \"Sepal Length (cm)\") +\n  scale_y_continuous(breaks = seq(0,10, by = 0.5)) +\n  theme_bw() +  # Background đen trắng\n  theme(legend.position = \"none\")  # Bỏ legend\n\n\n\n\nNhìn vào biểu đồ boxplot trên, ta thấy chiều dài đài hoa của loài hoa virginica lớn hơn so với 2 loài hoa còn lại ở cả giá trị median, quantile 25% và quantile 75%. Trong khi đó, setosa có chiều dài đài hoa ngắn nhất trong 3 loài hoa.\nTuy nhiên, để chắc chắn khẳng định rằng thực sự có sự khác biệt chiều dài đài hoa giữa các loài hoa hay không thì chúng ta cần phải sử dụng kiểm định ANOVA.\nKiểm định ANOVA sẽ giúp chung ta so sánh chiều dài đài hoa trung bình giữa các loài hoa, sau đó tính toán chênh lệch chiều dài đài hoa trung bình giữa các loài hoa, và cuối cùng sẽ kiểm định xem những sự chênh lệch đó thực sự có ý nghĩa về mặt thống kê hay không. Hay nói cách khác, phương pháp ANOVA kiểm định cặp giả thuyết sau:\n\nHo: Không có sự khác biệt giữa chiều dài đài hoa của các loài hoa (tức chênh lệch chiều dài đài hoa trung bình = 0)\nH1: Có sự khác biệt giữa chiều dài đài hoa của các loài hoa (chênh lệch chiều dài đài hoa trung bình khác 0)\n\nTrước tiên chúng ta sẽ so sánh chiều dài đài hoa trung bình của các loài hoa.\n\nlibrary(dplyr)\niris %>% \n  group_by(Species) %>% \n  summarise(mean = mean(Sepal.Length),\n            sd = sd(Sepal.Length))\n\n# A tibble: 3 × 3\n  Species     mean    sd\n  <fct>      <dbl> <dbl>\n1 setosa      5.01 0.352\n2 versicolor  5.94 0.516\n3 virginica   6.59 0.636\n\n\nKết quả cho ta thấy rằng chiều dài đài hoa trung bình của loài hoa sentosa là 5.01 cm, của versicolor là 5.94 cm, của virginica là 6.59 cm.\nTiếp theo chúng ta sẽ sử dụng phương pháp ANOVA để kiểm định xem thực sự có sự khác biệt giữa chiều dài đài hoa trung bình giữa các loài hoa hay không bằng việc sử dụng hàm aov()\n\n#ANOVA\nmodel <- aov(Sepal.Length ~ Species, data = iris) \nmodel %>% summary\n\n             Df Sum Sq Mean Sq F value Pr(>F)    \nSpecies       2  63.21  31.606   119.3 <2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nKết quả trên mô hình cho thấy chỉ số p value rất nhỏ, điều này cho phép chúng ta bác bỏ \\(H_0\\) và chấp nhận \\(H_1\\), tức là có sự khác biệt của giá trị trung bình của Sepal.Length giữa các loài hoa.\nTuy nhiên, trong thực tế, câu hỏi có khác biệt hay không chưa đủ, mà ta còn phải trả lời câu hỏi: “Sự khác biệt là bao nhiêu?”\nĐể trả lời cho câu hỏi trên, ta có thể sử dụng kiểm định TukeyHSD để tìm ra sự khác biệt giữa các nhóm.\n\nmodel %>% TukeyHSD\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Length ~ Species, data = iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n\n\nGiải thích ý nghĩa của bảng kết quả:\n\nCột diff: chênh lệch giá trị trung bình chiều dài đài hoa giữa các loài hoa\nlwr và upr: khoảng tin cậy 95% (lwr < diff < upr)\np adj: giá trị p-value, nếu p-value < 0.05 thì chúng ta đủ cơ sở để bác bỏ giả thuyết Ho, chấp nhận H1.\n\nKết quả trong trường hợp này cho ta thấy thực sự có sự khác biệt giữa chiều dài đài hoa của các loài hoa (các giá trị p-value = 0).\n\nNhư vậy, trong chương này, chúng ta đã cùng nhau tìm hiểu cách đọc hiểu những chỉ số thống kê cơ bản như: min/max (giá trị nhỏ nhất/lớn nhất), median (giá trị trung vị), mean (giá trị trung bình), quantile 25% và quantile 75%.\nNgoài ra, chúng ta cũng đã biết những kiểm định quan trọng và phổ biến như:\n\nChi-square test: sử dụng để xem giữa các biến rời rạc (discrete/categorical variables) có mối quan hệ độc lập hay phụ thuộc lẫn nhau\nCorrelation test: sử dụng để xem giữa các biến liên tục (continuous/numeric variables) có mối liên hệ hay không (tương quan tuyến tính)\nANOVA test: sử dụng để so sánh giữa các nhóm đối tượng khác nhau dựa vào giá trị trung bình của 1 biến nhất định nào đó"
  },
  {
    "objectID": "p02-05-cac-chi-so-thong-ke.html#kiểm-định-quan-hệ-chi-bình-phương",
    "href": "p02-05-cac-chi-so-thong-ke.html#kiểm-định-quan-hệ-chi-bình-phương",
    "title": "7  Các chỉ số thống kê mô tả cơ bản",
    "section": "7.3 Kiểm định quan hệ Chi-bình phương",
    "text": "7.3 Kiểm định quan hệ Chi-bình phương\nMột trong những câu hỏi ta thường xuyên phải giải quyết trong quá trình phân tích dữ liệu là tìm kiếm mối quan hệ giữa các biến rời rạc. Một trong những kỹ thuật phổ biến để tìm kiếm mối quan hệ này là sử dụng kiểm định Chi-square (Khi bình phương).\nĐể thực hiện phân tích mối quan hệ này, ta cần thực hiện ba bước:\n\nBước một, xây dựng bảng phân phối tần xuất hai chiều.\nBước hai, tính toán chỉ số \\(\\chi^2\\) để kiểm định giả thuyết độc lập giữa hai biến\nBước ba, đưa ra kết luận về mối quan hệ vừa được kiểm định\n\nĐể hiểu hơn về kiểm định \\(\\chi^2\\), ta xem xét ví dụ dưới đây.\nVí dụ: Một trang web quảng cáo muốn phân tích về mối quan hệ giữa phương thức quảng cáo và thiết bị sử dụng. Dữ liệu trong một tháng về số lượt người dùng truy cập website được thể hiện như bảng dưới đây (đơn vị nghìn user)\n\norder_dc <- c(\"Total\", \"Phone\", \"Tablet\", \"Desktop\")\norder_ch <- c(\"Organic Search\", \"Paid Search\", \"Email\", \"Display\", \"Total\")\ndf <- data.frame(device_category = c(\"Desktop\", \"Tablet\", \"Phone\", \"Total\"),\n                 organic_search = c(25,20,35,80),\n                 paid_search = c(20,30,15,65),\n                 email = c(35,25,10,80),\n                 display = c(20,25,40,85),\n                 total = c(100,100,100,300))\ndf\n\n  device_category organic_search paid_search email display total\n1         Desktop             25          20    35      20   100\n2          Tablet             20          30    25      25   100\n3           Phone             35          15    10      40   100\n4           Total             80          65    80      85   300\n\n\nĐể phân tích mối quan hệ giữa hai biến, ta có giả thuyết như sau:\n\n\\(H_0\\): Phương thức quảng cáo và thiết bị sử dụng không có mối liên hệ với nhau (hai biến độc lập)\n\\(H_1\\): Phương thức quảng cáo và thiết bị sử dụng có mối liên hệ với nhau (tồn tại mối quan hệ giữa hai biến)\n\nChỉ số \\(\\chi^2\\) được tính như sau:\n\\[\\chi^2 = \\sum\\frac{(O_i - E_i)^2}{E_i}\\]\nTrong đó:\n\n\\(O\\) là giá trị quan sát được thực tế (ví dụ: Desktop/Organic Search có 25 ngàn user)\n\\(E\\) là giá trị kỳ vọng của mỗi cặp giá trị.\n\nTa tính giá trị kỳ vọng như sau:\n\nTính tổng từng cột và từng dòng (giá trị ” Total “)\nNhân từng dòng với từng hàng và chia cho tổng số quan sát\n\nVí dụ: Giá trị kỳ vọng của Desktop và Organic Search có thể được tính như sau:\n\\[\n\\begin{aligned}\nUsers_{expected} &= \\frac{r_1}{T} \\times \\frac{c_1}{T} \\times T \\\\\n&= \\frac{100}{300} \\times \\frac{80}{300} \\times 300 \\\\\n&= .33 \\times .27 \\times 300 \\\\\n&= 26.67 \\\\\n&= 27\n\\end{aligned}\n\\]\nGiá trị kỳ vọng của tất cả các giá trị có kết quả như sau\n\n\n  device_category organic_search paid_search email display\n1         Desktop          26.67       21.67 23.33   28.33\n2          Tablet          26.67       21.67 23.33   28.33\n3           Phone          26.67       21.67 23.33   28.33\n\n\nTiếp đó, ta tính toán sự khác biệt giữa giá trị kỳ vọng và giá trị thực tế. Sau đó tính \\(\\chi^2\\) như sau:\n\nTính độ sai lệch\n\n\\[\n\\begin{aligned}\n&= Users_o - Users_e \\\\\n&= 25 - 26.67 \\\\\n&= -1.67\n\\end{aligned}\n\\] - Tính giá trị bình phương\n\\[-1.67^2=2.78\\]\n\nChia giá trị vừa tính được cho giá trị kỳ vọng:\n\n\\[\\frac{2.78}{27} = 0.10\\]\nTương tự, ta tính được cho tất cả các biến như sau:\n\ndf3 <- data.frame(device_category = c(\"Desktop\", \"Tablet\", \"Phone\"),\n                 organic_search = c(0.1, 1.67, 2.6),\n                 paid_search = c(0.13, 3.2, 2.05),\n                 email = c(5.83, 0.12, 7.62),\n                 display = c(2.45, 0.39, 4.8))\ndf3\n\n  device_category organic_search paid_search email display\n1         Desktop           0.10        0.13  5.83    2.45\n2          Tablet           1.67        3.20  0.12    0.39\n3           Phone           2.60        2.05  7.62    4.80\n\n\n\\(\\chi^2\\) thực tế được tính bằng tổng các giá trị trên \\(\\chi^2 = 30.96\\)\nVới dữ liệu trên, ta có 4 cột và 3 hàng. Như vậy, bậc tự do trong phân phôi \\(\\chi^2\\) là \\((4-1)*(3-1) = 6\\). Ta có thể so sánh giá trị vừa tìm được với giá trị \\(\\chi^2\\) tại đây. Với \\(\\alpha\\) bằng .1 và 6 bậc tự do, giá trị của \\(\\chi\\)2 là 10.64. Với giá trị thực tế lớn hơn giá trị lý thuyết, ta bác bỏ \\(H_0\\). Như vậy, về mặt thống kê, tồn tại mối quan hệ giữa phương thức marketing và thiết bị sử dụng.\n\nVí dụ với R\nTrong thực tế, khi phân tích dữ liệu, ta có thể ra quyết định nhanh chóng về mối quan hệ giữa hai biến rời rạc thông qua p-value.\n\ndf %>% select(-1) %>% as.matrix() %>% chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 30.908, df = 12, p-value = 0.002035\n\n\nVới \\(p-value = 0.002035\\), ta bác bỏ \\(H_0\\), chấp nhận \\(H_1\\). Nghĩa là tồn tại mối quan hệ giữa phương thức quảng cáo và thiết bị sử dụng."
  },
  {
    "objectID": "p02-05-cac-chi-so-thong-ke.html#prop-test",
    "href": "p02-05-cac-chi-so-thong-ke.html#prop-test",
    "title": "7  Các chỉ số thống kê mô tả cơ bản",
    "section": "7.4 Prop test",
    "text": "7.4 Prop test\nNếu Anova được sử dụng để đánh giá ảnh hưởng của 1 biến rời rạc lên 1 biến liên tục, prop test là một trường hợp đặc biệt của \\(\\chi^2\\) dùng để đánh giá ảnh hưởng của 1 biến rời rạc lên phân bổ của 1 biến khác.\nVí dụ: Thử nghiệm đánh giá khả năng phản hồi của 2 thông điệp quảng cáo đến các nhóm đối tượng tương tự nhau.\n\n\n\nQuảng cáo\nSố lượt xem\nSố lượt click\nTỷ lệ CR\n\n\n\n\nA\n1000\n20\n2%\n\n\nB\n900\n16\n1.8%\n\n\n\nĐể đánh giá chính xác quảng cáo A có thực sự tốt hơn B hay không, ta có test sau.\n\nobserve <- c(1000, 900)\nclick <- c(20, 16)\nprop.test(click, observe)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  click out of observe\nX-squared = 0.034685, df = 1, p-value = 0.8523\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.01107361  0.01551805\nsample estimates:\n    prop 1     prop 2 \n0.02000000 0.01777778 \n\nchisq.test(click, observe)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  click and observe\nX-squared = 0, df = 1, p-value = 1\n\n\nTrong trường hợp trên, ta không đủ kết luận rằng quảng cáo A tốt hơn quảng cáo B do \\(p-value=0.8523\\)\n\nLưu ý: \\(\\chi^2\\) và prop.test đều ra cùng kết quả của \\(p-value\\)\n\nx <- matrix(c(1000, 20, 900, 16), ncol = 2)\nchisq.test(x)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  x\nX-squared = 0.032264, df = 1, p-value = 0.8575\n\nprop.test(x)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  x\nX-squared = 0.032264, df = 1, p-value = 0.8575\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.2072561  0.1487766\nsample estimates:\n   prop 1    prop 2 \n0.5263158 0.5555556 \n\n\n\nLưu ý: prop.test chỉ được dùng khi đo lường tỷ lệ chuyển đổi, ảnh hưởng của các biến rời rạc (factor) lên đối tượng (discrete number). Do đó, nếu dùng prop.test để đánh giá trên biến giá trị, đặc biệt sau khi chuyển đổi đơn vị tiền tệ sẽ gây ra các nhận định sai lầm.\nVD: Đánh giá về tỷ lệ bảo hiểm tái tục năm 2 nếu khách hàng buộc phải mua bảo hiểm (không phải do tự nguyện), ta có bảng sau\n\n\n\nTiêu chí\nBắt buộc\nKhông bắt buộc\n\n\n\n\nSố lượng hợp đồng (A)\n2000\n3000\n\n\nGiá trị 1 hợp đồng (B)\n20 (M)\n20 (M)\n\n\nTổng giá trị (C=A*B)\n40 (B)\n60 (B)\n\n\nGiá trị huỷ năm 2 (D)\n18 (B)\n22 (B)\n\n\nTỷ lệ huỷ (E= D/C)\n45%\n36.7%\n\n\n\nBước tiếp theo, ta dùng prop.test để đánh giá hiệu ứng thực sự của việc bắt buộc phải mua bảo hiểm đến tỷ lệ huỷ năm 2.\nNếu dùng giá trị là tỷ đồng, ta có kết quả sau.\n\nhuy <- c(18, 22)\ntotal <- c(40, 60)\nprop.test(huy, total)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  huy out of total\nX-squared = 0.39062, df = 1, p-value = 0.532\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.1340627  0.3007294\nsample estimates:\n   prop 1    prop 2 \n0.4500000 0.3666667 \n\n\nKết quả cho thấy \\(p-value = 0.5\\), việc bắt buộc mua bảo hiểm không ảnh hưởng đến tỷ lệ huỷ năm 2!\nĐây là kết quả sai lầm do bị đi nhầm vào bản chất. Do mỗi hợp đồng trị giá trung bình 20 triêu VND, kết quả đúng phải như sau.\n\nhuy <- c(900, 1100)\ntotal <- c(2000, 3000)\nprop.test(huy, total)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  huy out of total\nX-squared = 34.376, df = 1, p-value = 4.543e-09\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.0551185 0.1115482\nsample estimates:\n   prop 1    prop 2 \n0.4500000 0.3666667 \n\n\nKết quả sau cho thấy việc bắt buộc mua bảo hiểm làm tỷ lệ huỷ năm 2 cao hơn 9% so với năm đầu.\n\nĐây là một lỗi rất sâu, rất dễ xảy ra sai lầm khi ra quyết định khi tính toán với prop.test"
  },
  {
    "objectID": "p02-06-lap-trinh-ham.html#cơ-bản-về-hàm",
    "href": "p02-06-lap-trinh-ham.html#cơ-bản-về-hàm",
    "title": "8  Lập trình hàm",
    "section": "8.1 Cơ bản về hàm",
    "text": "8.1 Cơ bản về hàm\nBa thành phần cơ bản của hàm:\n\nbody(): Code trong hàm\nfomals(): Tham số trong hàm\nenvironment(): Môi trường của hàm\n\n\nlibrary(dplyr)\nsquared &lt;- function(x){return(x^2)}\nsquared(4)\n\n[1] 16\n\nsquared %&gt;% body\n\n{\n    return(x^2)\n}\n\nsquared %&gt;% formals\n\n$x\n\nsquared %&gt;% environment()\n\n&lt;environment: R_GlobalEnv&gt;\n\n\nPrimitive function: Là các hàm được thực hiện thẳng từ .Primitive() trên R base và không có các thành phần cơ bản của hàm, vd: sum, sin, cos…\n\nsum %&gt;% formals()\n\nNULL\n\nsin %&gt;% body()\n\nNULL\n\ncos %&gt;% environment()\n\nNULL\n\n\nInfix function\nCác hàm trong R phần lớn đều là prefix function, nghĩa là tên hàm sẽ đứng trước tham số. Tuy nhiên ta có thể tạo ra infix function với các ký tự %name_function%. Loại hàm này có thể sử dụng như các toán từ +, X trong toán học. Infix function có kết cấu như sau:\n%Toán tử% = Hàm\n\n#Prefix function\nbin_string &lt;- function(a, b) {paste(a, b, sep = \" \")}\nbin_string(\"He\", \"lo\")\n\n[1] \"He lo\"\n\n#Infix function\n`%+%` &lt;- function(a, b) {paste(a, b, sep = \" \")}\n\"He\" %+% \"lo\" \n\n[1] \"He lo\"\n\n`%+%`(\"He\", \"Lo\")\n\n[1] \"He Lo\"\n\n#Ví dụ\n5 + 6\n\n[1] 11\n\n`+`(5,6)\n\n[1] 11"
  },
  {
    "objectID": "p02-06-lap-trinh-ham.html#quotes-vs.-unquotes",
    "href": "p02-06-lap-trinh-ham.html#quotes-vs.-unquotes",
    "title": "8  Lập trình hàm",
    "section": "8.2 Quotes vs. Unquotes",
    "text": "8.2 Quotes vs. Unquotes\n\nQuotes là cách thức biến chỉ được lưu dưới dạng string mà chưa đề cập đến giá trị mà biến đó chứa.\nUnquote là việc tính toán giá trị mà biến thực sự lưu trữ.\n\nVí dụ, x &lt;- 5 thì \"x\" là quote vì chỉ lưu tên của biến dưới dạng string. x = 5 là unquote vì đã tính đến giá trị thực sự mà x lưu trữ là 5.\nXem thêm ví dụ sau:\n\ngreet &lt;- function(name){\n  print(\"Hello, name!\")\n}\ngreet(\"duc anh\")\n\n[1] \"Hello, name!\"\n\n\nHàm trên không hoạt động như chúng ta mong muốn vì name đang được quotes và chỉ lưu trữ dưới dạng string.\n\ngreet &lt;- function(name){\n  glue::glue(\"Hello, {name}!\")\n}\ngreet(\"duc anh\")\n\nHello, duc anh!\n\n\nỞ hàm thứ hai, name được tính toán đến giá trị thực sự mà biến này đang lưu trữ (giá trị duc anh).\n\nĐể quotes, ta dùng hàm quo(). Kết quả của quo() là quosure, một dạng của biểu thức (formula).\n\nquo(x)\n\n&lt;quosure&gt;\nexpr: ^x\nenv:  global\n\nquo(a + b + c)\n\n&lt;quosure&gt;\nexpr: ^a + b + c\nenv:  global\n\nquo(\"group_var\")\n\n&lt;quosure&gt;\nexpr: ^\"group_var\"\nenv:  empty\n\n\nĐể sử dụng quosure trong hàm, ta sử dụng enquo. Sự khác biệt giữa quo và enquo có thể phân cấp như sau:\n\nquo: Tạo quosure với biến\nenquo: Tạo quosure với giá trị của biến.\n\n\nx &lt;- 5\n\n# Ví dụ 1\nquo(x)  \n\n&lt;quosure&gt;\nexpr: ^x\nenv:  global\n\nenquo(x)\n\n&lt;quosure&gt;\nexpr: ^5\nenv:  empty\n\n# Ví dụ 2\nquo(x + 2)\n\n&lt;quosure&gt;\nexpr: ^x + 2\nenv:  global\n\n\nĐể unquote, ta dùng hàm !! để tính các giá trị mà biến đang lưu trữ.\n\nmy_summarise &lt;- function(df, group_var) {\n  group_var &lt;- quo(group_var)\n  print(group_var)\n\n  df %&gt;%\n    group_by(!! group_var) %&gt;%\n    summarise(mean = mean(mpg))\n}\nmy_summarise(mtcars, cyl)\n\n&lt;quosure&gt;\nexpr: ^group_var\nenv:  0x55cd0421ee40\n\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\n\nCâu lệnh trên không thực hiện được vì quo(group_var) sẽ trả ra kết quả là ^group_var, giá trị này không tồn tại. Cái chúng ta cần là biểu thức (expression) dạng ^cyl. Lúc này, ta phải dùng enquo\n\nmy_summarise &lt;- function(df, group_var) {\n  group_var &lt;- enquo(group_var)\n  print(group_var)\n\n  df %&gt;%\n    group_by(!! group_var) %&gt;%\n    summarise(mean = mean(mpg))\n}\nmy_summarise(mtcars, cyl)\n\n&lt;quosure&gt;\nexpr: ^cyl\nenv:  global\n\n\n# A tibble: 3 × 2\n    cyl  mean\n  &lt;dbl&gt; &lt;dbl&gt;\n1     4  26.7\n2     6  19.7\n3     8  15.1\n\n\n\nmy_var &lt;- quo(mpg)\nmtcars %&gt;% summarise(mean = mean(!!my_var))\n\n      mean\n1 20.09062\n\n\nTa cũng có thể sử dụng quo, enquo với một nhóm các biểu thức như sau\n\nexp &lt;- quo(summarise(\n  mtcars, mean(mpg)\n))\nexp\n\n&lt;quosure&gt;\nexpr: ^summarise(mtcars, mean(mpg))\nenv:  global"
  },
  {
    "objectID": "p02-06-lap-trinh-ham.html#trường-hợp-dùng-quo_name",
    "href": "p02-06-lap-trinh-ham.html#trường-hợp-dùng-quo_name",
    "title": "8  Lập trình hàm",
    "section": "8.3 Trường hợp dùng quo_name",
    "text": "8.3 Trường hợp dùng quo_name\nquo_name cho phép convert biểu thức thành dạng string.\n\nx &lt;- 5\nquo(x)\n\n&lt;quosure&gt;\nexpr: ^x\nenv:  global\n\nquo(x) %&gt;% quo_name()\n\n[1] \"x\"\n\nenquo(x)\n\n&lt;quosure&gt;\nexpr: ^5\nenv:  empty\n\nenquo(x) %&gt;% quo_name\n\n[1] \"5\"\n\n\nLưu ý:\n\nKhi tạo các biến mới hoặc sử dụng dấu gán trong hàm khi lập trình, cần dùng dấu :=\nGiá trị được gán (bên trái dấu gán) phải có !!\n\n\n\nVí dụ khi sử dụng với rename\n\n\nmy_rename &lt;- function(data, var){\n  var &lt;- enquo(var)\n  new_var &lt;- paste0(\"new_\", quo_name(var))\n  data &lt;- data %&gt;% \n    rename(!!new_var := !!var)\n  return(data)\n}\nmy_rename(mtcars, mpg) %&gt;% head\n\n                  new_mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4            21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag        21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710           22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive       21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout    18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant              18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nKhi sử dụng với mutate\n\n\nmy_mutate &lt;- function(data, var){\n  var &lt;- enquo(var)\n  data &lt;- data %&gt;% \n    mutate(!!\"new_var\" := !!var*2)\n  return(data)\n}\nmtcars %&gt;% \n  select(mpg) %&gt;% \n  my_mutate(mpg) %&gt;% head\n\n                   mpg new_var\nMazda RX4         21.0    42.0\nMazda RX4 Wag     21.0    42.0\nDatsun 710        22.8    45.6\nHornet 4 Drive    21.4    42.8\nHornet Sportabout 18.7    37.4\nValiant           18.1    36.2"
  },
  {
    "objectID": "p02-06-lap-trinh-ham.html#trường-họp-nhiều-biến",
    "href": "p02-06-lap-trinh-ham.html#trường-họp-nhiều-biến",
    "title": "8  Lập trình hàm",
    "section": "8.4 Trường họp nhiều biến",
    "text": "8.4 Trường họp nhiều biến\n\nKhi có nhiều biến, ta dùng quos(...) & !!! thay cho enquo và !!\n\n\nmy_summarise &lt;- function(df, value_var,...){\n  group_var &lt;- quos(...)\n  value_var &lt;- enquo(value_var)\n  df %&gt;% group_by(!!!group_var) %&gt;% \n    summarise(mean = mean(!!value_var))\n}\n  \nmy_summarise(mtcars, mpg, cyl, vs)\n\n# A tibble: 5 × 3\n# Groups:   cyl [3]\n    cyl    vs  mean\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     4     0  26  \n2     4     1  26.7\n3     6     0  20.6\n4     6     1  19.1\n5     8     0  15.1\n\n\nLưu ý: Trong thực tế, khi không muốn viết hàm mà vẫn có thể sử dụng ứng dụng của toán tử !, ta có thể sử dụng toán tử !!! nhu sau.\n\nmy_var &lt;- c(\"mpg\", \"cyl\")\nmtcars %&gt;% select(!!!my_var) %&gt;% head\n\n                   mpg cyl\nMazda RX4         21.0   6\nMazda RX4 Wag     21.0   6\nDatsun 710        22.8   4\nHornet 4 Drive    21.4   6\nHornet Sportabout 18.7   8\nValiant           18.1   6"
  },
  {
    "objectID": "p02-06-lap-trinh-ham.html#ứng-dụng",
    "href": "p02-06-lap-trinh-ham.html#ứng-dụng",
    "title": "8  Lập trình hàm",
    "section": "8.5 Ứng dụng",
    "text": "8.5 Ứng dụng\n\n8.5.1 Hàm tính toán tổng hợp nhiều biến\n\nanalytics_summarise &lt;- function(data, value_var,...){\n  group_var &lt;- quos(...)\n  value_var &lt;- enquo(value_var)\n  data %&gt;% \n    group_by(!!!group_var) %&gt;% \n    summarise(n = n(),\n              total = sum(!!value_var),\n              min = min(!!value_var),\n              q25 = quantile(!!value_var, 0.25, na.rm = T),\n              q50 = quantile(!!value_var, 0.50, na.rm = T),\n              q75 = quantile(!!value_var, 0.75, na.rm = T),\n              q90 = quantile(!!value_var, 0.90, na.rm = T),\n              q95 = quantile(!!value_var, 0.95, na.rm = T),\n              max = max(!!value_var),\n              mean = mean(!!value_var, na.rm = T),\n              mean_trimed = mean(!!value_var, trim = 0.1, na.rm = T)) %&gt;% \n    ungroup -&gt; result\n  return(result)\n}\n\n# Một biến\nmtcars %&gt;% \n  analytics_summarise(mpg, cyl)\n\n# A tibble: 3 × 12\n    cyl     n total   min   q25   q50   q75   q90   q95   max  mean mean_trimed\n  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1     4    11  293.  21.4  22.8  26    30.4  32.4  33.2  33.9  26.7        26.4\n2     6     7  138.  17.8  18.6  19.7  21    21.2  21.3  21.4  19.7        19.7\n3     8    14  211.  10.4  14.4  15.2  16.2  18.3  18.9  19.2  15.1        15.2\n\n# Nhiều biến\nmtcars %&gt;% \n  analytics_summarise(mpg, cyl, am)\n\n# A tibble: 6 × 13\n    cyl    am     n total   min   q25   q50   q75   q90   q95   max  mean\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     4     0     3  68.7  21.5  22.2  22.8  23.6  24.1  24.2  24.4  22.9\n2     4     1     8 225.   21.4  25.2  28.8  30.9  32.8  33.4  33.9  28.1\n3     6     0     4  76.5  17.8  18.0  18.6  19.8  20.7  21.1  21.4  19.1\n4     6     1     3  61.7  19.7  20.4  21    21    21    21    21    20.6\n5     8     0    12 181.   10.4  14.0  15.2  16.6  18.6  18.9  19.2  15.0\n6     8     1     2  30.8  15    15.2  15.4  15.6  15.7  15.8  15.8  15.4\n# ℹ 1 more variable: mean_trimed &lt;dbl&gt;\n\n\n\n\n8.5.2 Vẽ đồ thị với ggplot2\nTương tự với dplyr, lập trình NSE có thể sử dụng đơn giản với ggplot2.\n\nmy_chart &lt;- function(data, value_var, group_var){\n  value_var &lt;- enquo(value_var)\n  group_var &lt;- enquo(group_var)\n  data %&gt;% \n    ggplot(aes(!!group_var, !!value_var)) +\n    geom_bar(stat = \"identity\",\n             aes(fill = !!group_var)) +\n    theme_minimal()\n}\nmtcars %&gt;% \n  mutate(cyl = as.factor(cyl)) %&gt;% \n  group_by(cyl) %&gt;% \n  summarise(mpg = sum(mpg)) %&gt;% \n  my_chart(mpg, cyl)"
  },
  {
    "objectID": "p02-06-lap-trinh-ham.html#các-lưu-ý-khác-khi-viết-hàm",
    "href": "p02-06-lap-trinh-ham.html#các-lưu-ý-khác-khi-viết-hàm",
    "title": "8  Lập trình hàm",
    "section": "8.6 Các lưu ý khác khi viết hàm",
    "text": "8.6 Các lưu ý khác khi viết hàm\n\n8.6.1 Sử dụng text như câu lệnh\nTa có thể đọc text như câu lệnh trong R với eval và parse:\n\nparse - cho phép biến đổi text sang dạng câu lệnh nhưng không thực hiện câu lệnh (not evaluate)\neval - cho phép thực hiện câu lệnh\n\n\n# Ví dụ 1\nx &lt;- \"5 + 5\"\neval(parse(text = x))\n\n[1] 10\n\n# Ví dụ 2\n\ny &lt;- \"iris %&gt;% ggplot(aes(\n  Sepal.Length, Sepal.Width\n)) +\ngeom_point(aes(col = Species))\"\n\neval(parse(text = y))\n\n\n\n\n\n\n8.6.2 Xử lý lỗi trong hàm\nKhi viết hàm trong R, cần phải lưu ý các trường hợp bị lỗi khiến hàm không triển khai được. Ví dụ:\n\nmy_func &lt;- function(x){\n  x + 7\n}\nmy_func(8)\n\n[1] 15\n\n# my_func(\"8\")\n\nĐể xử lý trường hợp trên, có 2 cách:\n\nViết lại logic trong hàm - yêu cầu điều kiện bắt buộc\n\n\n# Cách 1\nmy_func2 &lt;- function(x){\n  if (is.numeric(x)){\n    x + 7\n  } else {\n    NA\n  }\n}\nmy_func2(2)\n\n[1] 9\n\nmy_func2(\"2\")\n\n[1] NA\n\n\n\nSử dụng possibly trong purrr\n\n\nmy_func3 &lt;- possibly(my_func, otherwise = NA)\nmy_func3(8)\n\n[1] 15\n\nmy_func3(\"8\")\n\n[1] NA\n\nmap_dbl(list(\"1\", 8, 9), my_func3)\n\n[1] NA 15 16\n\n\nLưu ý: Với trường hợp kết quả trả ra là dataframe, sử dụng option otherwise = data.frame không cho kết quả như mong muốn mà phải chỉnh sửa ngay từ hàm bên trong\n\nmy_df &lt;- function(x){\n  return(data.frame(x = x, x_squared = x^2))\n}\n\nmy_df &lt;- possibly(my_df, otherwise = data.frame)\n\n# Does not work\n# list(8, 9, \"c\") %&gt;% map(my_df)\n\n\nmy_df2 &lt;- function(x){\n  if (is.numeric(x)){\n      return(data.frame(x = x, x_squared = x^2))\n  } else {\n    return(data.frame(x = NA, x_squared = NA))\n  }\n}\nlist(7, 8, \"a\") %&gt;% map_df(my_df2)\n\n   x x_squared\n1  7        49\n2  8        64\n3 NA        NA"
  },
  {
    "objectID": "p02-07-lap-trinh-ham-voi-purrr.html#nhóm-hàm-map",
    "href": "p02-07-lap-trinh-ham-voi-purrr.html#nhóm-hàm-map",
    "title": "9  Lập trình chức năng hàm với purrr",
    "section": "9.1 Nhóm hàm map",
    "text": "9.1 Nhóm hàm map\nCông thức tổng quát của nhóm hàm map\n\nmap(.x, .f, ...)\n\nGiải thích: Với mỗi giá trị của .x, thực hiện .f. Trong đó, x là một list.\nHàm map làm hàm tổng quát, ngoài ra, map còn có các biến thể chính sau\n\n\n\n\n\n\n\nCâu lệnh\n   Kết quả\n\n\n\n\nmap\nlist\n\n\nmap_dbl\nvector dạng double\n\n\nmap_int\nvector dạng int\n\n\nmap_chr\nvector dạng character\n\n\nmap_df\ndata.frame\n\n\n\n\n# Dạng list\niris %>% map(class)\n\n$Sepal.Length\n[1] \"numeric\"\n\n$Sepal.Width\n[1] \"numeric\"\n\n$Petal.Length\n[1] \"numeric\"\n\n$Petal.Width\n[1] \"numeric\"\n\n$Species\n[1] \"factor\"\n\n# Dạng char\niris %>% map_chr(class)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n   \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"     \"factor\" \n\n# Dạng data.frame\niris %>% map_df(class)\n\n# A tibble: 1 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n  <chr>        <chr>       <chr>        <chr>       <chr>  \n1 numeric      numeric     numeric      numeric     factor \n\n\nMap theo điều kiện với map_if và map_at\nTương tự với map, nhóm map_if và map_at cho phép tính toán theo điều kiện hoặc vị trí của list. Xem ví dụ sau.\n\n# map_if\niris %>% \n  map_if(is.numeric, as.character) %>% \n  as.data.frame %>% \n  str\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: chr  \"5.1\" \"4.9\" \"4.7\" \"4.6\" ...\n $ Sepal.Width : chr  \"3.5\" \"3\" \"3.2\" \"3.1\" ...\n $ Petal.Length: chr  \"1.4\" \"1.4\" \"1.3\" \"1.5\" ...\n $ Petal.Width : chr  \"0.2\" \"0.2\" \"0.2\" \"0.2\" ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# map_at\niris %>% \n  map_at(c(1,2), as.character) %>% \n  str\n\nList of 5\n $ Sepal.Length: chr [1:150] \"5.1\" \"4.9\" \"4.7\" \"4.6\" ...\n $ Sepal.Width : chr [1:150] \"3.5\" \"3\" \"3.2\" \"3.1\" ...\n $ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\nLưu ý: Với trường hợp có hai biến đầu vào, có thể sử dụng nhóm hàm map2. Ví dụ\n\n# Không chạy\nmap_dbl(1:3, 4:6, sum)\n\n\nmap2_dbl(1:3, 4:6, sum)\n\n[1] 5 7 9\n\n\nVới các trường hợp phức tạp, ta cần vận dụng linh hoạt.\nVí dụ: Với mỗi dòng trong iris , tách thành dataframe riêng và xoay chiều dữ liêu. Tên các cột trở thành biến attribute, giá trị các cột trở thành biến value.\n\nlibrary(tidyverse)\nget_data <- function(data, i){\n df <- data %>% \n    slice(i) %>% t %>% \n    as.data.frame\n result <- data.frame(attribute = rownames(df),\n                      value = df[,1])\n rownames(result) <- NULL\n return(result)\n}\n\nget_data(mtcars, 3)\n\n   attribute  value\n1        mpg  22.80\n2        cyl   4.00\n3       disp 108.00\n4         hp  93.00\n5       drat   3.85\n6         wt   2.32\n7       qsec  18.61\n8         vs   1.00\n9         am   1.00\n10      gear   4.00\n11      carb   1.00\n\nget_data(iris, 1)\n\n     attribute  value\n1 Sepal.Length    5.1\n2  Sepal.Width    3.5\n3 Petal.Length    1.4\n4  Petal.Width    0.2\n5      Species setosa\n\n\n\nmap2(replicate(3, iris, simplify = F),\n     c(1:3), get_data)\n\n[[1]]\n     attribute  value\n1 Sepal.Length    5.1\n2  Sepal.Width    3.5\n3 Petal.Length    1.4\n4  Petal.Width    0.2\n5      Species setosa\n\n[[2]]\n     attribute  value\n1 Sepal.Length    4.9\n2  Sepal.Width      3\n3 Petal.Length    1.4\n4  Petal.Width    0.2\n5      Species setosa\n\n[[3]]\n     attribute  value\n1 Sepal.Length    4.7\n2  Sepal.Width    3.2\n3 Petal.Length    1.3\n4  Petal.Width    0.2\n5      Species setosa"
  },
  {
    "objectID": "p02-07-lap-trinh-ham-voi-purrr.html#sửa-đổi-giá-trị-với-modify",
    "href": "p02-07-lap-trinh-ham-voi-purrr.html#sửa-đổi-giá-trị-với-modify",
    "title": "9  Lập trình chức năng hàm với purrr",
    "section": "9.2 Sửa đổi giá trị với modify",
    "text": "9.2 Sửa đổi giá trị với modify\nTương tự như map, modify cho áp dụng hàm vào một nhóm các list. Tuy nhiên, khác với map, modify cho ra kết quả với cấu trúc dữ liệu ban đâu.\n\n# map đổi cấu trúc của dataframe\niris %>% \n  map_if(is.factor, as.character) %>% \n  str\n\nList of 5\n $ Sepal.Length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : chr [1:150] \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n\n# modify giữ nguyên cấu trúc\niris %>% \n  modify_if(is.factor, as.character) %>% \n  str\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : chr  \"setosa\" \"setosa\" \"setosa\" \"setosa\" ..."
  },
  {
    "objectID": "p02-07-lap-trinh-ham-voi-purrr.html#tạo-hàm-nhanh-với-as_mapper",
    "href": "p02-07-lap-trinh-ham-voi-purrr.html#tạo-hàm-nhanh-với-as_mapper",
    "title": "9  Lập trình chức năng hàm với purrr",
    "section": "9.3 Tạo hàm nhanh với as_mapper",
    "text": "9.3 Tạo hàm nhanh với as_mapper\nas_mapper cho phép tạo hàm nhanh, đặc biệt hữu ích khi ta chỉ muốn tạo và sử dụng một hàm trong một vài trường hợp đặc biệt.\nCông thức tổng quát\n\n# Với một tham số\nas_mapper(~f(.x))\n# Với hai tham số\nas_mapper(f(.x, .y))\n\nXem các ví dụ sau:\n\n# Cộng 10 vào mỗi giá trị\nmap_dbl(1:3, ~ .x+10)\n\n[1] 11 12 13\n\n# Cộng hai vector với nhau\nmap2_dbl(1:3, 5:7, ~.x + .y)\n\n[1]  6  8 10\n\n# Cách viết khác\nmap2_dbl(1:3, 5:7, as_mapper(~.x + .y))\n\n[1]  6  8 10"
  },
  {
    "objectID": "p02-07-lap-trinh-ham-voi-purrr.html#xây-dựng-chuỗi-các-hàm-liên-tiếp-với-compose",
    "href": "p02-07-lap-trinh-ham-voi-purrr.html#xây-dựng-chuỗi-các-hàm-liên-tiếp-với-compose",
    "title": "9  Lập trình chức năng hàm với purrr",
    "section": "9.4 Xây dựng chuỗi các hàm liên tiếp với compose",
    "text": "9.4 Xây dựng chuỗi các hàm liên tiếp với compose\nHàm compose cho phép kết hợp nhiều hàm với nhau với hàm ở bên phải là input đầu vào cho hàm bên trái. Cấu trúc như sau.\n\ncompose(f_2, f_1) \n# Tương đương với\nargument %>% f_2 %>% f_1\n\nXem ví dụ sau:\n\nlibrary(tidyverse)\nlibrary(broom)\nlm(Sepal.Length ~ Sepal.Width, data = iris) %>% \n  tidy\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    6.53      0.479     13.6  6.47e-28\n2 Sepal.Width   -0.223     0.155     -1.44 1.52e- 1\n\n\nCách viết trên có thể thay thế như sau\n\ntidy_lm <- compose(tidy, lm)\ntidy_lm(Sepal.Length ~ Sepal.Width, data = iris)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    6.53      0.479     13.6  6.47e-28\n2 Sepal.Width   -0.223     0.155     -1.44 1.52e- 1\n\n\nTa có thể thêm các nhóm hàm khác đi cùng với compose như filter\n\nmy_func <- compose(\n  as_mapper(~filter(.x, p.value < 0.05)),\n  tidy, \n  lm)\nmy_func(Sepal.Length ~ Sepal.Width, data = iris)\n\n# A tibble: 1 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     6.53     0.479      13.6 6.47e-28"
  },
  {
    "objectID": "p02-07-lap-trinh-ham-voi-purrr.html#ứng-dụng",
    "href": "p02-07-lap-trinh-ham-voi-purrr.html#ứng-dụng",
    "title": "9  Lập trình chức năng hàm với purrr",
    "section": "9.5 Ứng dụng",
    "text": "9.5 Ứng dụng"
  },
  {
    "objectID": "p02-07-lap-trinh-ham-voi-purrr.html#thay-vòng-lặp-với-possibly",
    "href": "p02-07-lap-trinh-ham-voi-purrr.html#thay-vòng-lặp-với-possibly",
    "title": "9  Lập trình chức năng hàm với purrr",
    "section": "9.6 Thay vòng lặp với possibly",
    "text": "9.6 Thay vòng lặp với possibly\nCác nhóm hàm map của purrr là phương án hiệu quả để thay thế vòng lặp. Tuy nhiên, nếu 1 giá trị bị lỗi, nhóm hàm này sẽ không thực hiện được bình thường. Để giải quyết vấn đề, ta có thể dùng hàm possibly. Xem ví dụ sau:\n\nmy_func <- function(x){\n  if (x < 0){\n    stop(\"Error\")\n  } else\n    return(data.frame(x = rnorm(1, 1, 1)))\n}\n# Not run\nmy_func(-1)\n\n\nmy_func_possibly <- possibly(my_func, otherwise = data.frame)\nmy_func_possibly(-1)\n\nfunction (..., row.names = NULL, check.rows = FALSE, check.names = TRUE, \n    fix.empty.names = TRUE, stringsAsFactors = FALSE) \n{\n    data.row.names <- if (check.rows && is.null(row.names)) \n        function(current, new, i) {\n            if (is.character(current)) \n                new <- as.character(new)\n            if (is.character(new)) \n                current <- as.character(current)\n            if (anyDuplicated(new)) \n                return(current)\n            if (is.null(current)) \n                return(new)\n            if (all(current == new) || all(current == \"\")) \n                return(new)\n            stop(gettextf(\"mismatch of row names in arguments of 'data.frame', item %d\", \n                i), domain = NA)\n        }\n    else function(current, new, i) {\n        if (is.null(current)) {\n            if (anyDuplicated(new)) {\n                warning(gettextf(\"some row.names duplicated: %s --> row.names NOT used\", \n                  paste(which(duplicated(new)), collapse = \",\")), \n                  domain = NA)\n                current\n            }\n            else new\n        }\n        else current\n    }\n    object <- as.list(substitute(list(...)))[-1L]\n    mirn <- missing(row.names)\n    mrn <- is.null(row.names)\n    x <- list(...)\n    n <- length(x)\n    if (n < 1L) {\n        if (!mrn) {\n            if (is.object(row.names) || !is.integer(row.names)) \n                row.names <- as.character(row.names)\n            if (anyNA(row.names)) \n                stop(\"row names contain missing values\")\n            if (anyDuplicated(row.names)) \n                stop(gettextf(\"duplicate row.names: %s\", paste(unique(row.names[duplicated(row.names)]), \n                  collapse = \", \")), domain = NA)\n        }\n        else row.names <- integer()\n        return(structure(list(), names = character(), row.names = row.names, \n            class = \"data.frame\"))\n    }\n    vnames <- names(x)\n    if (length(vnames) != n) \n        vnames <- character(n)\n    no.vn <- !nzchar(vnames)\n    vlist <- vnames <- as.list(vnames)\n    nrows <- ncols <- integer(n)\n    for (i in seq_len(n)) {\n        xi <- if (is.character(x[[i]]) || is.list(x[[i]])) \n            as.data.frame(x[[i]], optional = TRUE, stringsAsFactors = stringsAsFactors)\n        else as.data.frame(x[[i]], optional = TRUE)\n        nrows[i] <- .row_names_info(xi)\n        ncols[i] <- length(xi)\n        namesi <- names(xi)\n        if (ncols[i] > 1L) {\n            if (length(namesi) == 0L) \n                namesi <- seq_len(ncols[i])\n            vnames[[i]] <- if (no.vn[i]) \n                namesi\n            else paste(vnames[[i]], namesi, sep = \".\")\n        }\n        else if (length(namesi)) {\n            vnames[[i]] <- namesi\n        }\n        else if (fix.empty.names && no.vn[[i]]) {\n            tmpname <- deparse(object[[i]], nlines = 1L)[1L]\n            if (startsWith(tmpname, \"I(\") && endsWith(tmpname, \n                \")\")) {\n                ntmpn <- nchar(tmpname, \"c\")\n                tmpname <- substr(tmpname, 3L, ntmpn - 1L)\n            }\n            vnames[[i]] <- tmpname\n        }\n        if (mirn && nrows[i] > 0L) {\n            rowsi <- attr(xi, \"row.names\")\n            if (any(nzchar(rowsi))) \n                row.names <- data.row.names(row.names, rowsi, \n                  i)\n        }\n        nrows[i] <- abs(nrows[i])\n        vlist[[i]] <- xi\n    }\n    nr <- max(nrows)\n    for (i in seq_len(n)[nrows < nr]) {\n        xi <- vlist[[i]]\n        if (nrows[i] > 0L && (nr%%nrows[i] == 0L)) {\n            xi <- unclass(xi)\n            fixed <- TRUE\n            for (j in seq_along(xi)) {\n                xi1 <- xi[[j]]\n                if (is.vector(xi1) || is.factor(xi1)) \n                  xi[[j]] <- rep(xi1, length.out = nr)\n                else if (is.character(xi1) && inherits(xi1, \"AsIs\")) \n                  xi[[j]] <- structure(rep(xi1, length.out = nr), \n                    class = class(xi1))\n                else if (inherits(xi1, \"Date\") || inherits(xi1, \n                  \"POSIXct\")) \n                  xi[[j]] <- rep(xi1, length.out = nr)\n                else {\n                  fixed <- FALSE\n                  break\n                }\n            }\n            if (fixed) {\n                vlist[[i]] <- xi\n                next\n            }\n        }\n        stop(gettextf(\"arguments imply differing number of rows: %s\", \n            paste(unique(nrows), collapse = \", \")), domain = NA)\n    }\n    value <- unlist(vlist, recursive = FALSE, use.names = FALSE)\n    vnames <- as.character(unlist(vnames[ncols > 0L]))\n    if (fix.empty.names && any(noname <- !nzchar(vnames))) \n        vnames[noname] <- paste0(\"Var.\", seq_along(vnames))[noname]\n    if (check.names) {\n        if (fix.empty.names) \n            vnames <- make.names(vnames, unique = TRUE)\n        else {\n            nz <- nzchar(vnames)\n            vnames[nz] <- make.names(vnames[nz], unique = TRUE)\n        }\n    }\n    names(value) <- vnames\n    if (!mrn) {\n        if (length(row.names) == 1L && nr != 1L) {\n            if (is.character(row.names)) \n                row.names <- match(row.names, vnames, 0L)\n            if (length(row.names) != 1L || row.names < 1L || \n                row.names > length(vnames)) \n                stop(\"'row.names' should specify one of the variables\")\n            i <- row.names\n            row.names <- value[[i]]\n            value <- value[-i]\n        }\n        else if (!is.null(row.names) && length(row.names) != \n            nr) \n            stop(\"row names supplied are of the wrong length\")\n    }\n    else if (!is.null(row.names) && length(row.names) != nr) {\n        warning(\"row names were found from a short variable and have been discarded\")\n        row.names <- NULL\n    }\n    class(value) <- \"data.frame\"\n    if (is.null(row.names)) \n        attr(value, \"row.names\") <- .set_row_names(nr)\n    else {\n        if (is.object(row.names) || !is.integer(row.names)) \n            row.names <- as.character(row.names)\n        if (anyNA(row.names)) \n            stop(\"row names contain missing values\")\n        if (anyDuplicated(row.names)) \n            stop(gettextf(\"duplicate row.names: %s\", paste(unique(row.names[duplicated(row.names)]), \n                collapse = \", \")), domain = NA)\n        row.names(value) <- row.names\n    }\n    value\n}\n<bytecode: 0x5558325ec3e8>\n<environment: namespace:base>\n\n\nĐặc biệt, kết hơp giữa possibly và nhóm map, ta có thể thay thế sử dụng vòng lặp try...Catch trong R\n\n9.6.1 Biến đổi dữ liệu với modify và map_df\nKhi phân tích dữ liệu, đôi khi ta cần chuẩn hóa dữ liệu cho tất cả các biến numeric trong data.frame. Với nhóm hàm của purrr, ta có thể thực hiện như sau\n\n# Tạo hàm\nstandardize_data <- function(x){\n  x <- (x - min(x))/(max(x) - min(x))\n  return(x)\n}\n# Sử dụng map_df\ndf <- iris\ndf[, 1:4] <- df[, 1:4] %>% map_df(standardize_data)\ndf %>% head\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1   0.22222222   0.6250000   0.06779661  0.04166667  setosa\n2   0.16666667   0.4166667   0.06779661  0.04166667  setosa\n3   0.11111111   0.5000000   0.05084746  0.04166667  setosa\n4   0.08333333   0.4583333   0.08474576  0.04166667  setosa\n5   0.19444444   0.6666667   0.06779661  0.04166667  setosa\n6   0.30555556   0.7916667   0.11864407  0.12500000  setosa\n\n# Sử dụng modify\n# Sử dụng map_df\ndf <- iris\ndf <- df %>% modify_if(is.numeric, standardize_data)\ndf %>% head\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1   0.22222222   0.6250000   0.06779661  0.04166667  setosa\n2   0.16666667   0.4166667   0.06779661  0.04166667  setosa\n3   0.11111111   0.5000000   0.05084746  0.04166667  setosa\n4   0.08333333   0.4583333   0.08474576  0.04166667  setosa\n5   0.19444444   0.6666667   0.06779661  0.04166667  setosa\n6   0.30555556   0.7916667   0.11864407  0.12500000  setosa\n\n\n\n\n9.6.2 Phân tích nhiều nhóm khác nhau cùng lúc\nKhi phân tích dữ liệu, đôi khi ta muốn xây dựng chuẩn phân tích dữ liệu qua một số bước, bao gồm:\n\nTính toán các chỉ số thống kê\nVẽ đồ thị\nXây dụng mô hình đơn giản\n\nQuy trình này sẽ không gặp vấn đề khi ta chỉ phải xử lý với một nhóm nhỏ dữ liệu. Khi số lượng nhóm tăng lên, việc phân tích dữ liệu trở nên khó khăn hơn rất nhiều và tốn thời gian. Tuy nhiên, với purrr, các vấn đề này trở nên rất đơn giản.\nVí dụ: Với mỗi nhóm của Species trong tập dữ liệu iris:\n\nTổng hơp dữ liệu\nVẽ đồ thị điểm giữa Sepal.Length vs. Petal.Length\nXây dựng mô hình hồi quy Sepal.Length ~ Petal.Length\n\n\nlibrary(tidyverse)\n# Bước một: Xây dựng hàm\nmy_stat <- function(data){\n  print(\"Summary data\")\n  print(\"====================\")\n  summary(data) %>% print\n  \n  p <- data %>% \n    ggplot(aes(Sepal.Length, Petal.Length)) +\n    geom_point() \n\n  print(\"Summary model\")\n  print(\"====================\")\n  model <- lm(Sepal.Length ~ Petal.Length, data = data)\n  summary(model) %>% print\n  p %>% print\n}\n# Test hàm\nmy_stat(iris)\n\n[1] \"Summary data\"\n[1] \"====================\"\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n[1] \"Summary model\"\n[1] \"====================\"\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4071 on 148 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\n\n\n# Bước 2: Xây dựng map\n\niris$Species %>% \n  unique %>% \n  map(function(value){\n    print(paste0(\"Analysis of \", value))\n    iris %>% \n      filter(Species == value) %>% \n      my_stat\n  })\n\n[1] \"Analysis of setosa\"\n[1] \"Summary data\"\n[1] \"====================\"\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.300   Min.   :1.000   Min.   :0.100  \n 1st Qu.:4.800   1st Qu.:3.200   1st Qu.:1.400   1st Qu.:0.200  \n Median :5.000   Median :3.400   Median :1.500   Median :0.200  \n Mean   :5.006   Mean   :3.428   Mean   :1.462   Mean   :0.246  \n 3rd Qu.:5.200   3rd Qu.:3.675   3rd Qu.:1.575   3rd Qu.:0.300  \n Max.   :5.800   Max.   :4.400   Max.   :1.900   Max.   :0.600  \n       Species  \n setosa    :50  \n versicolor: 0  \n virginica : 0  \n                \n                \n                \n[1] \"Summary model\"\n[1] \"====================\"\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57238 -0.20671 -0.03084  0.17339  0.93608 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.2132     0.4156  10.138 1.61e-13 ***\nPetal.Length   0.5423     0.2823   1.921   0.0607 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3432 on 48 degrees of freedom\nMultiple R-squared:  0.07138,   Adjusted R-squared:  0.05204 \nF-statistic:  3.69 on 1 and 48 DF,  p-value: 0.0607\n\n\n\n\n\n[1] \"Analysis of versicolor\"\n[1] \"Summary data\"\n[1] \"====================\"\n  Sepal.Length    Sepal.Width     Petal.Length   Petal.Width          Species  \n Min.   :4.900   Min.   :2.000   Min.   :3.00   Min.   :1.000   setosa    : 0  \n 1st Qu.:5.600   1st Qu.:2.525   1st Qu.:4.00   1st Qu.:1.200   versicolor:50  \n Median :5.900   Median :2.800   Median :4.35   Median :1.300   virginica : 0  \n Mean   :5.936   Mean   :2.770   Mean   :4.26   Mean   :1.326                  \n 3rd Qu.:6.300   3rd Qu.:3.000   3rd Qu.:4.60   3rd Qu.:1.500                  \n Max.   :7.000   Max.   :3.400   Max.   :5.10   Max.   :1.800                  \n[1] \"Summary model\"\n[1] \"====================\"\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73479 -0.20272 -0.02065  0.26092  0.69956 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    2.4075     0.4463   5.395 2.08e-06 ***\nPetal.Length   0.8283     0.1041   7.954 2.59e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3425 on 48 degrees of freedom\nMultiple R-squared:  0.5686,    Adjusted R-squared:  0.5596 \nF-statistic: 63.26 on 1 and 48 DF,  p-value: 2.586e-10\n\n\n\n\n\n[1] \"Analysis of virginica\"\n[1] \"Summary data\"\n[1] \"====================\"\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.900   Min.   :2.200   Min.   :4.500   Min.   :1.400  \n 1st Qu.:6.225   1st Qu.:2.800   1st Qu.:5.100   1st Qu.:1.800  \n Median :6.500   Median :3.000   Median :5.550   Median :2.000  \n Mean   :6.588   Mean   :2.974   Mean   :5.552   Mean   :2.026  \n 3rd Qu.:6.900   3rd Qu.:3.175   3rd Qu.:5.875   3rd Qu.:2.300  \n Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    : 0  \n versicolor: 0  \n virginica :50  \n                \n                \n                \n[1] \"Summary model\"\n[1] \"====================\"\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73409 -0.23643 -0.03132  0.23771  0.76207 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.05966    0.46677    2.27   0.0277 *  \nPetal.Length  0.99574    0.08367   11.90  6.3e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3232 on 48 degrees of freedom\nMultiple R-squared:  0.7469,    Adjusted R-squared:  0.7416 \nF-statistic: 141.6 on 1 and 48 DF,  p-value: 6.298e-16\n\n\n\n\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n9.6.3 Phân tích nhiều biến trong dataframe cùng lúc\nMột biên thể khác của map là sử dụng trong phân tích cùng lúc nhiều biến số với một biến thuộc dạng nhóm (group). Khi lập trình với NSE, ta cần phải sử dụng hàm syms() trước khi map\nVí dụ: Với mỗi biến số trong tập dữ liệu iris:\n\nSo sánh giá trị trung bình của biến này với các nhóm khác nhau của Species\nVẽ biểu đồ boxplot\n\n\nlibrary(tidyverse)\n\n# Bước 1: Xây dựng hàm\n\nmy_stat <- function(x){\n  x <- enquo(x)\n  iris %>% \n    group_by(Species) %>% \n    summarise(mean = mean(!!x)) %>% \n    print\n  iris %>% \n    ggplot(aes(Species, !!x)) +\n    geom_boxplot(aes(fill = Species)) +\n    theme_minimal()\n}\nmy_stat(Sepal.Length)\n\n# A tibble: 3 × 2\n  Species     mean\n  <fct>      <dbl>\n1 setosa      5.01\n2 versicolor  5.94\n3 virginica   6.59\n\n\n\n\n# Bước 2: Dùng map\niris %>% \n  select_if(is.numeric) %>% \n  names %>% \n  syms %>% \n  map(my_stat)\n\n# A tibble: 3 × 2\n  Species     mean\n  <fct>      <dbl>\n1 setosa      5.01\n2 versicolor  5.94\n3 virginica   6.59\n# A tibble: 3 × 2\n  Species     mean\n  <fct>      <dbl>\n1 setosa      3.43\n2 versicolor  2.77\n3 virginica   2.97\n# A tibble: 3 × 2\n  Species     mean\n  <fct>      <dbl>\n1 setosa      1.46\n2 versicolor  4.26\n3 virginica   5.55\n# A tibble: 3 × 2\n  Species     mean\n  <fct>      <dbl>\n1 setosa     0.246\n2 versicolor 1.33 \n3 virginica  2.03 \n\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\nLưu ý: Việc ứng dụng purrr với các hàm tính toán thống kê sẽ cho phép phân tích khám phá dữ liệu hàng loạt"
  },
  {
    "objectID": "p02-07-lap-trinh-ham-voi-purrr.html#tài-liệu-tham-khảo",
    "href": "p02-07-lap-trinh-ham-voi-purrr.html#tài-liệu-tham-khảo",
    "title": "9  Lập trình chức năng hàm với purrr",
    "section": "9.7 Tài liệu tham khảo",
    "text": "9.7 Tài liệu tham khảo\n\nhttps://purrr.tidyverse.org/"
  },
  {
    "objectID": "p02-08-bien-doi-du-lieu-text.html#giới-thiệu",
    "href": "p02-08-bien-doi-du-lieu-text.html#giới-thiệu",
    "title": "10  Biến đổi dữ liệu text",
    "section": "10.1 Giới thiệu",
    "text": "10.1 Giới thiệu\nBên cạnh dữ liệu định dạng số, dữ liệu chứa rất nhiều định dạng character. Do đó, việc nắm vững các nguyên lý và biến đổi dữ liệu định dạng character trong R sẽ giúp rất nhiều trong việc xử lý dữ liệu. Thư viện hỗ trợ biến đổi dữ liệu text phổ biến và quan trọng nhất là stringr.\nĐể tạo chuỗi string, trong R có 3 cách cơ bản:\n\nDùng dấu ngoặc kép\nDùng dấu ngoặc đơn\nDùng hàm character\n\n\nlibrary(tidyverse)\nlibrary(stringr)\nx <- \"Chú mèo nằm trên lan can\"\ny <- 'Chú mèo nằm trên lan can'\n\nLưu ý: Ta có thể để dấu ngoặc đơn hoặc ngoặc kép trong character như sau:\n\nNgoặc đơn trong ngoặc kép hoặc\nNgoặc kép trong ngoặc đơn\n\n\n(x <- \"This is 'MY' apple\")\n\n[1] \"This is 'MY' apple\"\n\n(y <-  'This is \"HIS\" orange')\n\n[1] \"This is \\\"HIS\\\" orange\"\n\n\nLưu ý: Empty string là chuỗi không có ký tự, được viết ““\n\na <- \"\"\na\n\n[1] \"\"\n\na %>% class\n\n[1] \"character\"\n\n\n\npaste và paste0: Đây là hai hàm cơ bản cho phép ghép các chuỗi lại với nhau\n\n\n#Hàm paste mặc định có khoảng cách giữa các object\npaste(\"I love\", pi)\n\n[1] \"I love 3.14159265358979\"\n\n#Hàm paste0 không có khoảng cách\npaste0(\"I love\", pi)\n\n[1] \"I love3.14159265358979\"\n\n#Paste có sep\npaste(\"I love\", pi, sep = \"___\")\n\n[1] \"I love___3.14159265358979\"\n\n#Paste khi có length khác nhau\npaste(1:3, \"a\", sep = \"\")\n\n[1] \"1a\" \"2a\" \"3a\"\n\n#Paste không có collapse\npaste(1:3, \"a\", sep = \"\", collapse = \"\")\n\n[1] \"1a2a3a\"\n\n#Khi paste giá trị NA, R biến NA thành string\npaste(\"abc \", NA)\n\n[1] \"abc  NA\"\n\n\n\ntoString cho phép biến đổi thành một vector thành chuối\n\n\na <- c(pi, 19)\na\n\n[1]  3.141593 19.000000\n\na %>% toString\n\n[1] \"3.14159265358979, 19\"\n\n\n\nCác hàm toupper, tolower, casefold cho phép biến đổi chuỗi thành các dạng in hoa và in thường\n\n\n#Chuyển sang lower\ntolower(c(\"aLL ChaRacterS in LoweR caSe\", \"ABCDE\"))\n\n[1] \"all characters in lower case\" \"abcde\"                       \n\n#Chuyển sang upper\ntoupper(c(\"aLL ChaRacterS in LoweR caSe\", \"ABCDE\"))\n\n[1] \"ALL CHARACTERS IN LOWER CASE\" \"ABCDE\"                       \n\n#Sử dụng với case fold\ncasefold(c(\"aLL ChaRacterS in LoweR caSe\", \"ABCDE\"))\n\n[1] \"all characters in lower case\" \"abcde\"                       \n\ncasefold(c(\"aLL ChaRacterS in LoweR caSe\", \"ABCDE\"), upper = T)\n\n[1] \"ALL CHARACTERS IN LOWER CASE\" \"ABCDE\"                       \n\n\n\nstr_c: Tương tự như paste, sử dụng \"\" là ký tự phân tách chuỗi mặc định\nstr_length: Kiểm tra độ dài của ký tự\n\n\nlibrary(stringr)\nstr_c(\"Ola\", \"ede\")\n\n[1] \"Olaede\"\n\nstr_c(\"Ola\", \"ede\", sep = \"_\")\n\n[1] \"Ola_ede\"\n\nc(\"olala\") %>% str_length()\n\n[1] 5\n\n\n\nstr_sub: Lấy chuỗi con trong 1 chuỗi\n\n\nx <- c(\"English\", \"Polish\", \"Other language\")\n#Lấy 2 ký tự đầu tiên\nstr_sub(x, start = 1, end = 2)\n\n[1] \"En\" \"Po\" \"Ot\"\n\n#Lấy 2 ký tự cuối cùng\nstr_sub(x, start = -2, end = -1)\n\n[1] \"sh\" \"sh\" \"ge\"\n\n#Sử dụng str_sub để thay thế\nstr_sub(x, start = -2, end = -1) <- str_sub(x, start = -2, end = -1) %>% toupper()\nx\n\n[1] \"EngliSH\"        \"PoliSH\"         \"Other languaGE\"\n\n\n\nstr_wrap: Wrap ký tự - hàm này rất hữu dụng khi sử dụng tên trong biểu đồ\n\n\nsome_quote = c(\n  \"I may not have gone\",\n  \"where I intended to go,\",\n  \"but I think I have ended up\",\n  \"where I needed to be\"\n  )\n\nsome_quote <- str_c(some_quote)\nsome_quote  \n\n[1] \"I may not have gone\"         \"where I intended to go,\"    \n[3] \"but I think I have ended up\" \"where I needed to be\"       \n\nstr_wrap(some_quote, width = 30, indent = 2) %>% cat\n\n  I may not have gone   where I intended to go,   but I think I have ended up   where I needed to be\n\n\nLưu ý: Khi làm việc với chuỗi, hàm cat cho phép thể hiện được các ký tự đặc biệt trên R console\n\n# Không dùng cat\nx <- \"Dòng 1 \\nDòng 2\"\nx\n\n[1] \"Dòng 1 \\nDòng 2\"\n\n# Sử dụng cat\nx %>% cat\n\nDòng 1 \nDòng 2\n\n\n\nstr_trim: Sử dụng để cắt các đoạn text có ký tự trắng\n\n\nbad_text = c(\"This\", \" example \", \"has several \", \" whitespaces \")\nbad_text %>% str_trim(side = \"left\")\n\n[1] \"This\"         \"example \"     \"has several \" \"whitespaces \"\n\nbad_text %>% str_trim(side = \"both\")\n\n[1] \"This\"        \"example\"     \"has several\" \"whitespaces\"\n\n\n\nstr_extract: Chiết xuất giá trị khỏi chuỗi\nstr_extract_all: Chiết xuất giá trị khỏi chuỗi, trả ra list\n\n\nstrings <- c(\n  \"apple\", \n  \"219 733 8965\", \n  \"329-293-8753\", \n  \"Work: 579-499-7527; Home: 543.355.3679\"\n)\n# Chiết xuất giá trị 5 ra khỏi chuỗi\nstrings %>% str_extract(\"5\")\n\n[1] NA  \"5\" \"5\" \"5\"\n\nstrings %>% str_extract_all(\"5\")\n\n[[1]]\ncharacter(0)\n\n[[2]]\n[1] \"5\"\n\n[[3]]\n[1] \"5\"\n\n[[4]]\n[1] \"5\" \"5\" \"5\" \"5\" \"5\"\n\n\n\nstr_count: Kiểm tra số lần xuất hiện của chuỗi con trong chuỗi\n\n\nstates <- rownames(USArrests)\nstates\n\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\n#Kiểm tra độ dài ký tự trong vector\nstates %>% str_length()\n\n [1]  7  6  7  8 10  8 11  8  7  7  6  5  8  7  4  6  8  9  5  8 13  8  9 11  8\n[26]  7  8  6 13 10 10  8 14 12  4  8  6 12 12 14 12  9  5  4  7  8 10 13  9  7\n\n#Đếm số lần xuất hiện ký tự trong vector\nstr_count(states %>% toupper(), \"A\")\n\n [1] 4 3 2 3 2 1 0 2 1 1 2 1 0 2 1 2 0 2 1 2 2 1 1 0 0 2 2 2 1 0 0 0 2 2 0 2 0 2\n[39] 1 2 2 0 1 1 0 1 1 1 0 0\n\n\n\nstr_detect: Kiểm tra tồn tại của ký tự trong chuỗi\n\n\nstring <- c(\"Hello\", \"Vietnam\", \"Apple\")\nstr_detect(string, \"l\")\n\n[1]  TRUE FALSE  TRUE\n\n\n\nstr_replace: Thay thế ký tự đầu tiên trong chuỗi, str_replace_all - thay thế tất cả ký tự trong chuỗi\n\n\nstring <- c(\"Hello\", \"Vietnam\", \"Apple\", \"\", NA)\n# Thay thế ký tự đầu tiên\nstring %>% str_replace(\"l\", \"99\")\n\n[1] \"He99lo\"  \"Vietnam\" \"App99e\"  \"\"        NA       \n\n# Thay thế tất cả ký tự\nstring %>% str_replace_all(\"l\", \"99\")\n\n[1] \"He9999o\" \"Vietnam\" \"App99e\"  \"\"        NA       \n\n# Thay thế NA\nstring %>% str_replace_na(\"9999\")\n\n[1] \"Hello\"   \"Vietnam\" \"Apple\"   \"\"        \"9999\"   \n\n\nLưu ý: \"\" được coi là một ký tự vẫn có độ dài bằng 0\n\n\"\" %>% str_length()\n\n[1] 0\n\n\n\nstr_locate: Kiểm tra vị trí đầu tiên xuất hiện ký tự, str_locate_all - kiểm tra tất cả các vị trí xuất hiện ký tự\n\n\n\"abbbbcab\" %>% str_locate(\"a\")\n\n     start end\n[1,]     1   1\n\n\"abbbbcaab\" %>% str_locate_all(\"a\")\n\n[[1]]\n     start end\n[1,]     1   1\n[2,]     7   7\n[3,]     8   8"
  },
  {
    "objectID": "p02-08-bien-doi-du-lieu-text.html#regular-expression",
    "href": "p02-08-bien-doi-du-lieu-text.html#regular-expression",
    "title": "10  Biến đổi dữ liệu text",
    "section": "10.2 Regular expression",
    "text": "10.2 Regular expression\nRegular expression (regex) là việc chỉ dẫn một hàm cách thức tìm, thay thế và xử lý dữ liệu dạng string. Cơ bản có 4 dạng:\n\nConcetenation (chuỗi ký tự): Tìm chuỗi ký tự: “abcd”\nLogical (phép logic): Tìm chuỗi chứa ab hoặc cd: “ab|cd”\nRepetition (lặp lại ký tự): Tìm chuỗi lặp lại một hoặc nhiều lần ký tự a: “a+”\nGrouping (Nhóm ký tự): Nhóm các ký tự muốn tìm, sử dụng dấu ngoặc đơn ()\n\nKhi phân tích regex, có hai khía cạnh cần quan tâm:\n\nHàm sử dụng trên R\nCách thức tương tác\n\nĐối với cách thức tương tác trong R, có ba nhóm vấn đề sau:\n\nMetadata\nSequennces\nCharacter class\n\n\n10.2.1 Metadata\nMetadata là các ký hiệu đặc biệt được thể hiện trong R nhằm biểu đạt toán tử nhất định. Để làm việc với các ký tự có định dạng giống meta data, cần thêm dâu \\\\. Xem ví dụ dưới đây\n\nx <- \"Vietnam|HaNoi\"\n# Không đúng định dang\nx %>% str_replace_all(\"|\", \"_\")\n\n[1] \"_V_i_e_t_n_a_m_|_H_a_N_o_i_\"\n\n# Sử dụng đúng metadata\nx %>% str_replace_all(\"\\\\|\", \"_\")\n\n[1] \"Vietnam_HaNoi\"\n\n\nTrong trường hợp trên, gsub xử lý ký tự $ như regular expression cho các ký tự đặc biệt.\n\ngsub(pattern = \"\\\\$\", replacement = \".\", \"Vietnam$Ha$Noi\")\n\n[1] \"Vietnam.Ha.Noi\"\n\n\nMetadata trong regular expressions bao gồm các ký tự sau:\n\n$ * + . ? [ ] ^ { } | ( )\n\n\n\n\n\n\n\n\n\nKý tự\n                      Ý nghĩa\n\n\n\n\n.\nmatches everything except for the empty sting ``.\n\n\n+\nthe preceding item will be matched one or more times.\n\n\n*\nthe preceding item will be matched zero or more times.\n\n\n^\nmatches the empty string at the at the beginning of a line\n\n\n$\nmatches empty string at the end of a line.\n\n\n|\ninfix operator: OR\n\n\n()\nbrackets for grouping.\n\n\n[]\ncharacter class brackets\n\n\n\nLưu ý: Khi sử dụng dấu gạch ngoặc kép (\\\\) là ký tự bình thưởng, sử dụng 4 gạch chéo. Xem các ví dụ dưới đây\n\nmeta_char <- c(\"$\",\"*\",\"+\",\".\",\"?\",\"[\",\"^\",\"{\",\"|\",\"(\",\"\\\\\")\nmeta_char\n\n [1] \"$\"  \"*\"  \"+\"  \".\"  \"?\"  \"[\"  \"^\"  \"{\"  \"|\"  \"(\"  \"\\\\\"\n\n#Không sử dụng chính xác\nstr_locate(meta_char, \"^\")\n\n      start end\n [1,]     1   0\n [2,]     1   0\n [3,]     1   0\n [4,]     1   0\n [5,]     1   0\n [6,]     1   0\n [7,]     1   0\n [8,]     1   0\n [9,]     1   0\n[10,]     1   0\n[11,]     1   0\n\n#Sử dụng chính xác\nmeta_char %>% str_locate(\"\\\\^\")\n\n      start end\n [1,]    NA  NA\n [2,]    NA  NA\n [3,]    NA  NA\n [4,]    NA  NA\n [5,]    NA  NA\n [6,]    NA  NA\n [7,]     1   1\n [8,]    NA  NA\n [9,]    NA  NA\n[10,]    NA  NA\n[11,]    NA  NA\n\n#Hai dâu gạch chéo\nmeta_char %>% str_locate(\"\\\\\\\\\")\n\n      start end\n [1,]    NA  NA\n [2,]    NA  NA\n [3,]    NA  NA\n [4,]    NA  NA\n [5,]    NA  NA\n [6,]    NA  NA\n [7,]    NA  NA\n [8,]    NA  NA\n [9,]    NA  NA\n[10,]    NA  NA\n[11,]     1   1\n\n\n\n\n10.2.2 Sequences\nSequence trong R có các nhóm sau:\n\n\n#sub thay thế giá trị phù hợp đầu tiên\nsub(\"\\\\d\",\"*\", \"Y2K was in 2000\")\n\n[1] \"Y*K was in 2000\"\n\n#gsub thay thế tất cả các giá trị phù hợp\ngsub(\"\\\\d\",\"*\", \"Y2K was in 2000\")\n\n[1] \"Y*K was in ****\"\n\n#Thay thế những ký tự không phải là số\ngsub(\"\\\\D\",\"*\", \"Y2K was in 2000\")\n\n[1] \"*2*********2000\"\n\n#Thay thế space\ngsub(\"\\\\s\",\"*\", \"Y2K was in 2000\")\n\n[1] \"Y2K*was*in*2000\"\n\n#Non space\ngsub(\"\\\\S\",\"*\", \"Y2K was in 2000\")\n\n[1] \"*** *** ** ****\"\n\n\n\n\n10.2.3 Character class\nCharacter class sẽ tìm MỘT ký tự phù hợp trong bracket []\n\n#Tìm ký tự từ a-z\ngsub(\"[a-z]\", \"*\", \"Vietnam in 2016\")\n\n[1] \"V****** ** 2016\"\n\n#Tìm số từ 0-9\ngsub(\"[0-9]\", \"*\", \"Vietnam in 2016\")\n\n[1] \"Vietnam in ****\"\n\n#Tìm các ký tự trừ số\ngrep(\"[^0-9]\", c(\"Vietnam in\", \" $%^$\", \"2016\"), value = T)\n\n[1] \"Vietnam in\" \" $%^$\"     \n\n\n\n\n10.2.4 POSIX class\nPosix class là kiểu dữ liệu đặc biệt được quy ước trong regex, cho phép biến đổi và thay thế dữ liệu character hiệu quả. Kiểu dữ liệu này được thể hiện trong 2 dấu ngoặc kép định dạng [[:class:]] như sau.\n\n\nexample <- c(\"Alo ala #^ 12,6.7\")\n#Loại blank\ngsub(\"[[:blank:]]\", \"\", example)\n\n[1] \"Aloala#^12,6.7\"\n\n#Loại dấu\ngsub(\"[[:punct:]]\", \"\", example)\n\n[1] \"Alo ala  1267\"\n\n\n\n\n10.2.5 Quantifiers\nQuantifier là nhóm điều kiện đếm số lần lặp lại của 1 nhóm ký tự\n\nXem các ví dụ sau:\n\n# people names\npeople = c(\"rori\", \"emilia\", \"matteo\", \"mehmet\", \"filipe\", \"anna\", \"tyler\",\n\"rasmus\", \"jacob\", \"youna\", \"flora\", \"adi\")\n# Tìm tên có xuất hiện m nhiều nhất 1 lần\ngrep(pattern = \"m?\", people, value = TRUE)\n\n [1] \"rori\"   \"emilia\" \"matteo\" \"mehmet\" \"filipe\" \"anna\"   \"tyler\"  \"rasmus\"\n [9] \"jacob\"  \"youna\"  \"flora\"  \"adi\"   \n\n# Tên xuất hiện m chính xác 1 lần\ngrep(pattern = \"m{1}\", people, value = TRUE, perl = FALSE)\n\n[1] \"emilia\" \"matteo\" \"mehmet\" \"rasmus\"\n\n# Tên có xuất hiện hoặc không có xuất hiện m rồi xuất hiện t\ngrep(\"m*t\", people, value = T)\n\n[1] \"matteo\" \"mehmet\" \"tyler\" \n\n#Tên có m một hoặc nhiều lần\ngrep(\"m+t\", people, value = T)\n\ncharacter(0)\n\n#Tên có m một hoặc nhiều lần rồi đến t\ngrep(\"m+.t\", people, value = T)\n\n[1] \"matteo\" \"mehmet\"\n\n\n\nparis_tweets = c(\n\"#Paris is chock-full of cultural and culinary attractions\",\n\"Some time in #Paris along Canal St.-Martin famous by #Amelie\",\n\"While you're in #Paris, stop at cafe: http://goo.gl/yaCbW\",\n\"Paris, the city of light\")\n# match (all) hashtags in 'paris_tweets'\nstr_match_all(paris_tweets, \"#[a-zA-Z]{1,}\")\n\n[[1]]\n     [,1]    \n[1,] \"#Paris\"\n\n[[2]]\n     [,1]     \n[1,] \"#Paris\" \n[2,] \"#Amelie\"\n\n[[3]]\n     [,1]    \n[1,] \"#Paris\"\n\n[[4]]\n     [,1]"
  },
  {
    "objectID": "p02-08-bien-doi-du-lieu-text.html#các-ví-dụ-tổng-hợp",
    "href": "p02-08-bien-doi-du-lieu-text.html#các-ví-dụ-tổng-hợp",
    "title": "10  Biến đổi dữ liệu text",
    "section": "10.3 Các ví dụ tổng hợp",
    "text": "10.3 Các ví dụ tổng hợp\n\n10.3.1 Ví dụ 1\n\nVấn đề: Thay thế các ký tự đặc biệt lặp đi lặp lại\nGiải pháp: Sử dụng dấu +\n\n\nd1 = data.frame(\n  id...of....patient = c(1, 2), \n  patient....age = c(1, + 2))\nnames(d1) <- gsub(pattern = \"\\\\.+\", \n                  replacement = \"_\", x = names(d1))\nnames(d1)\n\n[1] \"id_of_patient\" \"patient_age\"  \n\n\n\n\n10.3.2 Ví dụ 2\n\nVấn đề: Lọc các ký tự trong 1 nhóm\nGiải pháp:\n\nSử dụng ký tự []\nCó thể sử dụng dấu - để tìm trong khoảng\n\n\n\n#Tìm vị trí các ký tự\ngrep(\"[abcAB]\", x = c(\"Con ca\", \"Boong\", \"Xoong\"))\n\n[1] 1 2\n\ngrep(\"[0-3]\", \n     c(\"nose\", \"letter38\", \"window9\", \"apple0\"), value = TRUE)\n\n[1] \"letter38\" \"apple0\"  \n\n\nChi tiết xem trong bảng dưới đây\n\n\n\n10.3.3 Ví dụ 3\n\nd <- data.frame(\n  id = c(11, 22, 33, 44, 55, 66, 77, 88), \n  drug = c(\"vitamin E\", \"vitamin ESTER-C\", \n           \" vitamin Eabc \", \"vitamin E(ointment)\", \"\", \n           \"vitamin E \", \"provitamin E\\n\", \"vit E\"), \n  text = c(\"\",\" \", \" 3 times a day after meal\", \n           \"once a day\", \" \",\n           \" one per day \", \"\\t\", \"\\n \"), \n  stringsAsFactors = FALSE)\n(s <-  d$text)\n\n[1] \"\"                          \" \"                        \n[3] \" 3 times a day after meal\" \"once a day\"               \n[5] \" \"                         \" one per day \"            \n[7] \"\\t\"                        \"\\n \"                      \n\n#Các giá trị riêng của s\nunique(s)\n\n[1] \"\"                          \" \"                        \n[3] \" 3 times a day after meal\" \"once a day\"               \n[5] \" one per day \"             \"\\t\"                       \n[7] \"\\n \"                      \n\n#Xử lý ký tự\n#Các ký tự trong ngoặc vuông chuyển thành NA\ngsub(\"[\\t\\n\\r\\f\\v]+\", NA, s)\n\n[1] \"\"                          \" \"                        \n[3] \" 3 times a day after meal\" \"once a day\"               \n[5] \" \"                         \" one per day \"            \n[7] NA                          NA                         \n\n#Chuyển tất cả các ký tự\ngsub(\"^$|^( +)$|[\\t\\n\\r\\f\\v]+\", NA, s)\n\n[1] NA                          NA                         \n[3] \" 3 times a day after meal\" \"once a day\"               \n[5] NA                          \" one per day \"            \n[7] NA                          NA                         \n\n#Chuyển tất cả các khoảng trắng sang NULL\ngsub(\"^([ \\t\\n\\r\\f\\v]+)|([ \\t\\n\\r\\f\\v]+)$\", \"\", s)\n\n[1] \"\"                         \"\"                        \n[3] \"3 times a day after meal\" \"once a day\"              \n[5] \"\"                         \"one per day\"             \n[7] \"\"                         \"\"                        \n\n######\n#Tìm vitamin e\nt <- d$drug\ngrep(\"vitamin e\", t, ignore.case = T, value = T)\n\n[1] \"vitamin E\"           \"vitamin ESTER-C\"     \" vitamin Eabc \"     \n[4] \"vitamin E(ointment)\" \"vitamin E \"          \"provitamin E\\n\"     \n\n#Tìm vitamin e mà đi sau đi kèm ký tự alphabet\ngrep(\"vitamin e([a-zA-Z])\", t, ignore.case = T, value = T)\n\n[1] \"vitamin ESTER-C\" \" vitamin Eabc \" \n\n#Tìm vitamin e mà đi sau không đi kèm gì\ngrep(\"vitamin e($)\", t, ignore.case = T, value = T)\n\n[1] \"vitamin E\"\n\n#Tìm vitamin e mà đi sau không phải là ký tự alphabet\ngrep(\"vitamin e($|[^a-zA-Z])\", s, ignore.case = TRUE, value = TRUE)\n\ncharacter(0)\n\n#Tìm thêm vit e\ngrep(\"vitamin e($|[^a-zA-Z])|vit e($|[^a-zA-Z])\", t, \n     ignore.case = TRUE, value = TRUE)\n\n[1] \"vitamin E\"           \"vitamin E(ointment)\" \"vitamin E \"         \n[4] \"provitamin E\\n\"      \"vit E\"              \n\n#Tìm ký tự bắt đầu với _vit\ngrep(\" vit\", t, ignore.case = T, value = T)\n\n[1] \" vitamin Eabc \"\n\n#Loại prodvitamin\ngrep(\"([^a-z]+|^)vitamin e($|[^a-zA-Z])|([^a-z]+|^)vit e($|[^a-zA-Z])\",\n     t, ignore.case = TRUE, value = TRUE)\n\n[1] \"vitamin E\"           \"vitamin E(ointment)\" \"vitamin E \"         \n[4] \"vit E\"              \n\n\n\n\n10.3.4 Ví dụ 4\n\nVấn đề: Thay thế ký tự rỗng\nGiải pháp: Sử dụng \"^$\"\n\n\ndf <- data.frame(x = c(\"1\", \"viet\", \"\"))\n#Không chạy\ndf$x %>% as.character %>% str_replace_na(\"Missing\")\n\n[1] \"1\"    \"viet\" \"\"    \n\n#Chạy\ndf$x %>% as.character %>% str_replace_all(\"^$\", \"Missing\")\n\n[1] \"1\"       \"viet\"    \"Missing\"\n\n\n\n\n10.3.5 Ví dụ 5\n\nVấn đề: Xác định ký tự cuối cùng của chuỗi là _ hoặc -\nGiải pháp: Sử dụng regex định dạng $\n\n\nlibrary(tidyverse)\nx <- c(\"hello_\", \"a_b\", \"heloo-\", \"xu-ly\")\n# Tìm ký tự _ hoặc - ở cuối chuỗi\nx %>% str_detect(\"_$|-$\")\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\n\n\n10.3.6 Ví dụ 6\n\nVấn đề: Chiết xuất chuỗi giữa 2 nhóm ký tự\nGiải pháp: Sử dụng str_match và str_match_all trong stringr\n\n\nx <- c(\"select ABC, ZYZ from A\")\nresult <- str_match(x, \"select\\\\s*(.*?)\\\\s*from\")\nresult[, 2]\n\n[1] \"ABC, ZYZ\"\n\n\n\n\n10.3.7 Ví dụ 7\n\nVấn đề: Convert dữ liệu có dấu sang không dấu\nGiải pháp: Sử dụng stri_trans_general\n\n\nx <- \"Cộng hoà xã hội chủ nghĩa Việt Nam\"\nx %>% stringi::stri_trans_general(\"ascii\")\n\n[1] \"Cong hoa xa hoi chu nghia Viet Nam\""
  },
  {
    "objectID": "p02-08-bien-doi-du-lieu-text.html#tài-liệu-tham-khảo",
    "href": "p02-08-bien-doi-du-lieu-text.html#tài-liệu-tham-khảo",
    "title": "10  Biến đổi dữ liệu text",
    "section": "10.4 Tài liệu tham khảo",
    "text": "10.4 Tài liệu tham khảo\n\ngsub"
  },
  {
    "objectID": "p02-11-quan-ly-nhieu-mo-hinh.html",
    "href": "p02-11-quan-ly-nhieu-mo-hinh.html",
    "title": "11  Quản lý kết quả phân tích từ nhiều mô hình",
    "section": "",
    "text": "Khi phân tích nhiều mô hình cùng lúc trong R, output từ các mô hình thường được lưu ở dạng list và rất khó kết hợp với nhau. Nhiều trường hợp, ta cần xây dựng cùng lúc nhiều mô hình và tổng hợp cùng lúc kết quả từ các mô hình này. Broom cho phép làm sạch các output của mô hình.\nCác hàm giúp làm sạch output:\n\ntidy: Data frame cho phép tổng hợp kết quả của các mô hình, bao gồm coefficient, p-value\naugment: Thêm cột vào tập dữ liệu được phân tích\nglance: Tổng quan các chỉ số của nhiều mô hình\n\n\nlibrary(tidyverse)\nlibrary(broom)\n\nlmfit <- lm(mpg ~ wt, mtcars)\nlmfit %>% summary\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  37.2851     1.8776  19.858  < 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n#Tidy toàn bộ mô hình\nlmfit %>% tidy\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    37.3      1.88      19.9  8.24e-19\n2 wt             -5.34     0.559     -9.56 1.29e-10\n\n#Nhìn tổng quan mô hình\nlmfit %>% glance\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.753         0.745  3.05      91.4 1.29e-10     1  -80.0  166.  170.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n#Nối kết quả với mô hình\nlmfit %>% augment %>% head\n\n# A tibble: 6 × 9\n  .rownames           mpg    wt .fitted .resid   .hat .sigma  .cooksd .std.resid\n  <chr>             <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>    <dbl>      <dbl>\n1 Mazda RX4          21    2.62    23.3 -2.28  0.0433   3.07  1.33e-2    -0.766 \n2 Mazda RX4 Wag      21    2.88    21.9 -0.920 0.0352   3.09  1.72e-3    -0.307 \n3 Datsun 710         22.8  2.32    24.9 -2.09  0.0584   3.07  1.54e-2    -0.706 \n4 Hornet 4 Drive     21.4  3.22    20.1  1.30  0.0313   3.09  3.02e-3     0.433 \n5 Hornet Sportabout  18.7  3.44    18.9 -0.200 0.0329   3.10  7.60e-5    -0.0668\n6 Valiant            18.1  3.46    18.8 -0.693 0.0332   3.10  9.21e-4    -0.231 \n\n\nKhi xây dựng mô hình, ta có thể kết hợp broom và dplyr như sau.\n\nmtcars %>% group_by(cyl) %>% \n  do(lm(mpg ~ disp, data=.) %>% tidy) \n\n# A tibble: 6 × 6\n# Groups:   cyl [3]\n    cyl term        estimate std.error statistic    p.value\n  <dbl> <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1     4 (Intercept) 40.9       3.59       11.4   0.00000120\n2     4 disp        -0.135     0.0332     -4.07  0.00278   \n3     6 (Intercept) 19.1       2.91        6.55  0.00124   \n4     6 disp         0.00361   0.0156      0.232 0.826     \n5     8 (Intercept) 22.0       3.35        6.59  0.0000259 \n6     8 disp        -0.0196    0.00932    -2.11  0.0568    \n\n\nTuy nhiên, khi dữ liệu có quá nhiều, ta có thể sử dụng purrr thay cho broom như sau\n\nlibrary(purrr)\nlibrary(broom)\nmtcars %>%\n  split(.$cyl) %>%\n  map(~lm(mpg ~ wt, data = .)) %>% \n  map(summary)\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1513 -1.9795 -0.6272  1.9299  5.2523 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   39.571      4.347   9.104 7.77e-06 ***\nwt            -5.647      1.850  -3.052   0.0137 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.332 on 9 degrees of freedom\nMultiple R-squared:  0.5086,    Adjusted R-squared:  0.454 \nF-statistic: 9.316 on 1 and 9 DF,  p-value: 0.01374\n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nResiduals:\n     Mazda RX4  Mazda RX4 Wag Hornet 4 Drive        Valiant       Merc 280 \n       -0.1250         0.5840         1.9292        -0.6897         0.3547 \n     Merc 280C   Ferrari Dino \n       -1.0453        -1.0080 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   28.409      4.184   6.789  0.00105 **\nwt            -2.780      1.335  -2.083  0.09176 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.165 on 5 degrees of freedom\nMultiple R-squared:  0.4645,    Adjusted R-squared:  0.3574 \nF-statistic: 4.337 on 1 and 5 DF,  p-value: 0.09176\n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1491 -1.4664 -0.8458  1.5711  3.7619 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  23.8680     3.0055   7.942 4.05e-06 ***\nwt           -2.1924     0.7392  -2.966   0.0118 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.024 on 12 degrees of freedom\nMultiple R-squared:  0.423, Adjusted R-squared:  0.3749 \nF-statistic: 8.796 on 1 and 12 DF,  p-value: 0.01179\n\n#Version với broom\nmtcars %>% \n  group_by(cyl) %>% \n  do(lm(mpg ~ wt, data = .) %>% tidy)  \n\n# A tibble: 6 × 6\n# Groups:   cyl [3]\n    cyl term        estimate std.error statistic    p.value\n  <dbl> <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1     4 (Intercept)    39.6      4.35       9.10 0.00000777\n2     4 wt             -5.65     1.85      -3.05 0.0137    \n3     6 (Intercept)    28.4      4.18       6.79 0.00105   \n4     6 wt             -2.78     1.33      -2.08 0.0918    \n5     8 (Intercept)    23.9      3.01       7.94 0.00000405\n6     8 wt             -2.19     0.739     -2.97 0.0118    \n\n\nNhóm hàm của broom có thể sử dụng trong rất nhiều nhóm mô hình, bao gồm anova, t.test, quantile regression, hồi quy tuyến tính. Việc kết hợp và sử dụng nhóm hàm này sẽ giúp ích rất nhiều trong việc phân tích khám phá dữ liệu"
  },
  {
    "objectID": "p02-15-thong-ke-co-ban.html#xác-suất",
    "href": "p02-15-thong-ke-co-ban.html#xác-suất",
    "title": "12  Xác suất và phân phối thống kê cơ bản",
    "section": "12.1 Xác suất",
    "text": "12.1 Xác suất\n\n12.1.1 Nhân xác suất\n\\[P(A \\frown B) = P(AB) = P(A)*P(B|A) = P(B)*P(A|B)\\]\nVí dụ 1: Xét tập Khách hàng gồm:\n\n20% chỉ có TD => trong đó 30% có VPO\n65% chỉ có OD => trong đó 55% có VPO\n15% có cả TD và OD => trong đó có 40% có VPO\n\nTính tỷ lệ số Khách hàng có VPO ?\nGiả sử:\n\nA : KH có VPO ;\nB : Toàn bộ khách hàng\nB1: KH chỉ có TD => P(B1) = 0.2\nB2: KH chỉ có OD => P(B2) = 0.65\nB3: KH có cả TD và OD => P(B3) = 0.15\n\n\\(P(A|B1) = 0.3\\)\n\\(P(A|B2) = 0.55\\)\n\\(P(A|B3) = 0.4\\)\n$$P(A B) = P(A B1) + P(A B2) + P(A B3)\\\n          = P(B1)P(A|B1) + P(B2)P(A|B2) + P(B3)P(A|B3)\\\\\n          \n              = 0.2*0.3 + 0.65*0.55 + 0.15*0.4 = 0.4775 = 47.75\\%$$\nNếu A và B độc lập (lựa chọn A không ảnh hưởng đến lựa chọn B)\n\\[P(A \\frown B) = P(A)*P(B)\\]\nVí dụ 2:\n\n\\[P(B = 0) = 0.3 + 0.3 = 0.6\\]\n\\[P(A = 0) = 0.3 + 0.2 = 0.5\\]\n\\[P(A = 0, B = 0) = 0.3\\]\nVì \\(P(A = 0, B = 0) = P(A = 0)P(B = 0)\\) nên A và B độc lập với nhau\n\n\n12.1.2 Cộng xác suất\n\\[P(A + B) = P(A) + P(B) – P(A \\frown B)\\]\nNếu A và B xung khắc (chọn A thì không chọn B)\n\\[P(A + B) = P(A) + P (B)\\]"
  },
  {
    "objectID": "p02-15-thong-ke-co-ban.html#chỉ-số-thống-kê-biến-ngẫu-nhiên",
    "href": "p02-15-thong-ke-co-ban.html#chỉ-số-thống-kê-biến-ngẫu-nhiên",
    "title": "12  Xác suất và phân phối thống kê cơ bản",
    "section": "12.2 Chỉ số thống kê biến ngẫu nhiên",
    "text": "12.2 Chỉ số thống kê biến ngẫu nhiên\n\n12.2.1 Đơn biến\n\n\nKỳ vọng\n\n\\[E(X) = \\sum(x_i * p_i) ; i=1,…,n\\]\n\nPhương sai\n\n\\[Var(X) = E(X^{2}) – [E(X)]^{2} =  \\sum(x_i^{2} * p_i) – [\\sum(x_i * p_i)]^{2}\\]\n\nĐộ lệch chuẩn\n\n\\[\\sigma (X) = \\sqrt{V(X)}\\]\n\nHệ số biến thiên\n\n\\[CV = \\frac{\\sigma(X)}{E(X)} * 100\\%\\]\n\n\n12.2.2 Đa biến\n\n\nKỳ vọng\n\n\\[E(AB) = E(A)*E(B) + Cov(A,B)\\]\n\nPhương sai\n\n\\[V(AB) = V(A) + V(B) + 2*Cov(A,B)\\]\n\nHiệp phương sai\n\n\\[Cov(A,B) = E{[X – E(X)][Y – E(Y)]}\\]\nKỳ vọng của tích các sai lệch giữa các biến so với trung bình của chúng\nHiệp phương sai có đơn vị bằng tích các đơn vị đo lường, do đó với mỗi đơn vị lường khác nhau ta thu được kết quả khác nhau. Để khắc phục hạn chế này, ta sử dụng hệ số tương quan\n\nHệ số tương quan Pearson\n\n\\[r(A,B) = \\frac{Cov(A,B)}{\\sigma(A) * \\sigma(B)}\\]\nHệ số tương quan không có đơn vị đo và có các tính chất sau :\n\n\\(-1 <= r(A, B) <= 1\\)\nNếu \\(r(A, B) = +- 1\\) thì A và B phụ thuộc tuyến tính\nNếu \\(r(A, B) > 0\\) thì A và B đồng biến\nNếu \\(r(A, B) <0\\) thì A và B nghịch biến\nNếu A và B độc lập thì \\(r(A,B) = 0\\)\n\nHiệp phương sai và hệ số tương quan được dùng để đo mối quan hệ phụ thuộc giữa các biến\nVí dụ:\nTính hệ số tương quan của 2 biến\n\nlibrary(dplyr)\nmtcars %>% head\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\ncor(mtcars$cyl, mtcars$wt, method = \"pearson\")\n\n[1] 0.7824958\n\n\nKiểm định hệ số tương quan\n\ncor.test(mtcars$cyl, mtcars$wt)\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$cyl and mtcars$wt\nt = 6.8833, df = 30, p-value = 1.218e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5965795 0.8887052\nsample estimates:\n      cor \n0.7824958"
  },
  {
    "objectID": "p02-15-thong-ke-co-ban.html#các-phân-phối-cơ-bản",
    "href": "p02-15-thong-ke-co-ban.html#các-phân-phối-cơ-bản",
    "title": "12  Xác suất và phân phối thống kê cơ bản",
    "section": "12.3 Các phân phối cơ bản",
    "text": "12.3 Các phân phối cơ bản\n\n12.3.1 Phân phối không – một A(p)\n\nTrong phân phối này, biến ngẫu nhiên X nhận một trong hai giá trị {0, 1} với xác suất tương ứng {1 –p ; p} được gọi là phân phối theo quy luật không – một với tham số p.\n\\[P_x = p^{x}(1 – p)^{x}\\]\nCác tham số đặc trưng :\n\n\\(E(X) = p\\)\n\\(V(X) = E(X^{2}) – [E(X)]^{2} = p – p^{2} = p(1 – p)\\)\n\\(\\sigma (X) = \\sqrt{p(1 – p)}\\)\n\n\n\n12.3.2 Phân phối nhị thức – Bernoulli B(n, p)\n\nThực hiện n lần (hữu hạn) biến ngẫu nhiên Y (phân phối theo quy luật không – một với tham số p), thu được biến ngẫu nhiên X phân phối theo quy luật nhị thức với các tham số n và p \\[P_x = C_n^{x}p^{x}(1 – p)^{x}\\] Các tham số đặc trưng :\n\n\\(E(X) = n*p\\)\n\\(V(X) = np(1 - p)\\)\n\\(\\sigma (X) = \\sqrt{np(1 – p)}\\)\n\nVí dụ\nGiả sử tỷ lệ gửi TD Online của khách hàng có VPO là 60%. Nếu tiến hành 100 đợt truyền thông, mỗi đợt lấy ngẫu nhiên 50 người, phân bố của khách hàng có VPO gửi TD online là :\n\nBernoulli <- rbinom(100,50,0.6)\ntable(Bernoulli)\n\nBernoulli\n22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 \n 1  2  7  6  5  6 10 14  9 16  5  5  3  5  4  2 \n\n\nBiểu đồ phân bố :\n\nhist(Bernoulli,\n     main = \"Distribution of the number customer\",\n     col = \"lightblue\")\n\n\n\n\nTính xác suất để có 35 người gửi TD Online trong mỗi đợt truyền thông:\n\ndbinom(35,50,0.6)\n\n[1] 0.04154667\n\n\n\n\n12.3.3 Phân phối Poisson\n\nThực hiện n lần biến ngẫu nhiên Y (phân phối theo quy luật không – một với tham số p), trong trường hợp n quá lớn và p quá nhỏ, thu được biến ngẫu nhiên X phân phối theo quy luật Poisson với tham số \\(\\lambda = n*p\\) \\[P_x = e^{-\\lambda}\\frac{\\lambda^{x}}{x!}\\]\nCác tham số đặc trưng :\n\n\\[E(X) = \\lambda\\]\n\\[V(X) = \\lambda\\]\n\nLưu ý: công thức Poisson có thể dùng thay cho công thức Bernoulli nếu thỏa mãn điều kiện n >= 20 và p <= 0.1 hay np ≈ np(1 – p)\nVí dụ:\nMô phỏng 100 mẫu quan sát phân phối Poisson với \\(\\lambda = 10\\)\n\nPoisson <- rpois(1000,10)\n\nhist(Poisson,\n     main = \"Distribution of Poisson\",\n     col = \"lightblue\")\n\n\n\n\nVí dụ\nQua theo dõi các đợt truyền thông cho khách hàng inactive, cứ 1000 khách hàng active trở lại thì trung bình có 5 khách hàng map ví MOMO (0.5%). Tính xác suất để có 10 khách hàng map ví? nhiều hơn 10 khách hàng map ví?\nXác suất để có 10 KH map ví MOMO:\n\\[P(X = 10|\\lambda = 5) = \\frac{\\lambda^{x}}{x!}e^{-\\lambda}\\]\n\ndpois(10,5)\n\n[1] 0.01813279\n\n\nXác suất để có nhiều hơn 10 KH map ví MOMO: \\[P(X > 10|\\lambda = 5) = 1 - P(X <= 10|\\lambda = 5)\\]\n\n1 - ppois(10,5)\n\n[1] 0.01369527\n\n\n\n\n12.3.4 Phân phối lũy thừa E(\\(\\lambda\\))\n\n\n\nQuy luật Phân phối lũy thừa\n\n\nBiến ngẫu nhiên liên tục X gọi là phân phối theo quy luật lũy thừa nếu hàm mật độ xác suất của nó có dạng :\n\\[f(x) = \\begin{cases}0 & , x < 0\\\\\\lambda e^{-\\lambda x} & , x >= 0\\end{cases}\\]\nTrong đó \\(\\lambda\\) là một hằng số dương\nCác tham số đặc trưng :\n\n\\(E(X) = \\frac{1}{\\lambda}\\)\n\\(V(X) = \\frac{1}{\\lambda^{2}}\\)\n\nTính chất của quy luật phân phối lũy thừa : Xác suất xảy ra của biến ngẫu nhiên X trong khoảng thời gian t không phụ thuộc vào quãng thời gian trước đó mà chỉ phụ thuộc vào độ dài của khoảng thời gian t dang xét.\n\n\n12.3.5 Phân phối chuẩn N(\\(\\mu\\), \\(\\sigma^{2}\\))\nBiến ngẫu nhiên liên tục X gọi là phân phối theo quy luật chuẩn với các tham số \\(\\mu\\) và \\(\\sigma^{2}\\), nếu hàm mật độ xác suất của nó có dạng : \\[f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(x - \\mu)^{2}}{2\\sigma^{2}}}\\] Các tham số đặc trưng :\n\n\\(E(X) = \\mu\\)\n\\(V(X) = \\sigma^{2}\\)\n\nVí dụ:\nMô phỏng 1000 quan sát phân phối chuẩn với \\(\\mu = 5\\), \\(\\sigma = 1\\)\n\nN <- rnorm(1000,5,1)\nhist(N,\n     main = \"Normal distribution\",\n     col = \"lightblue\")\n\n\n\n\n\nƯớc lượng khoảng tin cậy của trung bình khi biết \\(\\sigma^{2}\\) \\[\\mu_X - U_\\frac{\\alpha}{2}\\frac{\\sigma}{\\sqrt{n}} < \\mu < \\mu_X + U_\\frac{\\alpha}{2}\\frac{\\sigma}{\\sqrt{n}}\\] Trong đó :\n\\(\\mu_X\\) : Trung bình mẫu quan sát\n\\(\\sigma\\) : Độ lệch chuẩn tổng thể\n\\(n\\) : Số quan sát của mẫu\n\\(U_\\alpha\\) : giá trị của thống kê U tại mức ý nghĩa \\(\\alpha\\)\n\nVí dụ:\nƯớc lượng trung bình tổng thể, biết \\(\\sigma\\) = 2; khảo sát mẫu gồm 1000 quan sát thu được \\(\\mu_X = 10\\).\nVới mức ý nghĩa \\(\\alpha = 0.05\\), trung bình tổng thể nằm trong khoảng :\n\na <- 0.05\nU <- qnorm(1 - a/2)\nmu_X <- 10\nn <-  1000\nsigma = 2\n\nTB <- c(mu_X - U * sigma/sqrt(n), mu_X + U * sigma/sqrt(n))\nTB\n\n[1]  9.876041 10.123959\n\n\n\nƯớc lượng khoảng tin cậy của trung bình khi chưa biết \\(\\sigma^{2}\\)\n\n\\[\\mu_X - t_\\frac{\\alpha}{2}^{(n-1)}\\frac{S}{\\sqrt{n}} < \\mu(X) < \\mu_X + t_\\frac{\\alpha}{2}^{(n-1)}\\frac{S}{\\sqrt{n}}\\] Trong đó :\n\n\\(\\mu_X\\) : Trung bình các mẫu quan sát\n\\(S\\) : Độ lệch chuẩn trung bình các mẫu quan sát\n\\(n\\) : Số quan sát của mẫu\n\\(t_\\alpha\\) : giá trị của thống kê T tại mức ý nghĩa \\(\\alpha\\)\n\nVí dụ:\nƯớc lượng trung bình tổng thể, biết rằng khi khảo sát 50 mẫu (mỗi mẫu 1000 quan sát) thu được \\(\\mu_X = 10\\) và \\(S = 1\\)\nVới mức ý nghĩa \\(\\alpha = 0.05\\), trung bình tổng thể nằm trong khoảng :\n\na <- 0.05\nt <- qnorm(1-a/2)   ##Vì k = 50 > 30 nên T(n) ~ U(0,1)\nmu_X <- 10\nn <-  1000\nS = 1\n\nTB <- c(mu_X - t * S/sqrt(n), mu_X + t * S/sqrt(n))\nTB\n\n[1]  9.93802 10.06198\n\n\n\n\n12.3.6 Quy luật chi bình phương \\(\\chi^{2} (n)\\)\n\nNếu \\(X_1, X_2,...,X_k\\) là các biến ngẫu nhiên độc lập có phân phối chuẩn hóa N(0,1) thì \\[\\chi_k^{2} = \\sum{X_i} \\ \\ ; với\\ i = 1,k\\] tuân theo phân phối Chi bình phương với k bậc tự do\nTính chất của phân phối chi bình phương : Là phân phối lệch trái, khi bậc tự do tăng dần thì phân phối chi bình phương tiến gần đến phân phối chuẩn N(k,2k)\n\n\n12.3.7 Phân phối t-student\n\n\n\nQuy luật Student\n\n\nNếu Z ~ N(0,1) và \\(\\chi_k^{2}\\) độc lập thống kê thì \\[T_k = \\frac{Z}{\\sqrt{\\chi_k^{2}/k}}\\] tuân theo phân phối Student (thống kê t) với k bậc tự do\nPhân phối Student cũng đối xứng quanh 0 như phân phối chuẩn hóa nhưng thấp hơn. Khi bậc tự do càng lớn (k >= 30) thì phân phối Student tiệm cận đến phân phối chuẩn hóa N(0,1)"
  },
  {
    "objectID": "p02-15-thong-ke-co-ban.html#tài-liệu-tham-khảo",
    "href": "p02-15-thong-ke-co-ban.html#tài-liệu-tham-khảo",
    "title": "12  Xác suất và phân phối thống kê cơ bản",
    "section": "12.4 Tài liệu tham khảo",
    "text": "12.4 Tài liệu tham khảo\n\nhttps://cran.r-project.org/doc/contrib/Intro_to_R_Vietnamese.pdf\nhttps://www.slideshare.net/vomanhtai/suy-din-thng-k-v-ngn-ng-r-1-tnh-ton-xc-sut-v-m-phng-36094689"
  },
  {
    "objectID": "p02-16-power-analysis.html#giới-thiệu",
    "href": "p02-16-power-analysis.html#giới-thiệu",
    "title": "13  Power analysis",
    "section": "13.1 Giới thiệu",
    "text": "13.1 Giới thiệu\nPower Analysis để trả lời ba vấn đề sau:\n\nSố lượng mẫu cần thiết cho thống kê\nĐánh giá ảnh hưởng của cỡ mẫu trên ước lượng\nĐánh giá độ mạnh của test\n\nKiểm định thống kê (statistical hypothesis)\nTrong kiểm định thống kê, cơ bản đi qua các bước sau:\n\nBước 1: Đưa ra giả định (null hypothesis ) \\(H_0\\) và giả định thay thế (alternative hypothesis) \\(H_1\\)\nBước 2: Tính toán xác suất có thể giữ lại \\(H_0\\) p-value.\nBước 3: Ra quyết định dựa trên p-value. Nếu p-value nhỏ hơn mức ý nghĩa thống kê (significance level) \\(\\alpha\\), ta loại bỏ \\(H_0\\), sử dụng \\(H_1\\). Ta nói rằng, kết quả đưa ra với độ tin cậy về mặt thống kê là \\(1-\\alpha\\)\n\nCác loại lỗi với thống kê\nKhi làm kiểm định sẽ có các trường hợp sau:\n\nBốn khía cạnh khi xem xét kiểm định thống kê\nKhi phân tích kiểm định thống kê cần xem xét đến 4 yếu tố:\n\nSample size (cớ mẫu): Số lượng quan sát có trong mẫu\nSignificance level (ý nghĩa thống kê): Xác suất xảy ra sai số loại 1 hay xác suất KHÔNG xảy ra \\(H_0\\)\nPower (Độ mạnh): 1 trừ xác suất xảy ra lỗi loại 2\nEffect size: Giá trị thực tế - Giá trị giả thuyết. VD: \\(H_0\\): giá trị trung bình là 90, giá trị trung bình thực tế là 100. Vậy, effect size = 90 - 100 = -10. Lưu ý: Effect size phụ thuộc nhiều vào phương pháp và do đó, có nhiều cách tính khác nhau"
  },
  {
    "objectID": "p02-16-power-analysis.html#ví-dụ",
    "href": "p02-16-power-analysis.html#ví-dụ",
    "title": "13  Power analysis",
    "section": "13.2 Ví dụ",
    "text": "13.2 Ví dụ\n\npwr.t.test(n=,d=,sig.level=,power=,level=, type=, alternatie=)\n\nTrong đó:\n\nn: cỡ mẫu\nd: effect size là độ khác biệt đã được chuẩn hóa giữa 2 mẫu (standardized mean difference) \\(d=\\fraction{\\mu_1-\\mu_2}{\\sigma}\\) với \\(\\mu_1\\), \\(\\mu_2\\) là giá trị trung bình của 2 nhóm, \\(\\sigma^2\\) là là phương sai của mẫu\nsig.level: significance level\npower: power level\ntype: two-saple, on-sample\nalternativeL Xác định test là 1 phía hay hai phía\n\nVí dụ: Ta cần so sánh thời gian phản ứng khi KH gọi đến cho 2 nhóm tài xế: Nhóm 1 sử dụng di động, nhóm 2 không sử dụng di động với độ lệch chuẩn là 1.25 giây. Giả định sự khác biệt giữa 2 nhóm là 1 giây được coi là cao thì d=1/1.25=0.8. Ta muốn chắc chắn 2 điều sau:\n\n90% chắc chắn rằng xảy ra sự khác biệt (power=90%)\n95% chắc chắn rằng yếu tố xảy ra không phải do ngẫu nhiên (sig.level=5%)\n\nHỏi cẩn bao nhiêu quan sát trong mẫu?\n\n#install.packages(\"pwr\")\nlibrary(dplyr)\nlibrary(pwr)\npwr.t.test(d=.8, sig.level=.05, power=0.9,type=\"two.sample\", \n             alternative=\"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 33.82555\n              d = 0.8\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nKết quả cho thấy ta cần 34 quan sát trong mỗi nhóm tài xế (sử dụng và không sử dụng di động) để có thể phát hiện ra độ khác biệt (effect size) 0.8 với độ chắc chắn 90% và 5% xảy ra hiện tượng trên chỉ do ngẫu nhiên\nCác chỉ số khác có thể được tính toán nếu có đủ 3 trong 4 yếu tố đầu vào\n\n#Tìm power\npwr.t.test(n=34,d=.8, sig.level=.05, type=\"two.sample\", \n             alternative=\"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 34\n              d = 0.8\n      sig.level = 0.05\n          power = 0.9015019\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n#Tìm d\npwr.t.test(n=34, power=0.9, sig.level = 0.05, type=\"two.sample\", \n             alternative=\"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 34\n              d = 0.7978727\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "p02-16-power-analysis.html#hiểu-sâu-hơn-về-p-value",
    "href": "p02-16-power-analysis.html#hiểu-sâu-hơn-về-p-value",
    "title": "13  Power analysis",
    "section": "13.3 Hiểu sâu hơn về p-value",
    "text": "13.3 Hiểu sâu hơn về p-value\nĐịnh nghĩa p-value: “P value is the probability of obtaining an effect at least as extreme as the one in your sample data, assuming the truth of the null hypothesis”\nGiải thích: Nếu ta giả định \\(H_0\\) đúng, \\(H_0\\) sẽ xảy ra với xác suất p_value do yếu tố ngẫu nhiên lấy mẫu\nCác bước tính toán p-value được diễn ra theo trình tự sau:\n\nĐề ra một giả thuyết chính (\\(H_1\\))\nTừ giả thuyết chính, đưa ra giả thuyết đảo (\\(H_0\\))\nThu thập dữ liệu trên mẫu D\nTính toán xác suất XẢY RA D nếu \\(H_0\\) là sự thật, hay \\(P(D|H_0)\\)\n\nVí dụ: Giả sử ta sử dụng 2 loại vắc-xin A & B để so sánh sự khác biệt về hiệu quả trên 1000 bệnh nhân, ta thấy p-value = 4%. Điều này nghĩa là: Nếu ta giả định rằng A & B không có sự khác biệt (\\(H_0: \\mu_A=\\mu_B\\)) thì xác suất xảy ra sự khác biệt giữa A & B về mặt thống kê chỉ là 4%\nKết luận sai: Nếu loại bỏ \\(H_0\\), giữ \\(H_1\\), chỉ có 4% khả năng xảy ra sai số.\nCác bước tính toán:\n\nBước 1: \\(H_1\\) - Có sự khác biệt về hiệu quả 2 vắc xin\nBước 2: \\(H_0\\) - Không có sự khác biệt giữa hai loại vắc-xin\nBước 3: Thu thập dữ liệu trên mẫu 1000 người\nBước 4: Giả định rằng \\(H_0\\) là chính xác, tức là sự khác biệt giữa người sử dụng A và B chỉ là do yếu tố ngẫu nhiên thì xác suất tìm được mẫu D có 1000 bệnh nhân mà sự khác biệt giữa hiệu quả của A & B trong nghiên cứu chỉ là 4%\n\nNhư vậy, p-value không nói trực tiếp đến giả thuyết chính (alternative hypothesis \\(H_1\\)) mà chỉ GIÁN TIẾP đưa ta quyết định lựa chọn.\nKhiếm khuyết của p-value: P-value đưa ra xác suất (độ khả dĩ) của dữ kiện mà không đửa ra xác suất của giả thuyết"
  },
  {
    "objectID": "p02-16-power-analysis.html#một-số-test-thống-kê",
    "href": "p02-16-power-analysis.html#một-số-test-thống-kê",
    "title": "13  Power analysis",
    "section": "13.4 Một số test thống kê",
    "text": "13.4 Một số test thống kê\n\n13.4.1 Sai lầm loại 1 và loại 2\n\n\n\nSAI LẦM LOẠI 1 VÀ LOẠI 2\n\n\nP(loại 1) = \\(\\alpha\\) : mức ý nghĩa\nP(loại 2) = \\(\\beta\\) ; với \\(1 - \\beta\\) gọi là lực kiểm định\nDo \\(\\alpha\\) và \\(\\beta\\) có xu thế ngược nhau, ta cố định \\(\\alpha\\) và chọn hàm quyết định để \\(\\beta\\) nhỏ nhất (hay \\(1 - \\beta\\) lớn nhất)\nHằng số C liên quan đến sai lầm loại 1 và loại 2 :\n\n\n\nHẰNG SỐ C\n\n\n\n\n13.4.2 Kích thước mẫu đối với một chỉ số trung bình\nCông thức : \\[n = \\frac{C}{(\\frac{d}{\\sigma})^{2}}\\] Trong đó,\n\n\\(C\\) : hằng số giữa sai lầm loại 1 và loại 2\n\\(d\\) : độ dao động của trung bình mà nghiên cứu muốn chỉ ra\n\\(\\sigma\\) : căn cứ về độ dao động từ các nghiên cứu trước đó\n\nVí dụ:\nƯớc tính chiều cao ở nam giới Việt Nam, chấp nhận sai số 1 cm (d = 1), khoảng tin cậy 95% (\\(\\alpha\\) = 0.05), power = 80% (\\(\\beta\\) = 0.2). Các nghiên cứu trước đó cho thấy độ lệch chuẩn khoảng 4.6 cm.\nCông thức ước tính cỡ mẫu cần thiết cho nghiên cứu : \\[n = \\frac{C}{(\\frac{d}{\\sigma})^{2}} = \\frac{7.85}{(1/4.6)^{2}} = 166\\] Dùng hàm power.t.test có thể ước tính cỡ mẫu trên:\n\npower.t.test(delta = 1\n             ,sd = 4.6\n             ,sig.level = .05\n             ,power = .80\n             ,type = 'one.sample')\n\n\n     One-sample t test power calculation \n\n              n = 168.0131\n          delta = 1\n             sd = 4.6\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\n13.4.3 Kích thước mẫu khi so sánh hai số trung bình\nCông thức: \\[n = 2\\frac{C}{(\\frac{d}{\\sigma})^{2}}\\] Trong đó,\n\n\\(C\\) : hằng số giữa sai lầm loại 1 và loại 2\n\\(d\\) : chênh lệch trung bình giữa 2 nhóm mà nghiên cứu muốn chỉ ra\n\\(\\sigma\\) : căn cứ về độ khác biệt trung bình từ các ghiên cứu trước đó\n\nVí dụ:\nĐể đánh giá hiệu quả của chương trình tặng quà Loyalty nhằm thúc đẩy khách hàng gửi TD Online. Có 2 nhóm được chọn: nhóm 1 được tặng quà Loyalty và nhóm 2 không được tặng quà Loyalty.Hiệu quả từ các chương trình trước đó cho thấy trung bình có 5% số khách hàng gửi TD Online, độ lệch chuẩn 0.5%. Cần nghiên cứu bao nhiêu khách hàng để “chứng minh” nhóm 1 gửi TD Online nhiều hơn nhóm 2 là 1.1 lần?\nGọi trung bình của 2 nhóm là \\(\\mu_1\\) và \\(\\mu_2\\)\nNhóm 1 gửi nhiều hơn nhóm 2 là 1.1 lần nên: \\[\\mu_1 = 1.1 * \\mu_2 = 1.1 * 0.05 = 0.055\\] Chênh lệch trung bình giữa 2 nhóm : \\[d = \\mu_1 - \\mu_2 = 0.055 - 0.05 = 0.005\\]\nVới \\(\\alpha = 0.05\\), \\(power = 0.9\\) công thức ước tính cỡ mẫu cần thiết cho nghiên cứu : \\[n = 2\\frac{C}{(\\frac{d}{\\sigma})^{2}} = 2\\frac{10.51}{(0.005/0.05)^{2}} = 2102\\]\nThực hành trong R\n\npower.t.test(delta = 0.005\n             ,sd = 0.05\n             ,sig.level = .05\n             ,power = .90\n             ,type = 'two.sample')\n\n\n     Two-sample t test power calculation \n\n              n = 2102.445\n          delta = 0.005\n             sd = 0.05\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nKết quả chỉ ra rằng cần nghiên cứu trên 2102 khách hàng mỗi nhóm (tổng cộng 4204 khách hàng)"
  },
  {
    "objectID": "p02-16-power-analysis.html#kích-thước-mẫu-khi-phân-tích-phương-sai",
    "href": "p02-16-power-analysis.html#kích-thước-mẫu-khi-phân-tích-phương-sai",
    "title": "13  Power analysis",
    "section": "13.5 Kích thước mẫu khi phân tích phương sai",
    "text": "13.5 Kích thước mẫu khi phân tích phương sai\nTrong phân tích phương sai (ANOVA), hệ số trung bình bình phương phần dư (residual mean square - RMS) chính là ước tính của độ dao động khi đo lường các nhóm.\nVí dụ\nTìm sự khác biệt giữa thu nhập của 2 người A và B\nTạo biến :\n\nA <- rnorm(1000,5,1)\nB <-  rnorm(1000,6,2)\n\nA %>% summary\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.528   4.352   5.010   5.030   5.737   7.997 \n\nB %>% summary\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.2391  4.6636  6.0423  6.0567  7.4081 13.6161 \n\n\nPhân tích ANOVA\n\nx = c(rep(\"A\",1000),rep(\"B\",1000))\nx <- as.factor(x)\ny = c(A, B)\n\ndata.anov <- data.frame(y, x)\n\ndata.anov %>% summary\n\n       y           x       \n Min.   :-0.2391   A:1000  \n 1st Qu.: 4.4512   B:1000  \n Median : 5.3701           \n Mean   : 5.5432           \n 3rd Qu.: 6.4775           \n Max.   :13.6161           \n\n\n\naov.model <- aov(y ~ x, data = data.anov)\nsummary(aov.model)\n\n              Df Sum Sq Mean Sq F value Pr(>F)    \nx              1    527   527.4   198.1 <2e-16 ***\nResiduals   1998   5318     2.7                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nXác định kích thước mẫu để chỉ ra rằng có sự khác biệt giữa thu nhập của A và B\n\ngroupmeans <- c(mean(A), mean(B))\n\npower.anova.test(groups = length(groupmeans)\n                 ,between.var = var(groupmeans)\n                 ,within.var = 2.5       ##mean Sq residuals\n                 ,power = 0.90\n                 ,sig.level = 0.05) \n\n\n     Balanced one-way analysis of variance power calculation \n\n         groups = 2\n              n = 50.79123\n    between.var = 0.5273603\n     within.var = 2.5\n      sig.level = 0.05\n          power = 0.9\n\nNOTE: n is number in each group\n\n\n\n13.5.1 Sử dụng khoảng tin cậy khi ước lượng trung bình\nXuất phát từ việc ước lượng trung bình của biến ngẫu nhiên X phân phối chuẩn ta có : \\[\\mu_X - U_\\frac{\\alpha}{2}\\frac{\\sigma}{\\sqrt{n}} < \\mu < \\mu_X + U_\\frac{\\alpha}{2}\\frac{\\sigma}{\\sqrt{n}}\\]\nKhoảng tin cậy theo ước lượng này :\n\\[I = 2U_\\frac{\\alpha}{2}\\frac{\\sigma}{\\sqrt{n}} = 2 U_\\frac{\\alpha}{2}S\\sqrt{\\frac{N - n}{nN}}\\]\nTrong đó,\n\n\\(S\\) : độ lệch chuẩn của mẫu điều tra thử\n\\(N\\) : kích thước tổng thể\n\\(n\\) : kích thước mẫu quan sát\n\nVới việc lựa chọn I (biến động của giá trị trung bình), ta có thể xác định kích thước mẫu : \\[n >= \\frac {N}{1 + N(\\frac{I}{2U_\\frac{\\alpha}{2} S})^{2}}\\] Tính hệ số \\(U_{\\alpha/2}\\)\n\na = 0.05\nqnorm(1-a/2)\n\n[1] 1.959964\n\n\n\n\n13.5.2 Sử dụng tỷ lệ phương sai\nCông thức tính phương sai:\n\\[\\sigma_X^{2} >= \\frac{\\sigma^{2}}{n} \\frac{N - n}{N - 1}\\] Từ đó : \\[n >= \\frac{N\\sigma^{2}}{(N – 1)\\sigma_X^{2} + \\sigma^{2}} = \\frac{N}{1 + (N -1)\\frac{\\sigma_X^{2}}{\\sigma^{2}}}\\]\nBằng việc lựa chọn tỷ lệ của phương sai trung bình mẫu so với phương sai tổng thể ( ), ta có thể xác định kích thước mẫu."
  },
  {
    "objectID": "p02-16-power-analysis.html#tài-liệu-tham-khảo",
    "href": "p02-16-power-analysis.html#tài-liệu-tham-khảo",
    "title": "13  Power analysis",
    "section": "13.6 Tài liệu tham khảo",
    "text": "13.6 Tài liệu tham khảo\n\nVizualize type 1, type 2, power, sample size\nPower of Hypothesis test"
  },
  {
    "objectID": "p02-20-sampling-methods.html#giới-thiệu",
    "href": "p02-20-sampling-methods.html#giới-thiệu",
    "title": "14  Phương pháp ước lượng tham số thông qua lấy mẫu",
    "section": "14.1 Giới thiệu",
    "text": "14.1 Giới thiệu\nKhi cần ước lượng 1 chỉ số nào đó từ một tập dữ liệu có sẵn, ví dụ giá trị trung bình, phương sai hoặc sai số mô hình (khi áp dụng kỹ thuật dự báo), ta không thể có được tham số cần tính toán trên toàn tập (population) mà chỉ có trên dữ liệu đang có (sample). Các kỹ thuật ước lượng tham số dựa qua phương pháp lấy mẫu là phương thức áp dụng 1 kỹ thuật phân tích, thống kê trên các tập dữ liệu khác nhau từ dữ liệu gốc. Các tập dữ liệu này đều được lựa chọn từ tập dữ liệu gốc và khác nhau do sử dụng kỹ thuật lấy mẫu khác nhau. Do đó, các phương pháp áp dụng kỹ thuật này được gọi là các phương pháp lấy mẫu (resampling methods). Các phương pháp này có thể được chia làm 2 nhánh lớn:\n\nCross validation\nBootstrap"
  },
  {
    "objectID": "p02-20-sampling-methods.html#cross-validation",
    "href": "p02-20-sampling-methods.html#cross-validation",
    "title": "14  Phương pháp ước lượng tham số thông qua lấy mẫu",
    "section": "14.2 Cross-Validation",
    "text": "14.2 Cross-Validation\n\n14.2.1 Validation Set\nValidation Set Approach: Trong các kỹ thuật dự báo, ước lượng, ta có thể chia dữ liệu thành 2 phần: train \\(\\&\\) test. Mô hình thu được từ tập “train” sẽ được ứng dụng để đánh giá chất lượng trong tập test bên tập “test”\n\n\n14.2.2 Leave-One-Out Cross-Validation (LOOCV)\nLOOCV là phương pháp tương tự như Validation Set. Tuy nhiên, LOOCV khác như sau:\n\nThay vì chia thành 2 tập dữ liệu ngẫu nhiên, LOOCV chia tập dữ liệu thành n lần (n là số quan sát trong tập):\nMỗi lần chia tập dữ liệu, tập train có (n-1) quan sát, tập test có 1 quan sát\nXây dựng mô hình, tính MSE cho mỗi lần\nMSE cho cả phương pháp bằng trung bình MSE\n\nCách làm như sau:\n\nLần 1: Train = \\(\\{(x_2, y_2),...,(x_n,y_n)\\}\\), Test = \\(\\{x_1,y_1\\}\\)\nLần 2: Train = \\(\\{(x_1, y_1),(x_3, y_3)...,(x_n,y_n)\\}\\), Test = \\(\\{x_2,y_2\\}\\)\nLần n: Train = \\(\\{(x_1, y_1),...,(x_{n-1},y_{n-1})\\}\\), Test = \\({x_n,y_n}\\)\nMSE với mỗi lần chia được tính như sau:\n\n\\[MSE_1 = (y_1 - \\hat{y_1})^2\\]\n\\[MSE_2 = (y_2 - \\hat{y_2})^2\\]\n\\[MSE_n = (y_n - \\hat{y_n})^2\\]\nMSE trung bình được tính như sau:\n\\[CV_{(n)}=\\frac{1}{n} \\sum_{i=1}^n MSE_i\\]\nĐiểm mạnh của LOOCV:\n\nGiảm rõ rệt bias (sai lệch so với thực tế) so với phương pháp “Validation Set Approach” (VSA)\nTính ổn định tốt hơn so với VSA. Với VSA, mỗi lần chia thành các tập train \\(\\&\\) set đều được làm ngẫu nhiên, do đó, MSE sẽ khác với mỗi lần thực hiện mô hình\n\nĐiểm yếu của LOOCV:\n\nĐòi hỏi khối lượng tính toán rất lớn\n\n\n\n14.2.3 k-Fold Cross-Validation\nThuật toán của k-Fold như sau:\n\nBước 1: Chia tập dữ liệu thành k phần ngẫu nhiên\nBước 2: Sử dụng tập con thứ nhất sẽ được dùng làm tập Validate, tính \\(MSE_1\\)\nBước 3: Sử dụng tập con thứ hai làm tập validate, tính \\(MSE_2\\)\nLàm tương tự đến tập k\nBước 4: MSE của tập test được ước lượng bằng trung bình cộng của các MSE\n\n\\[CV_{(k)}=\\frac{1}{n}\\sum_{i=1}^{k}MSE_i\\]\n\nLưu ý:\n\nLOOCV là trường hợp đặc biệt của k-fold CV với \\(k=n\\)"
  },
  {
    "objectID": "p02-20-sampling-methods.html#boostrap",
    "href": "p02-20-sampling-methods.html#boostrap",
    "title": "14  Phương pháp ước lượng tham số thông qua lấy mẫu",
    "section": "14.3 Boostrap",
    "text": "14.3 Boostrap\nBoostrap là phương pháp rất mạnh để ước lượng mô hình và các chỉ số thống kê. Với dữ liệu gốc có n quan sát, tại mỗi bước xây dựng mô hình hoặc ước lượng chỉ số thống kê, ta lấy ra ngẫu nhiêu n quan sát từ tập dữ liệu gốc có lặp lại. Ước lượng cuỗi cùng của mô hình là ước lượng của tất cả mô hình đã xây.\nGiả sử ta cần ước lượng giá trị trung bình của X nhưng chỉ có \\(n=3\\) quan sát. Ta áp dụng thuật toán như sau:\n\nBước 1: Lấy ngẫu nhiên \\(n=3\\) quan sát bất kỳ từ tập dữ liệu, cho phép lấy lại - nghĩa là 1 quan sát có thể xuất hiện 1 hoặc nhiều lần khi lấy ngẫu nhiên\nBước 2: Tính giá trị trung bình: \\(\\hat{X_1}\\)\nBước 3: Lặp lại bước 1,2 với \\(B\\) lần đủ lớn\nBước 4: Giá trị trung bình của X được ước lượng bằng trung bình cộng của \\(\\hat{X_i}\\)\n\nGọi \\(\\alpha_j\\) là giá trị (hoặc ước lượng) của \\(\\alpha\\) trong mỗi lần lấy dữ liệu của boostrap. Gọi \\(\\bar{\\hat{\\alpha}}\\) là ước lượng của \\(alpha\\) trong toàn bộ dữ liệu, ta có:\n\\[\\bar{\\hat{\\alpha}}=\\frac{1}{B} \\sum_{j=1}^B \\alpha_j\\]\n\\[SE_B(\\hat{\\alpha})=\\sqrt{\\frac{1}{B-1}\\sum_{j=1}^{B}(\\alpha_j-\\bar{\\hat{\\alpha}})^2}\\]\n\nLưu ý:\n\nCác phương pháp áp dụng cross-valiation sẽ được sử dụng rộng rãi trong việc xây dựng mô hình dự báo."
  },
  {
    "objectID": "p02-20-sampling-methods.html#ví-dụ-với-r",
    "href": "p02-20-sampling-methods.html#ví-dụ-với-r",
    "title": "14  Phương pháp ước lượng tham số thông qua lấy mẫu",
    "section": "14.4 Ví dụ với R",
    "text": "14.4 Ví dụ với R\n\n14.4.1 Cross Validation\n\nmpg_auto <- mtcars[mtcars$am == 0,]$mpg # automatic transmission mileage\nmpg_manual <- mtcars[mtcars$am == 1,]$mpg # manual transmission mileage\ntransmission_ttest <- t.test(mpg_auto, mpg_manual)\ntransmission_ttest$p.value\n\n[1] 0.001373638\n\n\n\n\n14.4.2 Boostrap\n\nSử dụng package boot để phân tích booststrap:\n\nboot: Tính boostrap\nboot.ci: Tính confidence level\n\n\n\nlibrary(boot)\nbootobject <- boot(data = , statistic = , R= , ...)\n\n#R: Số lần muốn bootstrap\n#statistic: chỉ số thống kê muốn tính boostrap\n\nboot.ci(bootobject, conf = , type =)\n\n\nlibrary(boot)\nlibrary(dplyr)\nlibrary(broom)\nmean.fun <- function(d, i){\n    m <- mean(d$mpg[i])\n    return(m)\n}\n\n#boot với tidy\n\nmtcars %>% \n  group_by(vs) %>% \n  do(boot(., mean.fun, 1000,\n          parallel = \"multicore\") %>% tidy) %>% print\n\n# A tibble: 2 × 4\n# Groups:   vs [2]\n     vs statistic   bias std.error\n  <dbl>     <dbl>  <dbl>     <dbl>\n1     0      16.6 0.0248     0.893\n2     1      24.6 0.0561     1.37 \n\n#boot.ci\nboot.mean <- boot(mtcars, mean.fun, 1000, parallel = \"multicore\")\nboot.mean\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = mtcars, statistic = mean.fun, R = 1000, parallel = \"multicore\")\n\n\nBootstrap Statistics :\n    original   bias    std. error\nt1* 20.09062 0.050425    1.063901\n\nboot.ci(boot.mean, conf = 0.95)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot.mean, conf = 0.95)\n\nIntervals : \nLevel      Normal              Basic         \n95%   (17.95, 22.13 )   (17.85, 22.14 )  \n\nLevel     Percentile            BCa          \n95%   (18.04, 22.33 )   (17.99, 22.23 )  \nCalculations and Intervals on Original Scale\n\n#boot.ci với tidy\nmtcars %>% \n  group_by(vs) %>% \n  do({\n    boot(., mean.fun, 1000,\n          parallel = \"multicore\") -> df;\n    (df %>% boot.ci)$normal %>% tidy\n    })\n\n# A tibble: 2 × 2\n# Groups:   vs [2]\n     vs x[,1]  [,2]  [,3]\n  <dbl> <dbl> <dbl> <dbl>\n1     0  0.95  14.9  18.3\n2     1  0.95  21.8  27.3"
  },
  {
    "objectID": "p02-20-sampling-methods.html#tài-liệu-tham-khảo",
    "href": "p02-20-sampling-methods.html#tài-liệu-tham-khảo",
    "title": "14  Phương pháp ước lượng tham số thông qua lấy mẫu",
    "section": "14.5 Tài liệu tham khảo",
    "text": "14.5 Tài liệu tham khảo\n\nR in Action (chapter 12)\nIntroduction to Statistical Learning (chapter 5)"
  },
  {
    "objectID": "p02-50-datatable.html#giới-thiệu",
    "href": "p02-50-datatable.html#giới-thiệu",
    "title": "15  Biến đổi dữ liệu với data.table",
    "section": "15.1 Giới thiệu",
    "text": "15.1 Giới thiệu\ndata.table và dplyr là hai packages được sử dụng nhiều nhất trong R dùng để tính toán, biến đổi dữ liệu. Cả hai packages đều có ưu nhược điểm khác nhau.\nNgữ pháp:\n\nCấu trúc tổng quát của data.table: DT[i, j, by, ...] tức là: “Sử dụng data table DT, lọc các dòng với i, tính toán theo j, nhóm các biến theo by”. Các options khác có thể dùng …\ndplyr sử dụng 3 nhóm câu lệnh cơ bản (filter(), arrange(), select(), mutate(), summarise()) và group_by(), tập trung vào việc dễ đọc, dễ hiểu\n\nHệ sinh thái:\n\ndplyr phụ thuộc vào các package trong hệ sinh thái của tidyverse, chain operator phụ thuộc vào package magrittr\ndata.table là package đứng độc lập, có toán tử chain operator riêng\n\nTốc độ:\n\ndata.table có thể nhanh hơn dplyr nhiều lần khi làm việc với khối lượng dữ liệu lớn (lớn hơn 3 triệu dòng)"
  },
  {
    "objectID": "p02-50-datatable.html#các-câu-lệnh-cơ-bản",
    "href": "p02-50-datatable.html#các-câu-lệnh-cơ-bản",
    "title": "15  Biến đổi dữ liệu với data.table",
    "section": "15.2 Các câu lệnh cơ bản",
    "text": "15.2 Các câu lệnh cơ bản\n\nlibrary(data.table)\nlibrary(dplyr)\ndf <- data.frame(\n                v1 = 1:9,\n                v2 = sample(c(1,2), 9, T),\n                v3 = round(rnorm(3),2),\n                v4 = letters[1:3])\ndata <- df %>% setDT(key = c(\"v1\"))\ndata <- data %>% as.data.table() \ndata %>% class\n\n[1] \"data.table\" \"data.frame\"\n\n\n\n15.2.1 Lọc dữ liệu theo dòng\n\n15.2.1.1 Lọc dữ liệu theo vị trí\n\n# Theo vị trí\ndata[3:4,]\n\n   v1 v2    v3 v4\n1:  3  2  0.78  c\n2:  4  1 -1.46  a\n\ndata[3:4] \n\n   v1 v2    v3 v4\n1:  3  2  0.78  c\n2:  4  1 -1.46  a\n\n\n\n\n15.2.1.2 Theo dấu -\n\ndata[!3:7,]\n\n   v1 v2    v3 v4\n1:  1  1 -1.46  a\n2:  2  1 -0.07  b\n3:  8  2 -0.07  b\n4:  9  1  0.78  c\n\ndata[-(3:7)] # same\n\n   v1 v2    v3 v4\n1:  1  1 -1.46  a\n2:  2  1 -0.07  b\n3:  8  2 -0.07  b\n4:  9  1  0.78  c\n\n\n\n\n15.2.1.3 Lọc theo điều kiện logic\n\ndata[v1 > 5,]\n\n   v1 v2    v3 v4\n1:  6  1  0.78  c\n2:  7  2 -1.46  a\n3:  8  2 -0.07  b\n4:  9  1  0.78  c\n\ndata[v4 %in% c(\"a\",\"c\"),] \n\n   v1 v2    v3 v4\n1:  1  1 -1.46  a\n2:  3  2  0.78  c\n3:  4  1 -1.46  a\n4:  6  1  0.78  c\n5:  7  2 -1.46  a\n6:  9  1  0.78  c\n\n\n\n\n15.2.1.4 Nhiều điều kiện\n\ndata[v1 > 5 & v4 == \"a\",]\n\n   v1 v2    v3 v4\n1:  7  2 -1.46  a\n\n\n\n\n15.2.1.5 Lấy unique\n\nunique(data)\n\n   v1 v2    v3 v4\n1:  1  1 -1.46  a\n2:  2  1 -0.07  b\n3:  3  2  0.78  c\n4:  4  1 -1.46  a\n5:  5  1 -0.07  b\n6:  6  1  0.78  c\n7:  7  2 -1.46  a\n8:  8  2 -0.07  b\n9:  9  1  0.78  c\n\n\n\n\n15.2.1.6 Sort\n\ndata[order(v3)]\n\n   v1 v2    v3 v4\n1:  1  1 -1.46  a\n2:  4  1 -1.46  a\n3:  7  2 -1.46  a\n4:  2  1 -0.07  b\n5:  5  1 -0.07  b\n6:  8  2 -0.07  b\n7:  3  2  0.78  c\n8:  6  1  0.78  c\n9:  9  1  0.78  c\n\ndata[order(desc(v3))]\n\n   v1 v2    v3 v4\n1:  3  2  0.78  c\n2:  6  1  0.78  c\n3:  9  1  0.78  c\n4:  2  1 -0.07  b\n5:  5  1 -0.07  b\n6:  8  2 -0.07  b\n7:  1  1 -1.46  a\n8:  4  1 -1.46  a\n9:  7  2 -1.46  a\n\n\n\n\n\n15.2.2 Select cột\n\n15.2.2.1 Một cột\n\ndata[,\"v3\"]  # Trả ra DT\n\n      v3\n1: -1.46\n2: -0.07\n3:  0.78\n4: -1.46\n5: -0.07\n6:  0.78\n7: -1.46\n8: -0.07\n9:  0.78\n\ndata[[\"v3\"]] # Trả ra vector\n\n[1] -1.46 -0.07  0.78 -1.46 -0.07  0.78 -1.46 -0.07  0.78\n\n\n\n\n15.2.2.2 Nhiều cột\n\ndata[, c(\"v3\", \"v2\")]\n\n      v3 v2\n1: -1.46  1\n2: -0.07  1\n3:  0.78  2\n4: -1.46  1\n5: -0.07  1\n6:  0.78  1\n7: -1.46  2\n8: -0.07  2\n9:  0.78  1\n\ndata[, .(v2, v3)]\n\n   v2    v3\n1:  1 -1.46\n2:  1 -0.07\n3:  2  0.78\n4:  1 -1.46\n5:  1 -0.07\n6:  1  0.78\n7:  2 -1.46\n8:  2 -0.07\n9:  1  0.78\n\ndata[, list(v2, v3)]\n\n   v2    v3\n1:  1 -1.46\n2:  1 -0.07\n3:  2  0.78\n4:  1 -1.46\n5:  1 -0.07\n6:  1  0.78\n7:  2 -1.46\n8:  2 -0.07\n9:  1  0.78\n\ndata[, !c(\"v3\", \"v2\")]\n\n   v1 v4\n1:  1  a\n2:  2  b\n3:  3  c\n4:  4  a\n5:  5  b\n6:  6  c\n7:  7  a\n8:  8  b\n9:  9  c\n\n\n\n\n15.2.2.3 Theo variable của biến\n\ncols <- c(\"v2\", \"v3\")\ndata[, ..cols]\n\n   v2    v3\n1:  1 -1.46\n2:  1 -0.07\n3:  2  0.78\n4:  1 -1.46\n5:  1 -0.07\n6:  1  0.78\n7:  2 -1.46\n8:  2 -0.07\n9:  1  0.78\n\n\n\n\n\n15.2.3 Tổng hợp dữ liệu\n\n15.2.3.1 Tổng hợp đơn giản\n\ndata[, .(sum_v2 = sum(v2),\n         sd_v2 = sd(v2)), by = v4]\n\n   v4 sum_v2     sd_v2\n1:  a      4 0.5773503\n2:  b      4 0.5773503\n3:  c      4 0.5773503\n\ndata[1:7, .(sum_v2 = sum(v1),\n         sd_v2 = sd(v1)), by = .(v2, v4)]\n\n   v2 v4 sum_v2   sd_v2\n1:  1  a      5 2.12132\n2:  1  b      7 2.12132\n3:  2  c      3      NA\n4:  1  c      6      NA\n5:  2  a      7      NA\n\n\n\n\n\n15.2.4 Update dữ liệu\n\ndata[, v2_new := v2^2]\ndata\n\n   v1 v2    v3 v4 v2_new\n1:  1  1 -1.46  a      1\n2:  2  1 -0.07  b      1\n3:  3  2  0.78  c      4\n4:  4  1 -1.46  a      1\n5:  5  1 -0.07  b      1\n6:  6  1  0.78  c      1\n7:  7  2 -1.46  a      4\n8:  8  2 -0.07  b      4\n9:  9  1  0.78  c      1\n\n\n\n\n15.2.5 .SD, SDcols\n\nHàm .SD cho phép tính toán nhanh 1 hàm nhất định vào nhiều cột\n.SDcols cho phép liệt kê các cột cần được tính toán\n\n\ndata[, lapply(.SD, mean)]\n\n   v1       v2    v3 v4 v2_new\n1:  5 1.333333 -0.25 NA      2\n\ndata[, lapply(.SD, mean), .SDcols = c(\"v1\", \"v2\")]\n\n   v1       v2\n1:  5 1.333333\n\n\n\nTổng hợp nhiều hàm với nhiều cột\n\n\ndata[, \n     c(lapply(.SD, mean),\n       max = lapply(.SD, max)),\n     by = v4,\n     .SDcols = c(\"v1\", \"v2\")]\n\n   v4 v1       v2 v1 v2\n1:  a  4 1.333333  7  2\n2:  b  5 1.333333  8  2\n3:  c  6 1.333333  9  2\n\n\n\n\n15.2.6 Chain operator\n\ndata[1:5][\n  , .(sum_v3 = sum(v3)), by = v4][\n    sum_v3 > 0\n  ]\n\n   v4 sum_v3\n1:  c   0.78"
  },
  {
    "objectID": "p02-50-datatable.html#dtplyr",
    "href": "p02-50-datatable.html#dtplyr",
    "title": "15  Biến đổi dữ liệu với data.table",
    "section": "15.3 dtplyr",
    "text": "15.3 dtplyr\ndplyr mạnh hơn datatable về độ tường minh trong câu lệnh và có thể kết hợp với tidyverse. Trong khi đó, datatable lại nhanh hơn trong cấu phần xử lý số liệu. Để kết hợp ưu điểm của cả 2, ta có thể sử dụng packages dtplyr.\n\nlibrary(dtplyr)\nlibrary(dplyr)\nlibrary(data.table)\n\niris2 <- iris %>% lazy_dt()\n\niris2 %>% \n  group_by(Species) %>% \n  summarise(no = n(),\n            mean_sepal = mean(Sepal.Length)) %>% \n  show_query()\n\n`_DT1`[, .(no = .N, mean_sepal = mean(Sepal.Length)), keyby = .(Species)]"
  },
  {
    "objectID": "p02-50-datatable.html#tài-liệu-tham-khảo",
    "href": "p02-50-datatable.html#tài-liệu-tham-khảo",
    "title": "15  Biến đổi dữ liệu với data.table",
    "section": "15.4 Tài liệu tham khảo",
    "text": "15.4 Tài liệu tham khảo\n\nhttps://atrebas.github.io/post/2019-03-03-datatable-dplyr/"
  },
  {
    "objectID": "p02-60-thu-thap-du-lieu-tu-website.html#rvest",
    "href": "p02-60-thu-thap-du-lieu-tu-website.html#rvest",
    "title": "16  Thu thập dữ liệu từ website",
    "section": "16.1 rvest",
    "text": "16.1 rvest\nrvest là package cho phép đọc dữ liệu html từ web trong hệ sinh thái của tidyverse. Sử dụng rvest cho phép thu thập dữ liệu một cách nhanh chóng từ web.\nMỗi thành phần của 1 file html được cấu thành từ các tag và các đối tượng nhất định được quy định bởi css của website đó. Khi thu thập dữ liệu từ web, ta cần biết được chính xác đối tượng ta muốn thu thập dữ liệu đang được gắn với tag nào của html. Có hai cách để biết được object thuộc tag nào:\n\nSử dụng Chrome >> F12\nSử dụng selectorgadget addin của chrome để xác định node\n\n\nvignette(\"selectorgadget\")\n\nVí dụ 1:\n\nurl <- 'http://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature'\n\nwebpage <- read_html(url)\nwebpage %>% \n  html_nodes(\".text-primary\") %>% \n  html_text %>% \n  as.numeric\n\nwebpage %>% \n  html_nodes('.lister-item-header a') %>%  \n  html_text\n\nwebpage %>% \n  html_nodes('.ratings-bar+ .text-muted') %>%  \n  html_text\n\nwebpage %>% \n  html_nodes('table') %>%  \n  html_text\n\nVí dụ 2\n\nlibrary(rvest)\nlego_movie <- read_html(\"http://www.imdb.com/title/tt1490017/\")\n\nlego_movie %>%\n  html_nodes(\"table\") %>%\n  html_table() %>% \n  as.data.frame %>% View"
  },
  {
    "objectID": "p02-60-thu-thap-du-lieu-tu-website.html#ứng-dụng---scrape-techcombank-atm",
    "href": "p02-60-thu-thap-du-lieu-tu-website.html#ứng-dụng---scrape-techcombank-atm",
    "title": "16  Thu thập dữ liệu từ website",
    "section": "16.2 Ứng dụng - Scrape Techcombank ATM",
    "text": "16.2 Ứng dụng - Scrape Techcombank ATM\n\nbranch_full <- character()\nfor (i in 1:4){\nurl <- paste0('https://www.techcombank.com.vn/mang-luoi-dia-diem-atm/danh-sach-chi-nhanh-phong-giao-dich-va-atm?page=',i)\n\nwebpage <- read_html(url, encoding = \"UTF-8\")\nwebpage %>% \n  html_nodes(\".title-entries a\") %>% \n  html_text %>% \n  as.character -> branch\nbranch_full <- c(branch_full, branch)\n}\ndf <- data.frame(branch = branch_full)\ndf %>% DT::datatable()\n\nlibrary(rvest)\nlibrary(dplyr)\ni <- 1\nurl <- paste0('https://www.techcombank.com.vn/mang-luoi-dia-diem-atm/danh-sach-chi-nhanh-phong-giao-dich-va-atm?page=',i)\n\nwebpage <- read_html(iconv(url,  from = \"ISO-8859-1\", to = \"UTF-8\"), encoding = \"UTF-8\")\nwebpage <- read_html(url, encoding = \"UTF-8\")\n\nwebpage <- read_html(iconv(url,  \n                          # from = \"ISO-8859-1\", \n                           to = \"UTF-8\"), encoding = \"utf8\")\n\nwebpage %>% \n  html_nodes(\".title-entries a\") %>% \n  html_text %>% \n  as.character %>% \n  repair_encoding() -> branch\ndf <- data.frame(branch = branch)\ndf"
  },
  {
    "objectID": "p02-60-thu-thap-du-lieu-tu-website.html#sử-dụng-rselenium",
    "href": "p02-60-thu-thap-du-lieu-tu-website.html#sử-dụng-rselenium",
    "title": "16  Thu thập dữ liệu từ website",
    "section": "16.3 Sử dụng RSelenium",
    "text": "16.3 Sử dụng RSelenium\nrvest rất thuận tiện trong việc thu thập dữ liệu. Tuy nhiên, khi làm việc với website, ta phải làm việc cùng lúc với nhiều trag web và tương tác với các thành phần khác nhau trong website. Ví dụ:\n\nClick vào login\nNhập mật khẩu\nClose một cửa sổ\n\nĐể giải quyết vấn đề trên, ta cần phải sử dụng selenium và RSelenium. Các bước sử dụng selenium và RSelenium:\n\nDownload selenium server standalone\nDonwload chrome driver\nSử dụng RSelenium và điều hướng website\n\nCách khởi động server\n\nCách 1: Khởi động server tự động\n\n\nselServ <- selenium(verbose = TRUE) #installs selenium\nselServ$process\n\n\nCách 2: Chạy server thủ công\n\n\nd:\n\ncd D:\\OneDrive - VPBank\\1. Personal projects\\Crawl data\\selenium\n\njava -jar selenium-server-standalone-2.47.0.jar -Dwebdriver.chrome.driver=./chromedriver.exe\n\n\nSử dụng Selenium để điều hướng web\n\n\n#Step 1: Khởi động server\nlibrary(dplyr)\nlibrary(RSelenium)\nremDr <- remoteDriver(remoteServerAddr = \"127.0.0.1\"\n                      , port = 4444\n                      , browserName = \"chrome\")\nremDr$open(silent = F)\n\n#Gọi website\nremDr$navigate(\"http://www.facebook.com\")\nremDr$navigate(\"http://vietnamnet.vn/\")\nremDr$getCurrentUrl()\nremDr$goBack()\nremDr$goForward()\n\n\nChèn các đối tượng vào server\n\n\nurl <- \"http://email-dbs.vpbank.com.vn/admin/index.php\"\nremDr$navigate(url)\n#Loại 1: Chèn thông tin\nwebElem_1 <- remDr$findElement(using = \"css\", \"[name = 'ss_username']\")\nwebElem_1$sendKeysToElement(list('Ebank'))\nwebElem_2 <- remDr$findElement(using = \"css\", \"[name = 'ss_password']\")\nwebElem_2$sendKeysToElement(list('NganHangSo@15092016'))\nwebElem_3 <- remDr$findElement(using = \"css\", \"[name = 'ss_takemeto']\")\nwebElem_3$sendKeysToElement(list('My Campaign Statistics'))\n\n#Loại 2: Chèn click\nwebElem_4 <- remDr$findElement(using = \"css\", \"[name = 'SubmitButton']\")\nwebElem_4$clickElement()\n\n\nCrawl data bằng rvest hoặc save toàn bộ dữ liệu\n\n\nurl <- \"http://email-dbs.vpbank.com.vn/admin/index.php?Page=Stats&Action=Newsletters&SubAction=ViewSummary&id=763#\"\nremDr$navigate(url)\n\nlibrary(XML)\nhtml_source <- remDr$getPageSource()[[1]]\nhtml_parse <- html_source %>% htmlParse\ndoc <- readHTMLTable(html_parse)\ndoc[[8]] %>% head\n\nCách 2\n\nhtml_source <- remDr$getPageSource()[[1]]\nhtml_source %>% \n  write.table(file = \"my_html.html\", \n                           row.names = F,\n                           quote = F,\n                           col.names = F,\n                           fileEncoding = \"utf8\")\nhtml_source %>% class\nread_html(\"my_html.html\", encoding = \"UTF-8\") %>% \n  html_nodes(\"table\") -> my_table\nmy_table[[8]] %>% html_table -> pop"
  },
  {
    "objectID": "p02-60-thu-thap-du-lieu-tu-website.html#sử-dụng-seleniumpipe",
    "href": "p02-60-thu-thap-du-lieu-tu-website.html#sử-dụng-seleniumpipe",
    "title": "16  Thu thập dữ liệu từ website",
    "section": "16.4 Sử dụng SeleniumPipe",
    "text": "16.4 Sử dụng SeleniumPipe\nBên cạnh RSelenium, ta có thể sử dụng SeleniumPipe để điều hướng website sử dụng toán tử %>%\n\nremDr <- remoteDr(port = 4444\n                      , browserName = \"chrome\")\n\nremDr %>% go(\"http://www.google.com/ncr\")\n\nwebElem <- remDr %>% findElement(\"name\", \"q\")"
  },
  {
    "objectID": "p02-60-thu-thap-du-lieu-tu-website.html#tài-liệu-tham-khảo",
    "href": "p02-60-thu-thap-du-lieu-tu-website.html#tài-liệu-tham-khảo",
    "title": "16  Thu thập dữ liệu từ website",
    "section": "16.5 Tài liệu tham khảo",
    "text": "16.5 Tài liệu tham khảo\n\nRSelenium"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#format-nhanh-tất-cả-code",
    "href": "p02-99-meo-trong-r.html#format-nhanh-tất-cả-code",
    "title": "17  Các mẹo trong R",
    "section": "17.1 Format nhanh tất cả code",
    "text": "17.1 Format nhanh tất cả code\n\nLựa chọn toàn bộ nhóm code cần format, nhấn tổ hợp phím Ctral + Shift + A"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#hiển-thị-số-bình-thường",
    "href": "p02-99-meo-trong-r.html#hiển-thị-số-bình-thường",
    "title": "17  Các mẹo trong R",
    "section": "17.2 Hiển thị số bình thường",
    "text": "17.2 Hiển thị số bình thường\nVấn đề: Khi sử dụng R, dữ liệu thường xuyên hiển thị dưới dạng khoa học (scientific).\nGiải pháp: Sử dụng options options(scipen = 999) để bỏ hiển thị dạng khoa học trong R"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#export-dữ-liệu-ra-excel",
    "href": "p02-99-meo-trong-r.html#export-dữ-liệu-ra-excel",
    "title": "17  Các mẹo trong R",
    "section": "17.3 Export dữ liệu ra excel",
    "text": "17.3 Export dữ liệu ra excel\nVấn đề: Export dữ liệu từ dataframe ra excel Giải pháp: Sử dụng openxlsx để export dữ liệu từ dataframe ra excel\n\nlibrary(openxlsx)\nlibrary(tidyverse)\n\n# Tạo dữ liệu ----\n\ndf <- data.frame(\"Date\" = Sys.Date()-0:4,\n                 \"Logical\" = c(TRUE, FALSE, TRUE, TRUE, FALSE),\n                 \"Currency\" = paste(\"$\",-2:2),\n                 \"Accounting\" = -2:2,\n                 \"hLink\" = \"https://CRAN.R-project.org/\",\n                 \"Percentage\" = seq(-1, 1, length.out=5),\n                 \"TinyNumber\" = runif(5) / 1E9, stringsAsFactors = FALSE)\ndf$Date <- df$Date %>% as.character()\nclass(df$Currency) <- \"currency\"\nclass(df$Accounting) <- \"accounting\"\nclass(df$hLink) <- \"hyperlink\"\nclass(df$Percentage) <- \"percentage\"\nclass(df$TinyNumber) <- \"scientific\"\n\n## Format ----\noptions(\"openxlsx.borderStyle\" = \"thin\")\noptions(\"openxlsx.borderColour\" = \"#4F81BD\")\n\n## Heading format\nhs1 <- createStyle(fgFill = \"darkgreen\", \n                   halign = \"CENTER\", textDecoration = \"Bold\",\n                   border = \"Bottom\", fontColour = \"white\")\n\n## Insert data (simple) ----\nwb <- createWorkbook()\naddWorksheet(wb, \"writeData auto-formatting\")\n\nwriteData(wb, 1, df, startRow = 1, startCol = 1, headerStyle = hs1,\n          borders = \"rows\", borderStyle = \"thin\")\n\n## Thêm dữ liệu (datatable)\n\naddWorksheet(wb, \"Sheet2\")\nsetColWidths(wb, 2, 1:100, widths = \"auto\")\n\nwriteDataTable(wb, 2, df, startRow = 1, startCol = 1, \n               tableStyle = \"TableStyleLight1\",\n               headerStyle = hs1,\n               withFilter = F)\nopenXL(wb) \n\n## Xóa gridLine cho tât cả các sheet ----\n\n1:length(wb$sheet_names) %>% \n  map(as_mapper(function(x){\n    showGridLines(wb, x, showGridLines = F)\n  }))\n\n# Lưu dữ liệu ----\nsaveWorkbook(wb, file = \"my_file.xlsx\", overwrite = TRUE)"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#làm-việc-khi-proxy-bị-chặn",
    "href": "p02-99-meo-trong-r.html#làm-việc-khi-proxy-bị-chặn",
    "title": "17  Các mẹo trong R",
    "section": "17.4 Làm việc khi proxy bị chặn",
    "text": "17.4 Làm việc khi proxy bị chặn\n\nSửa option chọn mặc định proxy: Tools-Global >> Options-Packages >> Uncheck Use Internet Explorer library/proxy for HTTP\nRestart lại R\nGõ câu lệnh file.edit('~/.Renviron')\nThêm nội dung sau vào trong R\n\n\noptions(internet.info = 0)\nhttp_proxy=\"http://user_id:password@your_proxy:your_port\"\n# Ví dụ\nhttp_proxy=\"http://anhhd3:password*@10.128.10.88:8080\""
  },
  {
    "objectID": "p02-99-meo-trong-r.html#tự-động-chạy-r-script-theo-lịch",
    "href": "p02-99-meo-trong-r.html#tự-động-chạy-r-script-theo-lịch",
    "title": "17  Các mẹo trong R",
    "section": "17.5 Tự động chạy R-script theo lịch",
    "text": "17.5 Tự động chạy R-script theo lịch\nCách 1: Sử dụng package taskscheduleR\n\nCài packages taskscheduleR\n\n\ndevtools::install_github(\"jwijffels/taskscheduleR\", force = T)\nlibrary(taskscheduleR)\n\n\nSử dụng add-in: Tools >> Addin >> Schedule R Script\nLựa chọn Script cần chạy\n\nCách 2: Chạy Rscript trên file bat, đặt lịch với Task Schedule của Windows\n\nBước 1: Tạo file.bat có câu lệnh sau:\n\n\n@echo off\n\"C:\\Program Files\\R\\R-3.2.5\\bin\\x64\\R.exe\" CMD BATCH --vanilla --slave \"C:\\Users\\MyPC\\Desktop\\Automation\\task.R\"\n\nHoặc\n\n@echo off\n\"C:\\Program Files\\R\\R-3.2.5\\bin\\x64\\Rscript.exe\" \"C:\\Users\\MyPC\\Desktop\\Automation\\task.R\"\n\nTrong đó:\n\n“C:Files.5.exe” là đường dẫn chạy file R.exe\n“C:.R” là đường dẫn của script cần đặt lịch\nBước 2: Sử dụng Windows Task Schedule:\n\nKhởi động “Task Schedule”\nTạo task mới: “Create basic task”\nChọn action: “Start a program”\nChọn script dẫn đến file.bat đã tạo\n\n\nTham khảo: Xem folder Automation\nLưu ý: Nếu file script R để tạo report với Rmarkdown, ta cần cài đặt pandoc trước.\n\ninstall.packages(\"installr\")\nlibrary(installr)\ninstall.pandoc()"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#đưa-tham-số-tử-cmd-vào-r",
    "href": "p02-99-meo-trong-r.html#đưa-tham-số-tử-cmd-vào-r",
    "title": "17  Các mẹo trong R",
    "section": "17.6 Đưa tham số tử CMD vào R",
    "text": "17.6 Đưa tham số tử CMD vào R\nVấn đề: Khi tạo theo project, file R script không thể chạy tự động với đường dẫn tương đối (copy thư mục từ phần này sang phần khác vẫn chạy) trong file .bat\nGiải quyết:\n\nTạo file bat và truyền tham số thư mục của file bat vào R\n\nFile bat\n\n@echo off\n\"C:\\Program Files\\R\\R-3.3.1\\bin\\x64\\Rscript.exe\" %cd%/\"test_script.R\" %cd%\n\nTrong đó:\n\nC:\\Program Files\\R\\R-3.3.1\\bin\\x64\\Rscript.exe: Đường dẫn vào thư mục chạy R\n%cd%/\"test_script.R: Đường dẫn tương đối của %cd% để chạy file test_script.R\n%cd%: Tham số đường dẫn thư mục\n\nFile R:\n\nargs <- commandArgs(trailingOnly=TRUE)\nif (length(args) != 0){\n  setwd(args)  \n}\n\n\nTruyền tham số %cd% vào R script. Nếu tham số rỗng (length = 0), giữ nguyên directory. Nêu tham số khác 0, đặt working directory là tham số được truyền vào"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#tạo-script-tự-động-tạo-header-thực-hiện-code",
    "href": "p02-99-meo-trong-r.html#tạo-script-tự-động-tạo-header-thực-hiện-code",
    "title": "17  Các mẹo trong R",
    "section": "17.7 Tạo script tự động tạo header & thực hiện code",
    "text": "17.7 Tạo script tự động tạo header & thực hiện code\nVấn đề:\n\nCho phép tự động hóa toàn bộ quá trình tạo các phân tích tự động\nXem thêm thư mục Automation multiple reports\nTham khảo Github"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#đọc-nhiều-file-có-cùng-cấu-trúc-dữ-liệu-trong-một-thư-mục",
    "href": "p02-99-meo-trong-r.html#đọc-nhiều-file-có-cùng-cấu-trúc-dữ-liệu-trong-một-thư-mục",
    "title": "17  Các mẹo trong R",
    "section": "17.8 Đọc nhiều file có cùng cấu trúc dữ liệu trong một thư mục",
    "text": "17.8 Đọc nhiều file có cùng cấu trúc dữ liệu trong một thư mục\n\n#Load multiple data in R with the same structure of variables (column)\n#Create a function in R regarding path destination\nload_data <- function(path) { \n  files <- dir(path, pattern = '\\\\.csv', full.names = TRUE)\n  tables <- lapply(files, read.csv)\n  do.call(rbind, tables)\n}\n\n#Read data from the path\nExample_data<-load_data(\"C:/Users/anhhd2/Desktop/App downloads data\")"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#wrap-biểu-đồ-có-factor-quá-dài",
    "href": "p02-99-meo-trong-r.html#wrap-biểu-đồ-có-factor-quá-dài",
    "title": "17  Các mẹo trong R",
    "section": "17.9 Wrap biểu đồ có factor quá dài",
    "text": "17.9 Wrap biểu đồ có factor quá dài\n\nProblem: Một số factor quá dài, khi lên biểu đồ bị chèn chữ\nSolution: Sử dụng package stringr, hàm str_wrap\n\n\nlibrary(ggplot2)\nlibrary(stringr)\ndata <- data.frame(x = c(\"Trung tam kinh doanh dien tu dien may\", \"Viet Nam\", \"Cong hoa xa hoi chu nghia Viet Nam\"),\n                   y = c(5,3,7))\n\n#Vấn đề với biểu đồ thường\np <- ggplot(data, aes(x, y)) + geom_bar(stat = \"identity\", aes(fill = x))\n#Giải pháp với str_wrap\np + scale_x_discrete(labels = str_wrap(levels(data$x), width = 20)) + \n  ggtitle(str_wrap(\"Viet Nam que huong toi rat trong xanh nhieu may\", width = 25))"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#đọc-dữ-liệu-utf-8-trên-r",
    "href": "p02-99-meo-trong-r.html#đọc-dữ-liệu-utf-8-trên-r",
    "title": "17  Các mẹo trong R",
    "section": "17.10 Đọc dữ liệu UTF-8 trên R",
    "text": "17.10 Đọc dữ liệu UTF-8 trên R\n\nR KHÔNG hỗ trợ đọc dữ liệu có ký tự UTF-8 trên data frame\nR hỗ trợ dữ liệu UTF-8 dạng vector hoặc list trên Windows, đây là lỗi của Windows. Khi chạy R trên Ubuntu hoặc MacOS, kết quả hiển thị dạng bình thường\n\n\nx<-c(\"Đức Anh\", \"Việt Nam\", \"Dịch vụ\")\ny<-c(1,2,3)\nz<-data.frame(x,y)\n#z không hiển thị được UTF-8\nz\n#Biến x (vector) thể hiện được\nas.character(z$x)"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#kiểm-tra-locale-trên-r",
    "href": "p02-99-meo-trong-r.html#kiểm-tra-locale-trên-r",
    "title": "17  Các mẹo trong R",
    "section": "17.11 Kiểm tra locale trên R",
    "text": "17.11 Kiểm tra locale trên R\n\n#Kiểm tra đọc dữ liệu UTF-8\nl10n_info()\n\n#Đọc dữ liệu UTF-8\noptions(RCurlOptions = list(cainfo = system.file(\"CurlSSL\", \"cacert.pem\", package = \"RCurl\")))\nSys.setlocale(\"LC_CTYPE\", \"vi_VI.UTF-8\")"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#không-save-được-ký-tự-utf-8-với-pdf-hoặc-render-từ-rmarkdown-ra-pdf",
    "href": "p02-99-meo-trong-r.html#không-save-được-ký-tự-utf-8-với-pdf-hoặc-render-từ-rmarkdown-ra-pdf",
    "title": "17  Các mẹo trong R",
    "section": "17.12 Không save được ký tự UTF-8 với PDF hoặc render từ rmarkdown ra PDF",
    "text": "17.12 Không save được ký tự UTF-8 với PDF hoặc render từ rmarkdown ra PDF\n\nSử dụng packages “Cairo”, thêm options dev = \"CairoPDF"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#thay-đổi-thư-viện-mặc-định-trên-r",
    "href": "p02-99-meo-trong-r.html#thay-đổi-thư-viện-mặc-định-trên-r",
    "title": "17  Các mẹo trong R",
    "section": "17.13 Thay đổi thư viện mặc định trên R",
    "text": "17.13 Thay đổi thư viện mặc định trên R\n\nVấn đề: Tự động đổi thư viện cài đặt trên R\nXử lý:\n\nCopy file Rprofile ra ngoài Desktop: C:/Program Files/R/R-3.3.1/library/base/R\nThêm câu lệnh: .libPaths(\"Đường dẫn về library\")\nCopy đè vào thư mục gốc\nKhởi động lại R"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#xóa-file-lưu-trữ-trên-máy-tính-từ-r",
    "href": "p02-99-meo-trong-r.html#xóa-file-lưu-trữ-trên-máy-tính-từ-r",
    "title": "17  Các mẹo trong R",
    "section": "17.14 Xóa file lưu trữ trên máy tính từ R",
    "text": "17.14 Xóa file lưu trữ trên máy tính từ R\n\nVấn đề: Muốn xóa các file trung gian lưu trữ trên máy tính trong quá trình thực hiện code\nXử lý: Thực hiện đoạn code sau\n\n\n#Bước 1: Khai báo file muốn xóa\nfn <- \"transaction.txt\"\n#Bước 2: Xóa file đã khai báo\nif(file.exists(fn)) file.remove(fn)"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#cài-đặt-qua-proxy",
    "href": "p02-99-meo-trong-r.html#cài-đặt-qua-proxy",
    "title": "17  Các mẹo trong R",
    "section": "17.15 Cài đặt qua proxy",
    "text": "17.15 Cài đặt qua proxy\n\n#Cách 1\nlibrary(downloader)\n\n#Sửa lại tên và đường dẫn theo github\ndownload(\"https://github.com/hoxo-m/easyRFM/archive/master.tar.gz\", \n         \"easyRFM.tar.gz\")\ninstall.packages(\"easyRFM.tar.gz\", repos = NULL, type = \"source\")\n\n\n#Cách 2\nlibrary(httr)\nlibrary(devtools)\n\n#Sửa lại proxy\nhttr::set_config(httr::use_proxy(\"10.36.22.22:8080\"))\n#Install như bình thường\ndevtools::install_github(\"rstats-db/odbc\")\n# Reset proxy\nhttr::reset_config()\n\nTạo hàm download qua github\n\ninstall_github_proxy <- function(repos){\n  require(stringr)\n  require(dplyr)\n  require(downloader)\n  \n  #Bước 1: Tạo link và package\n  repos_extract <- repos  %>% str_split(\"/\")\n  author <- repos_extract[[1]][1]\n  package <- repos_extract[[1]][2]\n  #Install\n  link_github <- paste0(\"https://github.com/\", repos, \"/archive/master.tar.gz\")\n  package <- paste0(package, \".tar.gz\")\n  #Download\n  download(link_github, package)\n  install.packages(package, repos = NULL, type = \"source\")\n  \n  #Bước 3: Xóa file tar\n  if (file.exists(package)){file.remove(package)}\n}\n\ninstall_github_proxy(\"ramnathv/slidifyLibraries\")"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#làm-việc-với-file",
    "href": "p02-99-meo-trong-r.html#làm-việc-với-file",
    "title": "17  Các mẹo trong R",
    "section": "17.16 Làm việc với file",
    "text": "17.16 Làm việc với file\n\n17.16.1 Metadata\n\nDanh sách các file: list.files()\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlist.files() %>% head(10)\n\n [1] \"_book\"                               \"_extensions\"                        \n [3] \"_freeze\"                             \"_quarto.yml\"                        \n [5] \"cover.png\"                           \"data\"                               \n [7] \"Images\"                              \"index.html\"                         \n [9] \"index.qmd\"                           \"p02-01-gioi-thieu-co-ban-ve-r_files\"\n\n#Danh sách các file\nmy_file <- data.frame(file = list.files())\n\n#Map thêm thời gian mởi file\ndf <- as.character(my_file$file) %>% \n  map_df(file.info) %>% \n  select(mtime)\nmy_file$mtime <- df$mtime\n\n#Separate theo định dạng cấu trúc\nmy_file <- my_file %>% \n  tidyr::separate(file, \n                  into = c(\"file_name\", \"index\", \"type\"), remove = F) \nmy_file %>% head\n\n         file file_name      index type               mtime\n1       _book                 book <NA> 2023-04-17 14:35:54\n2 _extensions           extensions <NA> 2023-04-17 02:47:27\n3     _freeze               freeze <NA> 2023-04-17 14:36:57\n4 _quarto.yml               quarto  yml 2023-04-17 13:41:59\n5   cover.png     cover        png <NA> 2023-04-17 11:35:40\n6        data      data       <NA> <NA> 2023-04-17 13:08:07\n\n\n\n\n17.16.2 Đổi tên file\n\nfile.rename(from = \"chien-tranh-thuong-mai.xlsx\", to = \"chien-tranh.xlsx\")\n\n\n\n17.16.3 Check extension và tên\n\n# Extension\ndir() %>% file_ext\n# File name\ndir() %>% file_path_sans_ext"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#load-tất-cả-dataframe-trong-1-thư-mục",
    "href": "p02-99-meo-trong-r.html#load-tất-cả-dataframe-trong-1-thư-mục",
    "title": "17  Các mẹo trong R",
    "section": "17.17 Load tất cả dataframe trong 1 thư mục",
    "text": "17.17 Load tất cả dataframe trong 1 thư mục\n\ngetwd()\nlist.files(\"F:/OneDrive - VPBank/1. Personal projects/Trick in R\")\nsapply(list.files(pattern = \"*.rda\"), load, .GlobalEnv)"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#gán-một-dataframe-vào-1-biến",
    "href": "p02-99-meo-trong-r.html#gán-một-dataframe-vào-1-biến",
    "title": "17  Các mẹo trong R",
    "section": "17.18 Gán một dataframe vào 1 biến",
    "text": "17.18 Gán một dataframe vào 1 biến\n\nresult_name <- \"df2\"\nassign(result_name, mtcars, envir = .GlobalEnv)\nhead(df2)\n\nLưu ý: Options envir = .GlobalEnv rất hữu ích khi gán dataframe vào biến và load trong global environment trong hàm"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#gọi-một-dataframe-từ-biến",
    "href": "p02-99-meo-trong-r.html#gọi-một-dataframe-từ-biến",
    "title": "17  Các mẹo trong R",
    "section": "17.19 Gọi một dataframe từ biến",
    "text": "17.19 Gọi một dataframe từ biến\n\nx <- \"mtcars\"\nget(x)\n#Lưu dạng dataframe\nsave(list = x, file = paste0(x,\".rda\"))"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#rbind-tất-cả-các-dataframe",
    "href": "p02-99-meo-trong-r.html#rbind-tất-cả-các-dataframe",
    "title": "17  Các mẹo trong R",
    "section": "17.20 Rbind tất cả các dataframe",
    "text": "17.20 Rbind tất cả các dataframe\n\n#Nối thành 1 sách\nrm(list = ls())\ndf1 <- df2 <- df3 <- iris\ndfs <-  sapply(.GlobalEnv, is.data.frame) \ndata <- do.call(rbind, mget(names(dfs)[dfs]))"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#remove-tất-cả-trừ-1-file",
    "href": "p02-99-meo-trong-r.html#remove-tất-cả-trừ-1-file",
    "title": "17  Các mẹo trong R",
    "section": "17.21 Remove tất cả trừ 1 file",
    "text": "17.21 Remove tất cả trừ 1 file\n\nrm(list = setdiff(ls(), \"data\"))"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#xem-location-của-file-script-đang-mở",
    "href": "p02-99-meo-trong-r.html#xem-location-của-file-script-đang-mở",
    "title": "17  Các mẹo trong R",
    "section": "17.22 Xem location của file script đang mở",
    "text": "17.22 Xem location của file script đang mở\n\nVấn đề: Khi thực hiện tự động hóa trong R, ta cần đặt directory tự động (không mở bằng project)\nGiải pháp: Sử dụng rstudioapi\n\n\ndirname(rstudioapi::getSourceEditorContext()$path)"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#chạy-một-đoạn-r-từ-bat",
    "href": "p02-99-meo-trong-r.html#chạy-một-đoạn-r-từ-bat",
    "title": "17  Các mẹo trong R",
    "section": "17.23 Chạy một đoạn R từ bat",
    "text": "17.23 Chạy một đoạn R từ bat\nTương tự như Python, R cho phép chạy thẳng code từ bat. Các bước thự hiện như sau:\n\nAdd RScript vào Global Path:\n\nView Advanced System Setting >> Advanced >> Environment Variables >> System variables >> Path\nNew >> Chọn đường dẫn vào R >> C:\\Program Files\\R\\R-3.4.0\\bin\n\nKiểm tra setup trong bat: RScript --version\nThực hiện câu lệnh:\n\n\nRScript -e \"bookdown::preview_chapter('chapter-01.Rmd')\""
  },
  {
    "objectID": "p02-99-meo-trong-r.html#khởi-động-rstudio-từ-terminal",
    "href": "p02-99-meo-trong-r.html#khởi-động-rstudio-từ-terminal",
    "title": "17  Các mẹo trong R",
    "section": "17.24 Khởi động RStudio từ terminal",
    "text": "17.24 Khởi động RStudio từ terminal\nTương tự như Python, ta có thể khởi động RStudio tại một thư mục bất kỳ. Thư mục đó sẽ trở thành thư mục làm việc trong R. Cách làm như sau\n\nopen Rstudio"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#tạo-nút-download-dữ-liệu-từ-html-markdown",
    "href": "p02-99-meo-trong-r.html#tạo-nút-download-dữ-liệu-từ-html-markdown",
    "title": "17  Các mẹo trong R",
    "section": "17.25 Tạo nút download dữ liệu từ html markdown",
    "text": "17.25 Tạo nút download dữ liệu từ html markdown\n\nlibrary(tidyverse)\nlibrary(DT)\n\niris %>%\n  datatable(extensions = 'Buttons',\n            options = list(dom = 'Blfrtip',\n                           buttons = c('copy', 'csv', 'excel', 'pdf', 'print'),\n                           lengthMenu = list(c(10,25,50,-1),\n                                             c(10,25,50,\"All\"))))"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#encript-thông-tin",
    "href": "p02-99-meo-trong-r.html#encript-thông-tin",
    "title": "17  Các mẹo trong R",
    "section": "17.26 Encript thông tin",
    "text": "17.26 Encript thông tin\nKhi cần gửi dữ liệu ra ngoài, ta cần encript thông tin nhạy cảm như số điện thoại, thu nhập.\n\nlibrary(safer)\ntemp <- encrypt_string(\"Hello man\", key = \"da\")\ntemp\ndecrypt_string(temp, key = \"da\")"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#tạo-job-run-tự-động-không-dùng-bat",
    "href": "p02-99-meo-trong-r.html#tạo-job-run-tự-động-không-dùng-bat",
    "title": "17  Các mẹo trong R",
    "section": "17.27 Tạo job run tự động không dùng bat",
    "text": "17.27 Tạo job run tự động không dùng bat\n\ni <-  0\nf <-  function() {\n  if (i %% 1 == 0) {\n    print(i)\n    save(mtcars, file = paste0(\"stream/mtcars_\", i, \".csv\"))\n  }\n  i <<- i + 1\n  later::later(f, 0.1)\n}\n\nf()"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#save-file-định-dạng-ký-tự-utf-8",
    "href": "p02-99-meo-trong-r.html#save-file-định-dạng-ký-tự-utf-8",
    "title": "17  Các mẹo trong R",
    "section": "17.28 Save file định dạng ký tự UTF-8",
    "text": "17.28 Save file định dạng ký tự UTF-8\n\nVấn đề: Khi lưu text có chứa ký tự UTF-8 trong windows sẽ bị lỗi nếu chỉ dùng cat\nGiải pháp: Chuyển đổi dữ liệu sang các Bin sau đó mới save sang file text\n\n\nx3 <- \"Dữ liệu cần được lưu \\n định dạng tiếng Việt\"\n\nwrite_utf8 <- function(x, file, bom=F) {\n  con <- file(file, \"wb\")\n  if(bom) writeBin(BOM, con, endian=\"little\")\n  writeBin(charToRaw(x), con, endian=\"little\")\n  close(con)\n}\n\nwrite_utf8(x3, \"da_test.Rmd\")"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#ghi-dữ-liệu-chứa-ký-tự-utf-8-vào-sql",
    "href": "p02-99-meo-trong-r.html#ghi-dữ-liệu-chứa-ký-tự-utf-8-vào-sql",
    "title": "17  Các mẹo trong R",
    "section": "17.29 Ghi dữ liệu chứa ký tự UTF-8 vào SQL",
    "text": "17.29 Ghi dữ liệu chứa ký tự UTF-8 vào SQL\n\n#Thiết lập function đổi mã\n\n##Function đổi mã UTF 8 sang UTF 16\n\nconvertToUTF16 <- function(s){\n  lapply(s, function(x) unlist(iconv(x,from=\"UTF-8\",to=\"UTF-16LE\",toRaw=TRUE)))\n} \n\n###Đổi mã các cột x,y có chữ tiếng Việt\ndata <- data %>% mutate(x = x %>%\n                          convertToUTF16,\n                        y = y %>%\n                          convertToUTF16())\n\n##Upload data lên trên SQL\ndbWriteTable(\n  con,\n  \"Ten_bang\",\n  #Đặt tên bảng trên sql\n  data,\n  #Tên table chứa data muốn upload\n  append = TRUE,\n  field.types = c(x = \"NVARCHAR(MAX)\",\n                  y = \"NVARCHAR(MAX)\")\n)   #x,y là tên cột chứa tiếng Việt"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#cách-convert-dữ-liệu-từ-tiếng-việt-sang-không-dâu",
    "href": "p02-99-meo-trong-r.html#cách-convert-dữ-liệu-từ-tiếng-việt-sang-không-dâu",
    "title": "17  Các mẹo trong R",
    "section": "17.30 Cách convert dữ liệu từ tiếng Việt sang không dâu",
    "text": "17.30 Cách convert dữ liệu từ tiếng Việt sang không dâu\n\nlibrary(stringi)\nstri_trans_general(\"Cộng hòa xã hội chủ nghĩa Việt Nam\", \"Latin-ASCII\")"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#refresh-app-shiny-tự-động",
    "href": "p02-99-meo-trong-r.html#refresh-app-shiny-tự-động",
    "title": "17  Các mẹo trong R",
    "section": "17.31 Refresh app shiny tự động",
    "text": "17.31 Refresh app shiny tự động\n\nVấn đề: Khi connect app Shiny với database, khi database thay đổi, Shiny sẽ không tự động lấy được dữ liệu mới nhất\nGiải pháp: Tạo nút click và chèn option session$reload() trong server\nLưu ý: Các source script cần được đưa vào trong function server\n\n\nsource(\"source.R\")\nsource(\"01-getsql.R\")\nsource(\"01-setup.R\")\n\nobserveEvent(input$refresh, {\n  session$reload()\n  return()\n})"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#tạo-cross-join-trong-r",
    "href": "p02-99-meo-trong-r.html#tạo-cross-join-trong-r",
    "title": "17  Các mẹo trong R",
    "section": "17.32 Tạo cross join trong R",
    "text": "17.32 Tạo cross join trong R\n\nVấn đề: Tạo cartesian join giữa 2 dataframe. VD: Mỗi khách hàng cần được map để tính khoảng cách đến các chi nhánh gần nhát\nGiải pháp: Tạo cartesian join với data.table\n\n\nlibrary(dplyr)\nlibrary(data.table)\n\ncartesian_join <- function(i, j){\n  # Cartesian join of two data.tables\n  # If i has M rows and j has N rows, the result will have M*N rows\n  # Example: cartesian_join(as.data.table(iris), as.data.table(mtcars))\n\n  # Check inputs\n  i <- i %>% as.data.table\n  j <- j %>% as.data.table\n  if(!is.data.table(i)) stop(\"'i' must be a data.table\")\n  if(!is.data.table(j)) stop(\"'j' must be a data.table\")\n  if(nrow(i) == 0) stop(\"'i' has 0 rows. Not sure how to handle cartesian join\")\n  if(nrow(j) == 0) stop(\"'j' has 0 rows. Not sure how to handle cartesian join\")\n\n  # Do the join (use a join column name that's unlikely to clash with a pre-existing column name)\n  i[, MrJoinyJoin := 1L]\n  j[, MrJoinyJoin := 1L]\n  result <- j[i, on = \"MrJoinyJoin\", allow.cartesian = TRUE]\n  result[, MrJoinyJoin := NULL]\n  i[, MrJoinyJoin := NULL]\n  j[, MrJoinyJoin := NULL]\n\n  return(result[])\n}\ncartesian_join(mtcars, iris %>% sample_n(10))"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#chỉnh-sửa-dữ-liệu-trong-datatable",
    "href": "p02-99-meo-trong-r.html#chỉnh-sửa-dữ-liệu-trong-datatable",
    "title": "17  Các mẹo trong R",
    "section": "17.33 Chỉnh sửa dữ liệu trong datatable",
    "text": "17.33 Chỉnh sửa dữ liệu trong datatable\nVấn đề: Cho phép chỉnh sửa dữ liệu với cột nhất định trong DT::datatable\n\niris %>% \n  head(10) %>% \n  datatable(editable = list(target = \"column\", disable = list(columns = 1:4)))"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#đọc-cùng-lúc-nhiều-sheet",
    "href": "p02-99-meo-trong-r.html#đọc-cùng-lúc-nhiều-sheet",
    "title": "17  Các mẹo trong R",
    "section": "17.34 Đọc cùng lúc nhiều sheet",
    "text": "17.34 Đọc cùng lúc nhiều sheet\n\npath %>% \n  excel_sheets() %>% \n  set_names() %>% \n  map_df(~ read_excel(path = path, sheet = .x, col_types = \"text\"), .id = \"Sheet\")"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#tạo-tổ-hợp-các-nhóm",
    "href": "p02-99-meo-trong-r.html#tạo-tổ-hợp-các-nhóm",
    "title": "17  Các mẹo trong R",
    "section": "17.35 Tạo tổ hợp các nhóm",
    "text": "17.35 Tạo tổ hợp các nhóm\nVấn đề: Tạo tổ hợp cặp giữa 2 nhóm\nPhương án: sử dụng exapnd.grid - cho phép lặp, hoặc combn - loại bỏ các trường hợp lặp\n\nx <- c(\"a\", \"b\", \"c\")\ny <- c(\"A\", \"B\")\nexpand.grid(x, y)\n\n  Var1 Var2\n1    a    A\n2    b    A\n3    c    A\n4    a    B\n5    b    B\n6    c    B\n\n# Tạo combination không lặp\nas.data.frame(t(combn(x, 2)))\n\n  V1 V2\n1  a  b\n2  a  c\n3  b  c"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#tìm-các-element-trùng-nhau-trong-2-vector",
    "href": "p02-99-meo-trong-r.html#tìm-các-element-trùng-nhau-trong-2-vector",
    "title": "17  Các mẹo trong R",
    "section": "17.36 Tìm các element trùng nhau trong 2 vector",
    "text": "17.36 Tìm các element trùng nhau trong 2 vector\nPhương án: Sử dụng hàm intersect\n\nx <- c(\"a\", \"b\", \"c\")\ny <- c(\"a\", \"c\", \"e\")\nintersect(x, y)\n\n[1] \"a\" \"c\""
  },
  {
    "objectID": "p02-99-meo-trong-r.html#đính-kèm-1-file-trong-rmarkdown",
    "href": "p02-99-meo-trong-r.html#đính-kèm-1-file-trong-rmarkdown",
    "title": "17  Các mẹo trong R",
    "section": "17.37 Đính kèm 1 file trong rmarkdown",
    "text": "17.37 Đính kèm 1 file trong rmarkdown\n\nVấn đề: Khi xây dựng báo cáo HTML trong Rmarkdown, cần đính kèm file dữ liệu (.xlsx, .csv)\nGiải pháp: Sử dụng xfun:embeded_file\n\n\n# a single file\nxfun::embed_file('source.Rmd')\n\n# multiple files\nxfun::embed_files(c('source.Rmd', 'data.csv'))\n\n# a directory\nxfun::embed_dir('data/', text = 'Download full data')"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#xác-định-1-url-tồn-tại",
    "href": "p02-99-meo-trong-r.html#xác-định-1-url-tồn-tại",
    "title": "17  Các mẹo trong R",
    "section": "17.38 Xác định 1 url tồn tại",
    "text": "17.38 Xác định 1 url tồn tại\n\nurl_exists <- function(url){\n  status <- NA\n  tryCatch({\n    hd <- httr::HEAD(url)\n    status <- hd$all_headers[[1]]$status\n  }, \n  error = function(e){})\n  status <- ifelse(!is.na(status), 1, 0)\n  return(status)\n}\n\nx <- c(\"https://www.r-bloggers.com/\",\n       \"https://google.com\")\n\nlibrary(purrr)\nmap_dbl(x, url_exists)"
  },
  {
    "objectID": "p02-99-meo-trong-r.html#tự-động-render-ra-kết-quả-phân-tích",
    "href": "p02-99-meo-trong-r.html#tự-động-render-ra-kết-quả-phân-tích",
    "title": "17  Các mẹo trong R",
    "section": "17.39 Tự động render ra kết quả phân tích",
    "text": "17.39 Tự động render ra kết quả phân tích\nLưu ý:\n\nCần có hai dấu cách (space) trước \\n\nĐặt chế độ result = 'asis' để biến text thành kết quả\n\n```{r result = 'asis'}\nfor(i in unique(Month)) {\n  cat(\"  \\n###\",  month.name[i], \"Air Quaility  \\n\")\n  #print(plot(airquality[airquality$Month == i,]))\n  plot(airquality[airquality$Month == i,])\n  cat(\"  \\n\")\n}\n```"
  },
  {
    "objectID": "p03-01-nguyen-ly-du-bao.html#các-nhánh-trong-học-máy-machine-learning",
    "href": "p03-01-nguyen-ly-du-bao.html#các-nhánh-trong-học-máy-machine-learning",
    "title": "18  Giới thiệu về học máy",
    "section": "18.1 Các nhánh trong học máy (machine learning)",
    "text": "18.1 Các nhánh trong học máy (machine learning)\nStatistical Learning (SL) hay Machine Learning là nghành học sử dụng nhiều phương pháp và công cụ toán học khác nhau để tìm hiểu về cấu trúc của dữ liệu. ML có thể được chia thành 2 dạng: Định hướng & không định hướng:\n\nPhân tích có định hướng trước (Supervised learning): Xây dựng các mô hình giữa biến phụ thuộc với một hoặc nhiều biến độc lập. Biến cần dự báo được gọi là biến phụ thuộc (target), các biến đầu vào được gọi là biến độc lập (input, feature). Các đối tượng (instances) được sử dụng trong việc xây dựng mô hình còn được gọi là quan sát (observation). Ví dụ:\n\nDự báo khách hàng vỡ nợ dựa vào các đặc trưng về nhân khẩu học và hành vi giao dịch của khách hàng.\nDự báo giá bán nhà phụ thuộc vào các tham số của ngôi nhà.\n\n\nCác thuật toán như cây quyết định, logistics, mô hình hồi quy tuyến tính đều thuộc loại này. Tuy nhiên, tùy thuộc vào biến cần dự báo, ta lại có hai nhóm nhỏ sau:\n\nBài toán phân loại (classification): Khi biến phụ thuộc là các biến định dạng nhóm (category). Kết quả đầu ra của các mô hình dạng này là điểm xác suất xảy ra một sự kiện. Ví dụ, khách hàng tốt hay xấu, khách hàng mua hay không mua sản phẩm,… Các nhóm cần được dự báo trong mô hình gọi là positive class\nBài toán dự báo (regression): Khi biến phụ thuộc biến định dạng số liên tục. Ví dụ, giá trị của tổng các giao dịch một khách hàng có trong 1 tháng,…\n\n\nPhân tích không định hướng trước(Unsupervised learning): Biến phụ thuộc chưa biết trước và mục tiêu của mô hình là tìm ra các mối qua hệ ẩn giữa các nhóm. . Ví dụ, phân nhóm khách hàng thành 5 nhóm các hành vi tương tự nhau. Các thuật toán như apriori, k-means, PCA, FA thuộc nhóm này. Các bài toán loại này có thể chia làm 2 nhóm lớn như sau:\n\nBài toán phân nhóm (clustering): Bài toán loại này giúp ta chia tập dữ liệu sẵn có thành nhiều nhóm khác nhau để sao cho đặc trưng của mỗi nhóm là gần nhau nhất\nBài toán tìm thành phần chính (PCA): Bài toán này giúp ta giảm số lượng các biến có sẵn trong dữ liệu gốc nhưng vẫn đảm bảo thể hiện được cấu trúc của toàn bộ dữ liệu"
  },
  {
    "objectID": "p03-01-nguyen-ly-du-bao.html#cách-xây-dựng-mô-hình",
    "href": "p03-01-nguyen-ly-du-bao.html#cách-xây-dựng-mô-hình",
    "title": "18  Giới thiệu về học máy",
    "section": "18.2 Cách xây dựng mô hình",
    "text": "18.2 Cách xây dựng mô hình\nMô hình là cách thức thể hiện mối quan hệ giữa các biến thông quan các công cụ toán học. Do mô hình là cách thức đơn giản hóa mối quan hệ giữa các biến trên thực tế, do đó, mô hình còn được nhiều nhà phân tích gọi là giả thuyết. Khi xây dựng một mô hình dự báo, bản chất là ta dựa vào tập dữ liệu cho trước, áp dụng một thuật toán để đưa ra một mô hình (giả thuyết) về mối quan hệ giữa biến đầu vào và đầu ra.\nKhi xây dựng mô hình, ta thường có ba tập dữ liệu\n\nTrain: Tập dữ liệu được sử dụng khi xây dựng (huấn luyện) mô hình\nTest: Tập dữ liệu dùng để đánh giá chất lượng mô hình được xây trên tập train, cập nhật lại các hyper-parameters mô hình để đưa ra mô hình cuối cùng.\nValidation: Tập dữ liệu độc lập dùng để đánh giá chất lượng mô hình cuối cùng\n\n\n18.2.1 Sampling\nTrong thực tế, ta có thể bỏ qua tập dữ liệu validation mà chỉ có hai tập dữ liệu train và test. Tỷ lệ train và test trong thực tế thường được chia thành 70-30, 80-20 hoặc 60-40. Khi chia tập dữ liệu phục vụ xây dựng mô hình, có hai cách chia chính.\n\nPhân chia quan sát ngẫu nhiên thông thường (simple random sampling)\nPhân chia quan sát sao cho phân phối của biến cần dự báo trên hai tập train và test giống nhau (stratified sampling)\n\nVí dụ về simple sampling\n\nlibrary(dplyr)\n# Cách 1: Sử dụng index thông thường\nsize <- nrow(iris)\ntrain_index <- sample(1:size, size = 0.7*size, replace = F)\ntrain <- iris[train_index,]\ntest <- iris[-train_index, ]\ntrain$Species %>% table %>% prop.table()\n\n.\n    setosa versicolor  virginica \n 0.3047619  0.3238095  0.3714286 \n\ntest$Species %>% table %>% prop.table()\n\n.\n    setosa versicolor  virginica \n 0.4000000  0.3555556  0.2444444 \n\n# Cách 2: Sử dụng rsample\nlibrary(rsample)\nsplit <- initial_split(iris, prop = 0.7)\ntrain <- training(split)\ntest <- testing(split)\ntrain$Species %>% table %>% prop.table()\n\n.\n    setosa versicolor  virginica \n 0.3238095  0.3523810  0.3238095 \n\ntest$Species %>% table %>% prop.table()\n\n.\n    setosa versicolor  virginica \n 0.3555556  0.2888889  0.3555556 \n\n\nVí dụ về strata sampling\n\nlibrary(rsample)\nsplit <- initial_split(iris, prop = 0.7, strata = \"Species\")\ntrain <- training(split)\ntest <- testing(split)\ntrain$Species %>% table %>% prop.table()\n\n.\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\ntest$Species %>% table %>% prop.table()\n\n.\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\n\n\n§\n\nImbalance class: Đây là hiện tượng khi xây dựng mô hình, có 1 nhóm (class) chiếm tỷ lệ đa số trong tất cả quan sát (99%-1%, 95%-5%). Ví dụ: Fraud vs. Non Fraud. Do đó, khi gặp phải trường hợp bất cân bằng giữa các nhóm, ta cần phải thực hiện kỹ thuật lấy mẫu để giảm tỷ lệ bất cân xứng giữa các nhóm. Có hai phương pháp phổ biến nhất như sau.\n\nDown sampling: Loại bỏ các quan sát thuộc nhóm chiếm đa số\nUp sampling: Lặp lại các quan sát của nhóm thiểu số\n\n\n§\n\nMô hình và việc xây dựng mô hình\nMô hình (hay giả thuyết h) giữa biến đầu vào và đầu ra được thể hiện như sau.\n\\[h_{\\theta} = \\theta_0 + \\theta_1 \\times X\\]\n\n\\(h_{\\theta}\\) được gọi là giả thuyết hay mô hình.\n\\(\\theta\\) là tham số của mô hình.\n\nVới các bài toán (supervised vs. nonsupervised) khác nhau, phương trình trên sẽ được thay đổi để phù hợp với bài toán thực tế.\nKhi xây dựng mô hình, tất cả các thuật toán của ML đều trải qua ba bước cơ bản theo sơ đồ sau.\n\n \n\nBước 1: Xây dựng mô hình với tham số bất kỳ. Với mỗi mô hình, sẽ có các tham số khác nhau. Ví dụ, mô hình OLS, tham số là hệ số \\(\\beta\\) trong mô hình \\(y = \\beta*X\\).\nBước 2: Đo lường sai số mô hình so với thực tế.\nBước 3: Update lại tham số của mô hình để giảm thiểu sai số giữa mô hình và thực tế.\n\nThuật toán sẽ tiếp tục diễn ra cho đến khi sai số của mô hình nhỏ hơn 1 mức sai số định trước.\nVới mô hình phân tích có định hướng (tồn tại biến Y cần dự báo), phương trình của mô hình dự báo có thể biểu diễn dưới dạng.\n\\[Y = f(X)\\]\nTrong đó:\n\nY được gọi là biến phụ thuộc (dependent variables)\nX được gọi là biến độc lập (independent variables) hay biến dự báo (predictors)\n\nLưu ý:\n\nBiến đầu vào X còn được gọi là features, predictor variable, independent variable, attributes, predictor, input\nBiến dự báo Y còn được gọi là target variable, dependent variable, response, response variable, output\nMô hình còn được gọi là model, learners\n\n\n§\n\nHyperparameters: Mỗi mô hình có các tham số cho phép thay đổi độ phức tạp của mô hình. Hyperparameters là tổ hợp các tham số để xây dựng thuật toán. Tuning hyperparameters là cách thức tinh chỉnh độ phức tạp của các tham số trong mô hình. Một thuật toán được gọi là có hyperparameter nếu mỗi lần thay đổi, ta sẽ có các mô hình khác nhau.\n\nVí dụ: Mô hình cây quyết định - hyperparameter là tổ hợp các tham số được cho trước khi xây dựng mô hình gồm số lá (leaf), số lần chia tối đa. Nếu điều chỉnh các chỉ số này, mỗi lần train mô hình sẽ ra các kết quả khác nhau.\n\nTuy nhiên, không phải thuật toán nào cũng có hyperparameters (ví dụ, OLS - mô hình hồi quy không có hyperparameters) nhưng phần lớn các thuật toán đều có tối thiểu 1 tham số. Để tìm kiếm tổ hợp tham số tốt nhất cho 1 mô hình, thường có 2 cách:\n\nGrid search: Tổ hợp tất cả các trường hợp có thể có của tham số\nRandom search: Tổ hợp ngẫu nhiên các trường hợp có thể có\n\nVới 2 cách trên, khối lượng công việc cần tính toán cũng rất nhiều. Do đó, ta có thể sử dụng early stopping để dừng việc tuning khi sai số mô hình đạt đến mức độ sai số cho phép (do data scientist xác định trước).\nPhân biệt parameter và hyper parameter:\n\nTham số (parameter) được máy học thông qua quá trình huấn luyện 1 mô hình cụ thể\nSiêu tham số (hyper-parameter) được máy học thông qua huấn luyện nhiều mô hình khác nhau để tìm 1 bộ siêu tham số tối ưu.\n\n\n\n18.2.2 Xây dựng mô hình\nSau khi xây dựng, bước tiếp theo là sử dụng mô hình trong việc dự báo thực tế.\nQuy trình xây dựng phân tích dữ liệu thực tế"
  },
  {
    "objectID": "p03-01-nguyen-ly-du-bao.html#các-nguyên-lý-trong-dự-báo",
    "href": "p03-01-nguyen-ly-du-bao.html#các-nguyên-lý-trong-dự-báo",
    "title": "18  Giới thiệu về học máy",
    "section": "18.3 Các nguyên lý trong dự báo",
    "text": "18.3 Các nguyên lý trong dự báo\n\n18.3.1 Reducible vs. irreducible error\nTrong thực tế, mối quan hệ giữa X & Y được biểu diễn qua hàm sau: \\[Y = f(X) + \\epsilon \\] Khi phân tích dữ liệu, ta tìm hàm \\(\\hat(Y)=\\hat{f}(X)\\) gần nhất với \\(f(X)\\). Sai số giữa thực tế và mô hình sẽ là:\n\\[E(Y-\\hat{Y})^2 = E[f(X) - \\hat{f}(X) - \\epsilon]^2\n\\\\=E[(f(x) - \\hat{f}(X))^2 - 2*\\epsilon*(f(x) - \\hat{f}(X)) + \\epsilon^2)]\n\\\\=E[(f(x) - \\hat{f}(X))^2] - \\underbrace{2*E(\\epsilon)}_{= 0}*E(f(x) - \\hat{f}(X))) + E(\\epsilon^2)\n\\\\=E[(f(x) - \\hat{f}(X))^2] + [E(\\epsilon^2) - \\underbrace{E(\\epsilon)^2}_{=0}]\n\\\\= E\\underbrace{[f(X) - \\hat{f}(X)]^2}_{reducible} +\n\\underbrace{Var(\\epsilon)}_{irreducible}\\]\nLưu ý: \\(Var(\\epsilon) = E(\\epsilon^2) - E(\\epsilon)^2\\)\nKhi xây dựng mô hình, ta chỉ có thể giảm bớt phần reducible error mà không thể giảm được phần variance của sai số. Do đó, mô hình sẽ không thể đạt được độ chính xác 100% mà luôn tồn tại một mức sai số nhất định.\n\n\n18.3.2 Khả năng giải thích và khả năng dự báo\nKhi xây dựng mô hình, có hai khía cạnh cần phải xử lý:\n\nKhả năng giải thích\nKhả năng dự báo\n\n\n§\n\nKhả năng giải thích hay khả năng rút ra kết luận từ mô hình (inference): Nhấn mạnh đến khả năng diễn đạt ý nghĩa các biến trong mô hình. Các câu hỏi thường dùng là:\n\nBiến độc lập (predictors) nào có quan hệ chặt chẽ với biến cần dự báo (dependent variables)?\nMối quan hệ giữa biến độc lập và biến dự báo là gì?\nMối quan hệ này có thể biểu diễn một cách đơn giản dạng mô hình tuyến tính hay phải mô tả dưới dạng phức tạp hơn?\n\nVí dụ về khả năng giải thích của mô hình:\n\nKhách hàng trả nợ trễ hạn 3 lần sẽ làm tăng khả năng trốn nợ lên 20%\nGiá giảm 10% sẽ khiên doanh thu tăng thêm khoảng 6%.\n\nCác thuật toán như OLS, apriori, Logistics thuộc nhóm này.\n\n§\n\nKhả năng dự báo của mô hình: Ưu tiên hơn đến tính chính xác của mô hình dự báo, không quan tâm đến việc mô tả quan hệ giữa các biến. Ví dụ: Random Forest, Neuron Network, KNN\n\n§\n\nNguyên lý: Khi xây dựng mô hình, ta buộc phải dánh đổi giữa độ chính xác của mô hình vs. khả năng diễn giải mô hình và không tồn tại một mô hình tốt nhất cho mọi trường hợp. Do đó, ta cần phải lựa chọn mô hình theo từng đối tượng. Mô hình có độ chính xác cao thường khó mô tả mối quan hệ giữa các biến (VD: decision tree) và ngược lại (VD: OLS)\n\n\n\n18.3.3 Nguyên lý chữ U\nKhi xây dựng mô hình, để đánh giá chất lượng, ta sẽ đo lường sai số trên tập train và tập test. Sai số của mô hình được tính như sau (với trường hợp regression)\n\\[MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat{f}(x_i))^2\\]\nKhi đánh giá chất lượng mô hình, ta cần đánh giá trên cả tập train và tập test. Thông thường, mô hình sẽ được xây trên tập train và được đánh giá trên tập test. Các tham số trên tập train sẽ được thay đổi để sai số mô hình được tối ưu. Số lượng tham số cần tối ưu càng nhiều, mô hình càng phức tạp.\n\nVí dụ: Ta cần dự báo \\(income\\), ta có hai mô hình sau.\n\n\n\n\n\n\n\nSTT\n                Phương trình\n\n\n\n\n1\n\\(income = \\beta_0 + \\beta_1*age\\)\n\n\n2\n\\(income = \\beta_0 + \\beta_1*age + \\beta_2*experience\\)\n\n\n\nTrong hai mô hình trên, mô hình 2 được gọi là phức tạp hơn mô hình 1 (compexity level).\nKhi xây dựng mô hình, nguyên lý chữ U cho ta biết về sự thay đổi sai số trên hai tập train và test theo độ phức tạp của mô hình như sau.\nKhi mô hình có độ phức tạp càng cao thì sai số của tập train sẽ ngày càng giảm trong khi sai số của tập test sẽ có dạng chữ U\n\nLưu ý:\n\nNguyên lý chữ U cho ta thấy, việc tăng thêm biến vào mô hình (# variables) không phải lúc nào cũng làm tăng chất lượng mô hình\nKhi xây dựng mô hình, cần phải tìm được điểm cân bằng giữa độ phức tạp mô hình và độ chính xác. Điểm này chính là điểm thấp nhất trên đường chữ U của sai số trên tập test\n\n\n\n18.3.4 Bias Variance Trade-Off\n\nBias là sai số giữa quan sát thực tế và kết quả dự báo kỳ vọng (kết quả dự báo trung bình) của mô hình.\nVariance: là độ biến động của kết quả dự báo từ mô hình đã xây dựng với các tập dữ liệu khác nhau.\n\nNhững mô hình đơn giản là mô hình có bias cao nhưng variance thấp. Trong khi đó, các mô hình có độ phức tạp cao hơn lại có bias thấp và variance cao. Bias đại diện cho độ chính xác của mô hình trong khi variance đại diện cho độ khái quát hóa của mô hình. Mô hình có độ khái quát cao (generalization) sẽ có bias cao, variance thấp và ngược lại\nVí dụ:\n\nModel 1 sử dụng median là mô hình dự báo có bias cao (sai số giữa thực tế và kỳ vọng) và variance thấp (tất cả các quan sát đều có chung 1 giá trị dự báo)\nMô hình 2 sử dụng sin(x) là mô hình dự báo có bias thấp (mô hình sát với thực tế) nhưng variance cao (mỗi quan sát sẽ có 1 giá trị dự báo khác nhau)\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(reshape2)\ndf <- data.frame(x = seq(0, 10, by = 0.01)) %>% \n  mutate(y = sin(x) + rnorm(nrow(.), 0, 0.3)) %>% \n  mutate(model1 = 0) %>% \n  mutate(model2 = sin(x)) %>% \n  melt(id.vars = c(\"x\", \"y\"))\ndf %>% \n  ggplot(aes(x, y)) + \n  geom_point(col = \"gray\", alpha = 0.7) +\n  geom_line(aes(x, value, col = variable), size = 2) +\n  theme_minimal() +\n  theme(legend.position = \"top\") +\n  labs(title = \"Bias vs. Variance\")\n\n\n\n\nNgười ta chứng minh được rằng:\n\\[Err(x) = E[(Y-\\hat{f}(x))^2]\\]\n\\[Err(x\\]\n\\[E(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) +\n[Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\\]\nTrong đó:\n\n\\(E(y_0 - \\hat{f}(x_0))^2\\): Kỳ vọng của MSE\n\\(Var(\\hat{f}(x_0))\\): Phương sai của MSE trong tập test khi ta thay đổi tập training\n\\([Bias(\\hat{f}(x_0))]^2\\): Sai số của mô hình ước lượng \\(\\hat{f}(x)\\) so với mô hình thực tế \\(f(x)\\). VD: Mô hình thực tế là \\(y=x^2\\), mô hình ước lượng là \\(y=0.9*x^2\\)\n\\(Var(\\epsilon)\\): Phương sai của nhiễu\n\nNguyên lý:\n\nVariance của mô hình tăng thì Bias sẽ giảm và ngược lại.\n\n\n§\n\nPhương pháp tính toán sai số trong bài toán phân loại - The Classification Setting\n\\[training\\_error\\_rate = \\frac{1}{n}\\sum_{i=1}^n I(y_i \\neq \\hat {y_i})\\]\nTrong đó \\(I(y_i \\neq \\hat{y_i}\\) là “indicator variable”, có giá trị bằng 1 nếu \\(y_i \\neq \\hat{y_i}\\), có giá trị bằng 0 nếu \\(y_i= \\hat{y_i}\\)"
  },
  {
    "objectID": "p03-01-nguyen-ly-du-bao.html#các-lưu-ý-khác",
    "href": "p03-01-nguyen-ly-du-bao.html#các-lưu-ý-khác",
    "title": "18  Giới thiệu về học máy",
    "section": "18.4 Các lưu ý khác",
    "text": "18.4 Các lưu ý khác\n\n18.4.1 Mô hình có tham số cho trước hoặc không có tham số cho trước (Parametric vs. Nonparametric)\nĐây là hai loại hai trường phái được sử dụng khi xây dựng các mô hình thống kê.\n\nParametric: Đưa ra mô hình biểu diễn mối quan hệ trước rồi sau đó ước lượng mô hình đưa ra. VD: OLS, Logistic Regression\nNonparametric: Không đưa ra mô hình, chỉ đưa ra phương pháp và để thuật toán tự động tìm kết quả. VD: Association rule, decision tree…\n\n\n\n18.4.2 Overfitting & regularization\nKhi xây dựng mô hình dự báo, có thể xảy ra 3 trường hợp:\n\nUnderfiting: Hiện tượng mô hình quá đơn giản, tính khái quát hóa cao nhưng dự báo không tốt, dẫn đến sai số cả ở tập train & test đều cao. Mô hình underfitting thường đi kèm với bias cao\nOverfitting: Hiện tượng xảy ra khi mô hình hoạt động tốt trên tập train nhưng dự báo rất kém trên tập test. Mô hình loại này sẽ khiến sai số trên các tập dữ liệu khác nhau thường rất khác nhau. Do đó, overfitting còn đi kèm với variance lớn\nMô hình được xây dựng tốt: Là loại mô hình hoạt động tốt trên cả tập train & test.\n\n3 trường hợp xây dựng mô hình được thể hiện ở hình dưới đây.\n\nlibrary(tidyverse)\ndf <- data.frame(\n  x = seq(0, 13, by = 0.5)\n) %>% \n  mutate(good_model = 10 + 2*x - 1/10*x^2) %>% \n  mutate(y = good_model + rnorm(nrow(.), 0, 1)) %>% \n  mutate(underfit = 10 + 1.2*x) %>% \n  mutate(overfit = y + rnorm(nrow(.), 0, 0.01))\n\nlibrary(reshape2)\ndf2 <- df %>% \n  melt(id.vars = c(\"x\", \"y\"))\ndf2 %>% ggplot() +\n  geom_line(aes(x, value, col = variable)) +\n  geom_point(aes(x, y), col = \"black\", alpha = 0.8) +\n  theme_minimal() +\n  theme(legend.position = \"top\") +\n  scale_color_discrete(name = \"Model\") +\n  labs(title = \"Different type of Model\",\n       y = \"y\")\n\n\n\n\nĐể giải quyết vấn đề overfiting, ta có thể làm như sau:\n\nGiảm biến bằng cách chọn biến thủ công hoặc áp dụng các kỹ thuật lựa chọn mô hình\nRegularization: Kỹ thuật cho phép giữ nguyên số lượng biến đầu vào nhưng giảm giá trị của các tham số \\(\\theta\\) trong mô hình\n\nRegularization: Regularization cho phép tính toán giá trị của các tham số \\(\\theta\\) trong mô hình, sao cho các giá trị của \\(\\theta\\) càng nhỏ càng tốt. Hàm tối ưu hóa được thay đổi lại như sau.\n\\[J(\\theta_0. \\theta_1) = \\underset{\\theta_0, \\theta_1}{\\text{minimize}}\\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{i=1}^n\\theta_j^2\\]\nTrong đó, \\(\\lambda\\sum_{i=1}^n\\theta_j^2\\) được gọi là tham số regularization. Hệ số, \\(\\lambda\\) sẽ quyết định vị độ mạnh của regularization. Nếu \\(\\lambda\\) càng lớn, yếu tố làm giảm thiểu overfitting càng mạnh. Tuy nhiên, nếu \\(\\lambda\\) quá lớn (ví dụ, \\(10^{10}\\)) sẽ khiến cho toàn bộ các tham số \\(\\theta\\) trở về 0, và mô hình sẽ thay đổi từ overfitting sang underfitting.\n\n\n18.4.3 Ensemble methods\nEnsemble methods (tạm dịch: phương pháp kết hợp) là phương pháp tổng hợp nhiều kết quả khác nhau từ những mô hình riêng biệt để có thể xây dựng một mô hình có độ chính xác cao hơn.\nCó 3 loại như sau:\n\nBagging: Xây dựng cùng lúc nhiều mô hình cùng loại trên nhiều tập con của train, mô hình cuối là tổng của các mô hình con (VD: Random Forest)\nBoosting: Nhiều mô hình cùng loại trên nhiều tập con, mô hình sau học và đánh trọng số trên những trường hợp phân loại sai của mô hình trước đó (VD: GBM, XGBoost)\nStacking: Là phương pháp sử dụng nhiều mô hình khác loại, kết hợp để tạo thành 1 mô hình tốt hơn từng mô hình riêng rẽ. Xem ví dụ sau:\n\n\n#Dữ liệu thực tế\n1111111111\n\n#Kết quả dự báo qua 3 classifier\n1111111100  (80%)\n0111011101  (70%)\n1000101111  (60%)\n\n#3 classifier theo số đông\n1111111101  (90%)\n\nLưu ý: Phương pháp này thực hiện tốt với những mô hình phân loại ít tương quan với nhau. Với những mô hình tương quan với nhau nhiều, mô hình này sẽ không áp dụng tốt. Xem ví dụ sau:\n\n#Dữ liệu thực tế\n1111111111\n\n#Kết quả dự báo qua 3 classifier\n1111111100  (80%)\n1111011101  (80%)\n1111011101  (80%)\n\n#3 classifier theo số đông\n1111011101  (80%)\n\nCác phương pháp stacking thông dụng:\n\nLinear ensemble: trung bình có trọng số xác suất các dự báo của từng mô hình phân loại. VD: C1(A-80%, B-20%), C2(A-60%,B-40%) => Mô hình cuối C(70%, B-30%)\nMax ensemble: Mỗi quan sát lấy xác suất lớn nhất trong tất cả các classifier. VD: C1(A-80%, B-20%), C2(A-60%,B-40%) => Mô hình cuối C(80%, B-40%)\nVote ensemble: Giá trị cuối được lấy vote theo từng classifier\nTwo step ensemble: Lấy điểm xác suất của mỗi mô hình làm biến đầu vào của data frame, sau đó áp dụng mô hình dự báo trên data frame mới."
  },
  {
    "objectID": "p03-01-nguyen-ly-du-bao.html#đánh-giá-chất-lượng-mô-hình",
    "href": "p03-01-nguyen-ly-du-bao.html#đánh-giá-chất-lượng-mô-hình",
    "title": "18  Giới thiệu về học máy",
    "section": "18.5 Đánh giá chất lượng mô hình",
    "text": "18.5 Đánh giá chất lượng mô hình\nĐối với mỗi bài toán khác nhau (regression & classification) sẽ có các chỉ số đánh giá chất lượng khác nhau.\n\n18.5.1 Mô hình hồi quy\nVới nhóm bài toán hồi quy, các chỉ số sau thường được dùng để đánh giá chất lượng mô hình.\n\nMSE: Giá trị trung bình của bình phương sai số (Mean squared error) \\(MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2\\). Mục tiêu: Tối thiểu hóa\nRMSE: \\(RMSE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat y_i)^2}\\). Mục tiêu: Tối thiểu hóa\nMAE: Mean absolute error. \\(MAE = \\frac{1}{n} \\sum^n_{i=1}(\\vert y_i - \\hat y_i \\vert)\\). Mục tiêu: Tối thiểu hóa\nRMSLE: Root Mean Squared Log Error \\[RMSLE = \\sqrt{\\frac{1}{n} \\sum^n_{i=1}(log(y_i + 1) - log(\\hat y_i + 1))^2}\\]. Với MSE/RMSE, 1 quan sát có sai số lớn sẽ ảnh hưởng rất lớn đến chất lượng mô hình. Khi sử dụng RMSLE, sai số của mỗi quan sát sẽ có trọng số tương đồng hơn và phù hợp khi xây dựng mô hình có các giá trị biến thiên rộng. Mục tiêu: Tối thiểu hóa\n\\(R^2\\): Chỉ số R-bình phương, cho biết tỷ lệ biến thiên của biến dự báo được thể hiện qua mô hình. Mục tiêu: Tối đa hóa\n\nTrong các chỉ số trên, \\(R^2\\) và \\(MSE\\) là quan trọng nhất. Để nắm vững đánh giá chất lượng mô hình hồi quy, xem kỹ việc đánh giá và diễn giải chỉ số của mô hình OLS.\n\n\n18.5.2 Mô hình phân loại\nĐối với bài toán phân loại, các chỉ số sau là quan trọng nhát:\n\nMisclassification: Tỷ lệ phân loại sai. Ví dụ: Có 2 nhóm Good vs. Bad. Tỷ lệ sai số mỗi nhóm là 10% và 20%. Tỷ lệ phân loại sai trên toàn bộ tập hợp là 30%. Mục tiêu: Tối thiểu hóa\nMean per class error: Tỷ lệ sai số trung bình trong mỗi nhóm. Trong ví dụ trên, tỷ lệ sai số trung bình mỗi nhóm là 15%. Mục tiêu: Tối thiểu hóa\nMSE: Mean square error. Bình phương sai số từ xác suất dự báo đến 1 với class positive. Ví dụ, với positive class, điểm xác suất từ mô hình là 0.95, \\(MSE = (1-0.95)^2\\). Mục tiêu: Tối thiểu hóa\nAUC: Area Under Curve. Được sử dụng để đo lường việc phân loại giữa 2 class trong mô hình phân loại 2 lớp (binary classification). Chỉ số này có giá trị trong khoảng 0-1. Mục tiêu: Tối đa hóa\nRecall (Sensitivity): Trong 100 trường hợp thực tế là positive, mô hình dự báo được bao nhiêu trường hợp. Mục tiêu: Tối đa hóa\nPrecision: Trong 100 trường hợp dự báo là positive, bao nhiều trường hợp thực sự là positive.\n\nVới các chỉ số trên, cần nắm vững chỉ số AUC, Recall và Precision. Để nắm vững đánh giá chất lượng mô hình phân loại, xem kỹ phần mô hình logistic."
  },
  {
    "objectID": "p03-01-nguyen-ly-du-bao.html#nhược-điểm-của-máy-học",
    "href": "p03-01-nguyen-ly-du-bao.html#nhược-điểm-của-máy-học",
    "title": "18  Giới thiệu về học máy",
    "section": "18.6 Nhược điểm của máy học",
    "text": "18.6 Nhược điểm của máy học\nMặc dù máy học (machine learning) có thể giải quyết được rất nhiều vấn đề khác nhau trong cuộc sống. Tuy nhiên, ta không nên coi machine learning là chìa khóa vạn năng có thể giải quyết mọi vấn đề. Ngược lại, machine learning gặp nhiều hạn chế như sau.\n\nKhông áp dụng được với tập dữ liệu hoàn toàn mới: Do các thuật toán của máy học được xây dựng dựa trên dữ liệu của quá khứ, các thuật toán này sẽ không dự báo được chính xác với tập dữ liệu hoàn toàn mới.\n\nVí dụ: Mô hình dự báo khách hàng churn tại Mỹ sẽ không thể áp dụng được tại Việt Nam và ngược lại.\nChính vì lý do này, trong các ứng dụng của học máy, các bài toán nhận diện hình ảnh, âm thanh lại là nhóm được tái sử dụng nhiều nhất do dữ liệu hình ảnh, âm thanh được chuẩn hóa và khá tương đồng với các tập dữ liệu khác nhau.\n\nKhó giải thích: Phần lớn các thuật toán (ngoại trừ linear model) đều thực hiện nhiều phép biến đổi toán học phức tạp và là tổ hợp của nhiều mô hình con (bagging, boosting). Vì vậy, kết quả dự báo của từng quan sát rất khó giải thích và gây ra nhiều khó khăn khi thuyết phục các đơn vị kinh doanh chấp nhận sử dụng kết quả từ mô hình. Để khắc phục nhược điểm này, hiện tại machine learning đang phát triển ngành học máy có giải thích, giúp tập trung giải thích cả global prediction và local prediction.\nĐòi hỏi chất lượng và khối lượng dữ liệu: Các mô hình ML chỉ có thể giải quyết tốt khi chất lượng cũng như khối lượng dữ liệu (số quan sát, số biến) đầu vào đạt ở mức độ nhất đinh. Trong nhiều trường hợp, ta không thể áp dụng các bài toán machine learning mà phải thay thế bằng các kỹ thuật phân tích khám phá dữ liệu."
  },
  {
    "objectID": "p03-01-nguyen-ly-du-bao.html#tài-liệu-tham-khảo",
    "href": "p03-01-nguyen-ly-du-bao.html#tài-liệu-tham-khảo",
    "title": "18  Giới thiệu về học máy",
    "section": "18.7 Tài liệu tham khảo",
    "text": "18.7 Tài liệu tham khảo\n\nIntroduction to Statistical Learning\nHands on Machine Learning with R"
  },
  {
    "objectID": "p03-02-mo-hinh-ols.html#giới-thiệu",
    "href": "p03-02-mo-hinh-ols.html#giới-thiệu",
    "title": "19  Mô hình hồi quy tuyến tính",
    "section": "19.1 Giới thiệu",
    "text": "19.1 Giới thiệu\nMô hình hồi quy tuyến tính là mô hình cơ bản nhất giúp ta nắm được nguyên lý làm việc của machine learning. Với mô hình này, ta cần tìm kiếm một tổ hợp giữa \\(\\theta_0\\) và \\(\\theta_1\\) sao cho khoảng cách giữa mô hình tìm được và các quan sát là nhỏ nhất.\n\n\n\n\n\nĐể tìm ra được tổ hợp tham số \\(\\theta\\), mục tiêu của thuật toán là tối ưu hóa tổng khoảng cách giữa các điểm dự báo và các điểm thực tế. Nói cách khác, ta cần tối ưu hàm sau.\n\\[\\underset{\\theta_0, \\theta_1}{\\text{minimize}}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2\\]\nTrong đó:\n\n\\((i)\\) là thứ tự các quan sát trong tập train\n\\(m\\) là số lượng quan sát\n\\(h_\\theta\\) là mô hình ước lượng mối quan hệ giữa x và y\n\\(y^{(i)}\\) là quan sát thực tế của biến cần dự báo\n\\(x^{(i)}\\) là các quan sát thực tế của biến inputs\n\nTrong thực tế, để dễ dàng hơn trong việc tính toán, ta sẽ tìm giá trị nhỏ nhất của trung bình của bình phương độ lêch. Ta sẽ thay đổi thành hàm sau.\n\\[J(\\theta_0. \\theta_1) = \\underset{\\theta_0, \\theta_1}{\\text{minimize}}\\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2\\]\n\\(J(\\theta_0. \\theta_1)\\) được gọi là cost function. Về mặt bản chất, cost function cho phép chúng ta biết được chất lượng của mô hình so với thực tế. Thông thường, giá trị của hàm này cang thấp, chất lượng mô hình càng tốt."
  },
  {
    "objectID": "p03-02-mo-hinh-ols.html#xây-dựng-mô-hình-cơ-bản",
    "href": "p03-02-mo-hinh-ols.html#xây-dựng-mô-hình-cơ-bản",
    "title": "19  Mô hình hồi quy tuyến tính",
    "section": "19.2 Xây dựng mô hình cơ bản",
    "text": "19.2 Xây dựng mô hình cơ bản\nĐể xây dựng mô hình, ta sử dụng tập dữ liệu phân tích về doanh số bán hàng với quảng cáo.\n\n# Packages\nlibrary(tidyverse)  \nlibrary(modelr)     \nlibrary(broom)    \nlibrary(ISLR)\n\n# Load data (remove row numbers included as X1 variable)\n#advertising <- read_csv(\"http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\") %>%\n#  select(-X1) %>% \n#  rename_all(tolower)\n\nadvertising <- read_csv(\"data/Advertising.csv\") %>%\n select(-1) %>%\n rename_all(tolower)\n\nadvertising %>% head\n\n# A tibble: 6 × 4\n     tv radio newspaper sales\n  <dbl> <dbl>     <dbl> <dbl>\n1 230.   37.8      69.2  22.1\n2  44.5  39.3      45.1  10.4\n3  17.2  45.9      69.3   9.3\n4 152.   41.3      58.5  18.5\n5 181.   10.8      58.4  12.9\n6   8.7  48.9      75     7.2\n\n\nTrong tập dữ liệu trên, sales là doanh số bán hàng (ngàn sản phẩm), các biến khác là chi tiêu marketing theo kênh (ngàn USD). Để xây dựng mô hình, ta cũng chia dữ liệu thành hai phần: train và test.\n\nset.seed(123)\ntrain <- sample_frac(advertising,size = 0.6)\ntest <- advertising %>% anti_join(train)\n\nTa cũng có thể tạo train và test theo cách khác như sau\n\n# Cách khác\ntrain <- sample <- sample(c(TRUE, FALSE), \n                          nrow(advertising), \n                          replace = T, prob = c(0.6,0.4))\ntrain <- advertising[sample, ]\ntest <- advertising[!sample, ]\n\n\n19.2.1 Mô hình hồi quy đơn biến\nMô hình hồi quy đơn biến là mô hình chỉ có 1 biến X. Trong ví dụ này, ta sẽ xây dựng mô hình mối quan hệ giữa doanh số bán hàng và ngân sách được dành cho quảng cáo.\n\\[ Y = \\beta_0 + \\beta_1X + \\epsilon \\tag{1}\\]\nTrong đó:\n\n\\(Y\\) là biến sales\n\\(X\\) là biến TV advertising budget\n\\(\\beta_0\\) là hệ số tự do (intercept)\n\\(\\beta_1\\) là hệ số tương quan hay hệ số góc (độ dốc của đường hồi quy tuyến tính)\n\\(\\epsilon\\) là sai số của mô hình - sai số này thường được giả định là có giá trị trung bình bằng 0 và tuân theo phân phối chuẩn.\n\nĐể xây dựng mô hình, ta thực hiện như sau.\n\nmodel1 <- lm(sales ~ tv, data = train)\nmodel1 %>% summary\n\n\nCall:\nlm(formula = sales ~ tv, data = train)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.794 -2.083  0.008  1.884  6.359 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.503512   0.558123   13.44   <2e-16 ***\ntv          0.043695   0.003296   13.26   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.077 on 118 degrees of freedom\nMultiple R-squared:  0.5983,    Adjusted R-squared:  0.5949 \nF-statistic: 175.8 on 1 and 118 DF,  p-value: < 2.2e-16\n\n\n\ntrain %>% \n  mutate(sale_fit = predict(model1, train)) %>% \n  sample_frac(0.4) %>% \n  ggplot(aes(tv, sales)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_line(aes(x = tv, y = sale_fit), col = \"darkred\", size = 1) +\n  geom_segment(aes(x = tv, xend = tv, y = sales, yend = sale_fit)) +\n  theme_bw() +\n  labs(title = \"Sales vs. TV marketing budget\") +\n  theme(panel.border = element_blank(), \n        axis.line = element_line(colour = \"black\"))\n\n\n\n\n\n\n19.2.2 Diễn giải mô hình\n\nresult <- model1 %>% tidy\nresult\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   7.50     0.558        13.4 1.48e-25\n2 tv            0.0437   0.00330      13.3 4.00e-25\n\n\nHàm tidy trong broom cho phép làm sạch kết quả mô hình hồi quy và chuyển dữ liệu thành dạng bảng dataframe. Mối quan hệ giữa sale và marketing được thể hiện như sau.\nsales = 7.5035125 + 0.0436952 * tivi marketing budget\nNhư vậy, nếu marketing tăng 1 đơn vị (1000 USD), doanh số sẽ tăng 43.6951732 sản phẩm\nĐánh giá mức độ ý nghĩa của hệ số góc\n\\[SE(\\beta_0)^2 = \\sigma^2\\bigg[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2} \\bigg], \\quad SE(\\beta_1)^2 = \\frac{\\sigma^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2}   \\tag{3} \\]\ntrong đó \\[\\sigma^2 = Var(\\epsilon)\\].\nViệc tính toán \\(SE\\) cho phép ta tính được mức độ tinh cậy 95% của hệ số \\(\\beta\\) như sau.\n\\[ \\beta \\pm 2 \\cdot SE(\\beta)  \\tag{4}\\]\nTrong R, để tìm ra mức độ tin cậy 95% của hệ số \\(\\beta\\), ta có thể làm như sau.\n\nconfint(model1)\n\n                 2.5 %     97.5 %\n(Intercept) 6.39827600 8.60874896\ntv          0.03716873 0.05022162\n\n\nNhư vậy, ta thấy độ tin cậy 95% của hệ số \\(\\beta_1\\) khác 0. Ta có thể kiểm tra dựa trên p-value của mô hình đều nhỏ hơn 0.05\n\nmodel1 %>% tidy\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   7.50     0.558        13.4 1.48e-25\n2 tv            0.0437   0.00330      13.3 4.00e-25"
  },
  {
    "objectID": "p03-02-mo-hinh-ols.html#đánh-giá-chất-lượng-mô-hình",
    "href": "p03-02-mo-hinh-ols.html#đánh-giá-chất-lượng-mô-hình",
    "title": "19  Mô hình hồi quy tuyến tính",
    "section": "19.3 Đánh giá chất lượng mô hình",
    "text": "19.3 Đánh giá chất lượng mô hình\nĐể đánh giá chất lượng mô hình, ta sẽ quan tâm đến 3 chỉ số sau:\n\nSai số của mô hình\nR bình phương (\\(R^2\\))\nF-statistic\n\n\n19.3.1 Sai số của mô hình\n\\[ RSE = \\sqrt{\\frac{1}{n-2}\\sum^n_{i=1}(y_i - \\hat{y}_i)^2} \\tag{6}\\]\n\nsigma(model1)\n\n[1] 3.077356\n\n\nVới sai số như trên, ta có thể nói rằng, doanh số thực tế trên thị trường sẽ có sai số so với dự báo 1 khoảng 3.1. Sai số tương đối của mô hình so với thực tế sẽ là\n\nsigma(model1)/mean(train$sales)\n\n[1] 0.2214324\n\n\nNhư vậy, sai số của mô hình so với thực tế sẽ là 23.6%.\n\n\n19.3.2 R bình phương \\(R^2\\)\nChỉ số RSE cho ta biết được sai số của mô hình khi dự báo. Bên cạnh đó, 1 chỉ số thường xuyên được sử dụng là chỉ số \\(R^2\\). Chỉ số này đo lường tỷ lệ biến động của biến Y được dự báo qua việc sử dụng mô hình (proportion of variance explained). Nếu không sử dụng mô hình, ta có thể dự báo giá trị của Y bằng giá trị trung bình.\n\nTổng bình phương các sai số giữa giá trị thực tế và giá trị trung bình được gọi là độ biến động của dữ liệu (TSS - Total Sum of Square)\nTổng bình phương các sai số giữa giá trị dự báo và giá trị thực tế được gọi là sai số của mô hình (RSS - Residual Sum of Square)\nTổng bình phương các sai số giữa giá trị dự báo và giá trị trung bình thực tế được gọi là tổng biến động được giải thích qua mô hình (ESS - Explained Sum of Square)\n\n\\[ R^2 = 1 - \\frac{RSS}{TSS}= 1 - \\frac{\\sum^n_{i=1}(y_i-\\hat{y}_i)^2}{\\sum^n_{i=1}(y_i-\\bar{y}_i)^2} \\tag{7}\\]\n\nrsquare(model1, data = train)\n\n[1] 0.5983356\n\n\nVới kết quả trên, mô hình giải thích được 59.8335559% độ biến động của dữ liệu thực tế.\n\n\n\n19.3.3 F statistic\nChỉ số thống kê F được tính như sau.\n\\[F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)} \\tag{8} \\]\nChỉ số này được sử dụng để kiểm định các hệ số trong mô hình có thực sự khác 0.\n\\(H_0\\): Tất cả các hệ số \\(\\beta\\) trong mô hình đều bằng không \\(H_1\\): Có ít nhất một hệ số \\(\\beta\\) trong mô hình khác không\nTrong mô hình trên, giá trị p-value của F-statistic rất thấp, như vậy, có ít nhất một hệ số trong mô hình khác không."
  },
  {
    "objectID": "p03-02-mo-hinh-ols.html#dự-báo",
    "href": "p03-02-mo-hinh-ols.html#dự-báo",
    "title": "19  Mô hình hồi quy tuyến tính",
    "section": "19.4 Dự báo",
    "text": "19.4 Dự báo\nĐể dự báo kết quả mô hình, ta có thể dùng hàm predict trong R như sau.\n\n# Model result\nresult <- test %>% \n  select(tv, sales) %>% \n  mutate(predict = predict(model1, test)) %>% \n  mutate(error = predict - sales)\n\nsqrt(sum(result$error^2)/nrow(result))\n\n[1] 3.55047\n\n# Cách hai\nmodelr::rmse(model1, test)\n\n[1] 3.55047\n\n\nVẽ kết quả mô hình với dự báo\n\nresult %>% \n  select(-error) %>% \n  rename(predicted_sales = predict,\n         real_sales = sales) %>% \n  gather(key = \"type\", value = \"value\", -tv) %>% \n  ggplot(aes(tv, value)) +\n  geom_point(aes(col = type), alpha = 0.5) +\n  geom_line(aes(col = type), alpha = 0.6) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  labs(title = \"Sales vs. TV marketing budget\",\n       y = \"Sales\",\n       x = \"TV budget\") +\n  scale_color_manual(values = c(\"darkred\", \"darkblue\")) +\n  theme(panel.border = element_blank(), \n        axis.line = element_line(colour = \"black\"))"
  },
  {
    "objectID": "p03-02-mo-hinh-ols.html#thuật-toán-tối-ưu-với-gradient-decent",
    "href": "p03-02-mo-hinh-ols.html#thuật-toán-tối-ưu-với-gradient-decent",
    "title": "19  Mô hình hồi quy tuyến tính",
    "section": "19.5 Thuật toán tối ưu với gradient decent",
    "text": "19.5 Thuật toán tối ưu với gradient decent\nỞ phần trước, ta đã học cách áp dụng mô hình hồi quy đơn giản trong việc dự báo các biến liên tục. Ở phần này, ta sẽ tìm hiểu thêm về cách sử dụng học máy trong việc xây dựng mô hình hồi quy tuyến tính.\nViệc ưu hóa hàm chi phí với các tham số khác nhau là vấn đề cốt lõi của bất kỳ thuật toán machine learning nào. Tuy nhiên làm thế nào để tối ưu được, ta cần phải áp dụng các thuật toán tối ưu. Trong đó, quan trọng và phổ biến nhất là thuật toan gradient descent.\nĐể tối ưu hóa cost function, ta có thể thay đổi các tổ hợp giá trị của \\(\\theta\\) cho đến khi tìm được tổ hợp mà tại đó, cost function đạt giá trị nhỏ nhất. Tuy nhiên, với trường hợp mô hình nhiều biến, ta sử dụng thuật toán gradient decient để tìm tập \\(\\theta\\) mà tại đó, cost function đạt giá trị nhỏ nhất.\nQuá trình sẽ diễn ra cho đến khi kết quả của cost function hội tụ tại 1 điểm với thuật toán sau.\n\\[\\theta_j := \\theta_j - \\alpha\\frac{d}{d\\theta_j}J(\\theta_0, \\theta_1)\\]\nTrong đó:\n\n\\(\\alpha\\) được gọi là learning rate, cho phép kiểm soát tốc độ update tham số trong mô hình.\n:= là dấu gán\n\\(\\frac{d}{d\\theta_j}J(\\theta_0, \\theta_1)\\) là đạo hàm riêng phần theo \\(\\theta\\) của hàm \\(cost function\\)\n\nÁp dụng công thức tính đạo hàm hàm hợp, ta được\n\\[\\cases{\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i = 1}^m(h_{\\theta}(x^{(i) - y^{(i)}}))\\\\\n        \\theta_1 := \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i = 1}^m(h_{\\theta}(x^{(i) - y^{(i)}}))\\times x^{(i)}}\\]\nThuật toán trên còn được gọi là batch gradient bởi lẽ thuật toán trên tính toán đến toàn bộ quan sát trên tập dữ liệu huấn luyện (train data). Bên cạnh batch gradient, còn có stochastic gradient descent và mini batch gradient descent.\nCông thức tổng quát để update tham số trong mô hình machine learning như sau.\n\\[\\Theta^1 = \\Theta^0 - \\alpha \\nabla J(\\Theta)\\] Trong đó:\n\n\\(\\Theta^1\\): Giá trị tham số mới\n\\(\\Theta^0\\): Giá trị tham số hiện tại\n\\(\\alpha\\): Learning rate, tốc độc update tham số\n\\(\\nabla J(\\Theta)\\): Đạo hàm riêng phần của từng tham số theo giá trị của biến đầu vào.\n\\(-\\): Hướng update tham số. Nếu giá trị đạo hàm riêng phần là dương, ta cần giảm giá trị của tham số, nếu giá trị đạo hàm riêng phần là âm, ta cần tăng giá trị của tham số.\n\n\nGiải thích gradient descent bằng ngôn ngữ đơn giản:\nThuật toán gradient descent trông thì phức tạp, nhưng thực tế, cách triển khai và tối ưu thuật toán khá giống với phương pháp agile trong phát triển phần mềm. Thuật toán có thể được mô tả đơn giản lại như sau.\n\nXây dựng mô hình một cách nhanh chóng (build). Tại bước này, ta chọn ngẫu nhiên giá trị của tham số để có thể xây dựng mô hình một cách đơn giản nhất.\nĐo lường độ lệch của mô hình so với thực tế (measure). Tại bước này, ta tính toán sai số giữa mô hình vừa xây dựng và dữ liệu thực tế để đo lường sai số của mô hình.\nThay đổi/update mô hình dựa trên feedback (learn/action). Bước này sẽ phức tạp hơn một chút do ta phải trả lời hai câu hỏi:\n\nTham số cần được thay đổi theo hướng nào? Tăng giá trị lên hay giảm giá trị xuống?\nTốc độ thay đổi là bao nhiêu.\n\n\nVới câu hỏi thứ nhất, ta cần tính được giá trị đạo hàm riêng phần của từng tham số tại giá trị của X. Nếu giá trị này dương, ta cần giảm giá trị của tham số và ngược lại\nVới câu hỏi thứ hai, ta phải đặt trước một giá trị quy định tốc độ thay đổi của tham số. Giá trị này được gọi là learning rate.\nMô hình hồi quy.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nn     <- 200 # số lượng quan sát\nbias  <- 4\nslope <- 3.5\ndot   <- `%*%` # Hàm tính phép nhân ma trận\n\nx   <- rnorm(n) * 2\nx_b <- cbind(x, rep(1, n)) # Thêm hệ số beta_0 cho ma trận x\ny   <- bias + slope * x + rnorm(n)\ndf  <- data_frame(x = x,  y = y)\n\ndf %>% \n  ggplot(aes(x,y)) +\n  geom_point(alpha = 0.5) +\n  theme_minimal()\n\n\n\nlearning_rate <- 0.05\nn_iterations  <- 100\ntheta         <- matrix(c(20, 20)) # Giá trị tham số đầu tiên\n\nb0    <- vector(\"numeric\", length = n_iterations)\nb1    <- vector(\"numeric\", length = n_iterations)\nsse_i <- vector(\"numeric\", length = n_iterations)\n\n\nfor (iteration in seq_len(n_iterations)) { \n  # Mô hình dự báo với theta\n  yhat               <- dot(x_b, theta)         \n  # Tính sai só của mô hình\n  residuals_b        <- yhat - y                \n  # Tính gradient (đạo hàm riêng phần cho giá trị theta)\n  gradients          <- 2/n * dot(t(x_b), residuals_b)  # Lưu ý\n  theta              <- theta - learning_rate * gradients # update theta \n  \n  sse_i[[iteration]] <- sum((y - dot(x_b, theta))**2)\n  b0[[iteration]]    <- theta[2]\n  b1[[iteration]]    <- theta[1]\n}\n\nmodel_i <- data.frame(model_iter = 1:n_iterations, \n                      sse = sse_i, \n                      b0 = b0, \n                      b1 = b1)\n\nLưu ý: Với mô hình hồi quy, hàm chi phí (cost function) là MSE.\n\\[J(\\beta_0, \\beta_1) = \\frac{1}{n}(\\beta_0 + \\beta_1*X - Y)^2\\]\n\\[\\frac{dJ}{d\\beta_0}=\\frac{2}{n}(\\beta_0 + \\beta_1*X - Y) = 2 * error\\] \\[\\frac{dJ}{d\\beta_1}=\\frac{2}{n}(\\beta_0 + \\beta_1*X - Y)*X= 2 * error * X\\] Do đó, ta có thể khái quát hóa gradient cho hai tham số như sau.\n\\[\\nabla J = 2*error*X'\\]\nTrong đó, X’ là [1, X]\n\np1 <- df %>% \n  ggplot(aes(x=x, y=y)) + \n  geom_abline(aes(intercept = b0, \n                  slope = b1, \n                  colour = -sse),\n              data = model_i, \n              alpha = .50 \n              ) +\n  geom_point(alpha = 0.4) + \n  geom_abline(aes(intercept = b0, \n                  slope = b1), \n              data = model_i[100, ], \n              alpha = 0.5, \n              size = 2, \n              colour = \"dodger blue\") +\n  geom_abline(aes(intercept = b0, \n                  slope = b1),\n              data = model_i[1, ],\n              colour = \"red\", \n              alpha = 0.5,\n              size = 2) + \n  scale_color_continuous(low = \"red\", high = \"grey\") +\n  guides(colour = FALSE) +\n  theme_minimal()\n\np1\n\n\n\np2 <- model_i[1:30,] %>%\n  ggplot(aes(model_iter, sse, colour = -sse)) + \n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  labs(x = \"Model iteration\", \n       y = \"Sum of Sqaured errors\") + \n  scale_color_continuous(low = \"red\", high = \"dodger blue\") +\n  guides(colour = FALSE)\np2"
  },
  {
    "objectID": "p03-02-mo-hinh-ols.html#các-lưu-ý-khác-trong-xây-dựng-mô-hình",
    "href": "p03-02-mo-hinh-ols.html#các-lưu-ý-khác-trong-xây-dựng-mô-hình",
    "title": "19  Mô hình hồi quy tuyến tính",
    "section": "19.6 Các lưu ý khác trong xây dựng mô hình",
    "text": "19.6 Các lưu ý khác trong xây dựng mô hình\nKhi xây dựng mô hình, OLS cần phải đảm bảo 4 giả định sau.\n\nPhương sai của sai số không đổi theo thời gian (Homoscedaticity)\nSai số mô hình có phân phối chuẩn (Normality)\nSai số không có tự tương quan (Auto correlation)\nKhông có hiện tượng đa cộng tuyến (Msulti colinearity)\n\n\n19.6.1 Heteroskedaticity\n\nĐịnh nghĩa: Phương sai của residuals là ổn định\nTest Goldfeld-Quandt:\n\nHo: Phương sai ổn định\nH1: Phương sai không ổn định\n\nCách thực hiện:\n\nChia residuals thành 2 phần ngẫu nhiên\nKiểm tra phương sai giữa 2 residuals\nThực hiện test so sánh phương sai\n\n\n\nrequire(lmtest)\nx <- rep(c(-1,1), 50)\nerr1 <- c(rnorm(50, sd=1), rnorm(50, sd=2)) # heteroskedastycznosc\nerr2 <- rnorm(100) # homoskedastycznosc\ny1 <- 1 + x + err1\ny2 <- 1 + x + err2\ngqtest(y1 ~ x); \n\n\n    Goldfeld-Quandt test\n\ndata:  y1 ~ x\nGQ = 5.0738, df1 = 48, df2 = 48, p-value = 4.929e-08\nalternative hypothesis: variance increases from segment 1 to 2\n\n# Loại bỏ Ho, residuals có phương sai ko ổn đinh\ngqtest(y2 ~ x);\n\n\n    Goldfeld-Quandt test\n\ndata:  y2 ~ x\nGQ = 0.84014, df1 = 48, df2 = 48, p-value = 0.7257\nalternative hypothesis: variance increases from segment 1 to 2\n\n# Giữ Ho, mô hình có phương sai của residual ổn định\n\n\nVí dụ 2:\n\n\nrequire(lmtest);\nn <- 10^3;\nx <- sort(runif(n, -2, 2));\nerr <- rnorm(n, 0, 1);\ny1 <- 1 + x + x*err; # Heteroskedaticity\ny2 <- 1 + x + err; # Homoskedaticity\n\npar(mfrow=c(2,1));\nplot(x, y1, pch=20, col=\"gray\");\nmodel1 <- lm(y1 ~ x);\nabline(model1);\nplot(x, y2, pch=20, col=\"gray\");\nmodel2 <- lm(y2 ~ x);\nabline(model2);\n\n\n\ngqtest(y1 ~ x); # Heteroskedaticity\n\n\n    Goldfeld-Quandt test\n\ndata:  y1 ~ x\nGQ = 0.91204, df1 = 498, df2 = 498, p-value = 0.8477\nalternative hypothesis: variance increases from segment 1 to 2\n\ngqtest(y2 ~ x); # Homoskedaticity\n\n\n    Goldfeld-Quandt test\n\ndata:  y2 ~ x\nGQ = 0.92658, df1 = 498, df2 = 498, p-value = 0.8024\nalternative hypothesis: variance increases from segment 1 to 2\n\n\n\n\n19.6.2 Phân phối chuẩn của phần dư\n\nTest Shapiro:\n\nHo: Residuals có phân phối chuẩn\nH1: Không có phân phối chuẩn\n\n\n\nrequire(lmtest);\nshapiro.test(rnorm(1000, mean = 5, sd = 3))\n\n\n    Shapiro-Wilk normality test\n\ndata:  rnorm(1000, mean = 5, sd = 3)\nW = 0.99909, p-value = 0.916\n\nshapiro.test(runif(1000, min = 2, max = 4))\n\n\n    Shapiro-Wilk normality test\n\ndata:  runif(1000, min = 2, max = 4)\nW = 0.95705, p-value < 2.2e-16\n\nshapiro.test(residuals(model1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(model1)\nW = 0.96333, p-value = 3.663e-15\n\npar(mfrow = c(1,1))\nhist(residuals(model1))\n\n\n\n#Kiểm tra đồ thị\nqqnorm(residuals(model1), col = \"blue\")\nqqline(residuals(model1), col = \"red\") \n\n\n\n\n\nTest Jaque-Bera\n\nHo: Phân phối chuẩn\nH1: Không có phân phối chuẩn\n\n\n\nrequire(tseries);\nx <- rnorm(100)  # H0\njarque.bera.test(x)\n\n\n    Jarque Bera Test\n\ndata:  x\nX-squared = 1.6723, df = 2, p-value = 0.4334\n\nx <- runif(100)  # Ha\njarque.bera.test(x)\n\n\n    Jarque Bera Test\n\ndata:  x\nX-squared = 4.5872, df = 2, p-value = 0.1009\n\n\n\n\n19.6.3 Auto correlation của residuals\n\nTest Dublin Watson\n\nHo: Giá trị auto correlation = 0\nH1: rho khác 0\n\n\n\nlibrary(stats)\nerr1 <- rnorm(100) # Nhiễu trắng (white error)\nerr2 <- stats::filter(err1, 0.7, method=\"recursive\") \n# AR với rho = 0.7\nx <- runif(100, 0, 1)\ny1 <- 1 + x + err1\ny2 <- 1 + x + err2\ndwtest(y1 ~ x)\n\n\n    Durbin-Watson test\n\ndata:  y1 ~ x\nDW = 2.1056, p-value = 0.7004\nalternative hypothesis: true autocorrelation is greater than 0\n\ndwtest(y2 ~ x)\n\n\n    Durbin-Watson test\n\ndata:  y2 ~ x\nDW = 0.60472, p-value = 9.094e-13\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\nTest Breusch - Godfrey\n\n\nerr1 <- rnorm(100) # bialy szum\nerr2 <- stats::filter(err1, 0.7, method=\"recursive\") # model AR(1) z parametrem 0.7\nx <- runif(100, 0, 1)\ny1 <- 1 + x + err1\ny2 <- 1 + x + err2\nbgtest(y1 ~ x, order=2);\n\n\n    Breusch-Godfrey test for serial correlation of order up to 2\n\ndata:  y1 ~ x\nLM test = 1.271, df = 2, p-value = 0.5297\n\nbgtest(y2 ~ x, order=2);\n\n\n    Breusch-Godfrey test for serial correlation of order up to 2\n\ndata:  y2 ~ x\nLM test = 37.973, df = 2, p-value = 5.68e-09\n\n\n\n\n19.6.4 Multi-collinearity\nHiện tượng đa cộng tuyến xảy ra khi các biến độc lập có tương quan với nhau.\n\nlibrary(car)\nx <- rnorm(100,1,1)\ny <- 3*x + rnorm(100,0,1)\nz <- x + 2*y + rnorm(100,1,2)\nmodel1 <- lm(z ~ x + y) \nsummary(model1)\n\n\nCall:\nlm(formula = z ~ x + y)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3956 -1.3773 -0.1967  1.2191  7.1716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.7597     0.3290   2.309   0.0231 *  \nx             1.4728     0.6678   2.206   0.0298 *  \ny             1.9277     0.2151   8.961 2.34e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.245 on 97 degrees of freedom\nMultiple R-squared:  0.9005,    Adjusted R-squared:  0.8984 \nF-statistic: 438.8 on 2 and 97 DF,  p-value: < 2.2e-16\n\n#Kiểm tra\nfit <- lm(mpg~disp+hp+wt+drat, data=mtcars)\nvif(model1) # variance inflation factors \n\n       x        y \n7.201945 7.201945 \n\nsqrt(vif(model1))\n\n       x        y \n2.683644 2.683644 \n\n\nNếu giá trị của sqrt(VIF) > 2, khả năng có multi-collinearity là rất cao. Khi có đa cộng tuyến, ta có thể bỏ biến xảy ra hiện tượng trên hoặc sử dụng Ridge regression."
  },
  {
    "objectID": "p03-02-mo-hinh-ols.html#các-quan-sát-ngoại-lai",
    "href": "p03-02-mo-hinh-ols.html#các-quan-sát-ngoại-lai",
    "title": "19  Mô hình hồi quy tuyến tính",
    "section": "19.7 Các quan sát ngoại lai",
    "text": "19.7 Các quan sát ngoại lai\nCó 3 loại quan sát ngoại lai:\n\nOutliers (discrepancy): Y nằm ngoài giá trị thông thường\nLeverage: X có giá trị khác thường\nInfluentual observation: Bao gồm cả quan sát của Y và X.\n\nXem phân biệt 3 loại trong bảng dưới đây\n\n\n19.7.1 Outliers\nOutliers là các quan sát không được dự báo tốt trong mô hình. Các quan sát này có thể thấy rõ trên đường Q-Q plot\n\nlibrary(car)\nstates <- as.data.frame(\n  state.x77[,c(\"Murder\", \"Population\",\n\"Illiteracy\", \"Income\", \"Frost\")])\nfit <- lm(Murder ~ \n          Population + Illiteracy + Income + Frost,\n          data=states)\nqqPlot(fit, \n       labels=row.names(states), \n       id.method=\"identify\",\n       simulate=TRUE, \n       main=\"Q-Q Plot\")\n\n\n\n\n      Nevada Rhode Island \n          28           39 \n\noutlierTest(fit)\n\n       rstudent unadjusted p-value Bonferroni p\nNevada 3.542929         0.00095088     0.047544\n\n\nKiểm định outliers test:\n\n\\(H_0\\): Quan sát là quan sát thông thường\n\\(H_1\\): Quan sát là outliers\n\n\n\n19.7.2 High-leverage points\nCác quan sát được cho là có leverage cao được tính theo hat-statistics. Giá trị trung bình của chỉ số này là p/n. Nếu các quan sát có hat cao hơn 2 lần giá trị trung bình thì cần phải được xem xét kỹ.\n\nhat.plot <- function(fit) {\n  p <- length(coefficients(fit))\n  n <- length(fitted(fit))\n  plot(hatvalues(fit), main = \"Index Plot of Hat Values\")\n  abline(h = c(2, 3) * p / n,\n  col = \"red\",\n  lty = 2)\n  identify(1:n, hatvalues(fit), names(hatvalues(fit)))\n}\nstates <- as.data.frame(\n  state.x77[, c(\"Murder\", \"Population\",\n            \"Illiteracy\", \"Income\", \"Frost\")])\nfit <- lm(Murder ~ \n          Population + Illiteracy + Income + Frost,\n          data=states)\nhat.plot(fit)\n\n\n\n\ninteger(0)\n\n\n\n\n19.7.3 Influential observations\nCác quan sát có ảnh hưởng mạnh đến mô hình là các quan sát mà khi ta loại bỏ quan sát đó ra, sẽ thay đổi mô hình.\nCook-distance lớn hơn \\(\\frac{4}{n-k-1}\\) được gọi là các quan sát có ảnh hưởng\n\ncutoff <- 4/(nrow(states)-length(fit$coefficients)-2)\nplot(fit, which = 4, cook.levels = cutoff)\nabline(h = cutoff, col = \"red\")\n\n\n\n\nTuy nhiên, cách tiếp cận trên không chỉ cho ta biết chính xác các quan sát đó ảnh hưởng đến mô hình như thế nào.\n\nlibrary(car)\navPlots(fit, ask = F, id.method = \"identity\")\n\n\n\ninfluencePlot(fit, id.method =\"identity\")\n\n\n\n\n                StudRes        Hat       CookD\nAlaska        1.7536917 0.43247319 0.448050997\nCalifornia   -0.2761492 0.34087628 0.008052956\nNevada        3.5429286 0.09508977 0.209915743\nRhode Island -2.0001631 0.04562377 0.035858963"
  },
  {
    "objectID": "p03-02-mo-hinh-ols.html#tài-liệu-tham-khảo",
    "href": "p03-02-mo-hinh-ols.html#tài-liệu-tham-khảo",
    "title": "19  Mô hình hồi quy tuyến tính",
    "section": "19.8 Tài liệu tham khảo",
    "text": "19.8 Tài liệu tham khảo\n\nhttp://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/\nChapter 8 - Regresssion - R in Actions"
  },
  {
    "objectID": "p03-03-logistic-regression.html#giới-thiệu",
    "href": "p03-03-logistic-regression.html#giới-thiệu",
    "title": "20  Mô hình hồi quy logistic",
    "section": "20.1 Giới thiệu",
    "text": "20.1 Giới thiệu\nKhi phân tích dữ liệu, bên cạnh biến liên tục, ta thường xuyên phải làm việc với các biến rời rạc, các biến có đặc tinh nhóm (category) như “mua hàng vs. không mua hàng”, “giàu vs. nghèo”, “nợ xấu vs. không nợ xấu”. Quá trình phân tích và dự báo các quan sát thuộc về nhóm nào được gọi là quá trình phân loại (classification).\nĐối với bài toán phân loại, ta có thể chia thành hai nhóm lớn là phân loại hai biến và phân loại nhiều biến. Trong đó, bài toán phân loại hai biến là bài toán phổ biến nhất, được ứng dụng trong nhiều lĩnh vực trong đời sống như đánh giá điểm tín dụng của khách hàng, dự báo khách hàng phản ứng với chiến dịch marketing…\nTrong chương này, chúng ta sẽ làm quen vơi kỹ thuật nổi tiếng và được ứng dụng sớm nhất trong việc phân loại hai biến (binary classification) là logistic regression.\nNếu như mô hình hồi quy tuyến tính thường (linear regression) cho phép chúng ta tìm được mối quan hệ giữa biến phụ thuộc liên tục (continous dependent variable) và một/nhiều biến độc lập (independent variable) - liên tục hoặc không liên tục thì mô hình hồi quy Logistic tìm mối quan hệ giữa biến phụ thuộc là biến nhị phân - binary variable. Biến cần dự báo dạng này chỉ nhận 2 giá trị: có/không, sống/chết, yêu/không yêu… và các biến độc lập có thể là liên tục hoặc không liên tục.\nVí dụ, ta tìm hiểu mối quan hệ giữa việc KH có nợ xấu hay không dựa vào dư nợ của KH - thông qua việc tìm hiểu khả năng khách hàng có nợ xấu dựa vào thông tin dư nợ của KH. Trong đó:\n\nBiến phụ thuộc:\n\n\\[Y = \\begin{cases}1, & KH\\:có\\:nợ\\:xấu\\\\\n  0,  & KH\\:không\\:có\\:nợ\\:xấu\\end{cases}\\]\n\nBiến độc lập: X - dư nợ của KH (hay số tiền khách hàng nợ ngân hàng)\n\nTrong mô hình này, ta không thể sử dụng mô hình hồi quy thông thường bởi lẽ biến phụ thuộc chỉ có hai giá trị 0 và 1. Hơn nữa, cái ta cần dự báo là “xác suất” xảy ra sự kiện (xác suất vỡ nợ) cho mỗi quan sát. Để giải quyết vấn đề nay, ta có thể sử dụng mô hình logistic có dạng như sau:\n\\[p= Pr(Y=1)=\\frac{e^{B_{0} + B_{1}X}}{1+e^{B_{0} + B_{1}X}}\\] Trong đó, \\(p\\) là xác suất khách hàng có nợ xấu, \\((1-p)\\) là xác suất khách hàng không có nợ xấu.\n\\[Pr(Y=0)=1-p=\\frac{1}{1+e^{B_{0} + B_{1}X}}\\]\nLưu ý: Trong các bài toán phân loại, nhóm ta cần dự báo còn được gọi là class positive hay event of interest. Khi ứng dụng với các bài toán cụ thể khác nhau, class sẽ được xây dựng khác nhau. Ví dụ: trong dự báo khách hàng phản hồi, thì sự kiện khách hàng phản hồi email là 1, không phản hồi email là 0. Khi dự báo khách hàng có nợ xấu, khách hàng có nợ xấu được đánh dấu là 1, không có nợ xấu được đánh dấu là 0.\nTrong mô hình logistic, ta còn có một chỉ số khác được gọi là \\(Odds\\: ratio\\), tạm dịch là khả năng xảy ra sự kiện so với khả năng không xảy ra sự kiện. Với bài toán dự báo nợ xấu, chỉ số này cho chúng ta biết xác suất khách hàng có nợ xấu cao hơn bao nhiêu lần bao nhiêu lần so với xác suất khách hàng không có nợ xấu.\n\\[Odds=\\frac{p}{1-p}=e^{B_{0} + B_{1}X}\\]\nVí dụ:\n\nXác suất khách hàng có nợ xấu: p = 0.8\nXác suất khách hàng không có nợ xấu: 1-p = 1-0.8=0.2\nOdd ration: \\(p/(1-p) = 4\\) - khả năng khách hàng có nợ xấu cao gấp 4 lần khả năng khách hàng không có nợ xấu.\n\nLogit: Là hàm log của odd ratio, được tính như sau.\n\\[logit=ln(Odds)=ln(\\frac{p}{1-p}) = B_{0} + B_{1}X\\]\nPhương trình trên gọi là phương trình hồi quy Logistic. Hàm này có thể chứa các giá trị bất kỳ trong khoảng từ \\((-\\infty, +\\infty)\\). Lúc này, hàm trên đã trở về hàm hồi quy thông thường. Như vậy, mô hình chúng ta cần xây dựng có dạng:\n\\[logit(p) = \\alpha + \\beta X\\]\nhay:\n\\[odds(p) = \\frac{p}{1-p} = e^{\\alpha + \\beta X}\\]\nGiải thích ý nghĩa: Khi x tăng thêm 1 đơn vị, thì Odds ratio (tỷ số khả dĩ) sẽ tăng thêm \\(e^{\\beta}\\). Nghĩa là, khi x tăng thêm 1 đơn vị, khả năng (chance) xảy ra biến cố X sẽ tăng lên \\(e^{\\beta}\\) lần.\n\nKhi \\(x = x_0\\), khả năng xảy ra biến cố là: \\[odds(p|x=x_0) = e^{\\alpha + \\beta x_0}\\]\nKhi \\(x = x_0 + 1\\), khả năng xảy ra biến cố là: \\[odds(p|x=x_0+1) = e^{\\alpha + \\beta (x_0+1)}\\]\nTỷ số giữa 2 khả năng là: \\[\\frac{e^{\\alpha + \\beta (x_0+1)}}{e^{\\alpha + \\beta (x_0)}} = e^{\\beta}\\]\n\nLưu ý: Cách giải thích ý nghĩa của hệ số \\(\\beta\\) trong Logistic rất dễ nhầm lẫn. Cơ bản có 2 cách như sau:\n\nCách 1: Giải thích dựa trên “xác suất” xảy ra biến cố A. Cách tính này dựa trên việc đưa ra tính toán xác suất xảy ra A với X cho trước.\nCách 2: Dựa trên “khả năng” xảy ra biến cố A (Odd ratio)"
  },
  {
    "objectID": "p03-03-logistic-regression.html#cách-xây-dựng-mô-hình",
    "href": "p03-03-logistic-regression.html#cách-xây-dựng-mô-hình",
    "title": "20  Mô hình hồi quy logistic",
    "section": "20.2 Cách xây dựng mô hình",
    "text": "20.2 Cách xây dựng mô hình\nĐể ước lượng các tham số \\(\\beta_j\\), ta dùng phương pháp hợp lý tối đa (maximum likelihood), nghĩa là tìm \\(\\beta_j\\) để tối đa hóa hàm likelihood.\n\\[L = p_1*p_2*...*p_n= \\prod_{i=1}^np_i\\]\nTrong đó, \\(p_i\\) là xác suất xảy sự kiện của quan sát thứ \\(i\\).\nTrong bài toán hồi quy logistic, ta quan tâm đến việc dự báo chính xác sự kiện xảy ra (\\(P_{y = 1}\\)). Do đó, hàm likelihood trong bài toán nhị phân được tính như sau.\n\\[L = \\prod[-y_i*log(h_\\theta(x_i)) - (1-y_i)log(1-h_\\theta(x_i))]\\] Trong đó:\n\n\\(y_i\\): Là giá trị thực tế của biến cần dự báo (1 hoặc 0) tại quan sát i\n\\(h_{\\theta}(x_i)\\): Giá trị dự báo của quan sát i. Giá trị này nằm trong khoảng [0, 1]\n\nXem ví dụ dưới đây:\n\n\n\n\n\n\n\n\nThực tế\nDự báo\nL\n\n\n\n\n1\n0.7\n0.7\n\n\n0\n0.6\n0.4\n\n\n1\n0.8\n0.8\n\n\n\nVới mô hình trên, ta có:\n\\[L = 0.7 * 0.4 * 0.8 = 0.224\\]\n\\[log(L) = log(0.224) = -1.496\\]\nNhư vậy, về mặt hình ảnh, hàm Likelihood sẽ tìm cách fit dữ liệu sao cho sát với dữ liệu thực tế nhiều nhất. Hàm này sẽ đạt giá trị tối đa bằng 1 (L = 1), ứng với \\(log(L_{max}) = 0\\)\nĐể hiểu hơn về cách tối ưu hóa Logistic trong trường hợp đơn giản, xem ví dụ trong file excel đính kèm"
  },
  {
    "objectID": "p03-03-logistic-regression.html#ví-dụ-xây-dựng-mô-hình-đơn-giản",
    "href": "p03-03-logistic-regression.html#ví-dụ-xây-dựng-mô-hình-đơn-giản",
    "title": "20  Mô hình hồi quy logistic",
    "section": "20.3 Ví dụ xây dựng mô hình đơn giản",
    "text": "20.3 Ví dụ xây dựng mô hình đơn giản\n\nlibrary(ISLR)\nlibrary(tidyverse)\ndata <- Default %>% \n  select(-student) %>% \n  mutate(default = case_when(\n    default == \"Yes\" ~ 1,\n    TRUE ~ 0\n  ))\n\n\nlibrary(broom)\nmodel <- glm(default ~ income + balance, data = data, \n    family = \"binomial\") \nmodel %>% summary\n\n\nCall:\nglm(formula = default ~ income + balance, family = \"binomial\", \n    data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4725  -0.1444  -0.0574  -0.0211   3.7245  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.154e+01  4.348e-01 -26.545  < 2e-16 ***\nincome       2.081e-05  4.985e-06   4.174 2.99e-05 ***\nbalance      5.647e-03  2.274e-04  24.836  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1579.0  on 9997  degrees of freedom\nAIC: 1585\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nlibrary(broom)\nmodel %>% \n  tidy %>% \n  filter(term != \"(Intercept)\") %>% \n  select(term, estimate) %>% \n  mutate(`odd_increase (thousands USD)` = exp(1000*estimate))\n\n# A tibble: 2 × 3\n  term     estimate `odd_increase (thousands USD)`\n  <chr>       <dbl>                          <dbl>\n1 income  0.0000208                           1.02\n2 balance 0.00565                           283.  \n\n\nÝ nghĩa:\n\nNếu balance tăng 1000 USD, khả năng khách hàng có nợ xấu tăng 283 lần\nNếu income tăng 1000 USD, khả năng khách hàng có nợ xấu tăng 1.02 lần\n\nKhi ước lượng được các tham số \\(\\beta_j\\), với mỗi KH cụ thể ta có thể ước lượng được XS KH có nợ xấu bằng bao nhiêu dựa vào dư nợ của KH đó. Từ đó, ta có thể phân loại được KH Good/Bad (ko có nợ xấu/có nợ xấu) bằng việc so sánh XS có nợ xấu của KH với giá trị ngưỡng (“cutoff point”).\nVí dụ, XS có nợ xấu của KH A là 0.2, điểm cutoff ta lựa chọn để phân loại là 0.5 -> KH A - “Good”.\nDự báo trên tập mới.\nĐể dự báo trên tập dữ liệu mới, ta có thể dùng hàm predict với option type = \"response\" để trả ra kết quả dưới dạng xác suất xảy ra event (trong ví dụ này là xác suất xảy ra nợ xâu)\n\npredict(model, data %>% head, type = \"response\")\n\n           1            2            3            4            5            6 \n0.0015047280 0.0012619299 0.0080262106 0.0004059957 0.0018267237 0.0020424398"
  },
  {
    "objectID": "p03-03-logistic-regression.html#các-chỉ-số-cần-quan-tâm-khác",
    "href": "p03-03-logistic-regression.html#các-chỉ-số-cần-quan-tâm-khác",
    "title": "20  Mô hình hồi quy logistic",
    "section": "20.4 Các chỉ số cần quan tâm khác",
    "text": "20.4 Các chỉ số cần quan tâm khác\nNull deviance vs. Residual Deviance\n\nLL: Log Likelihood \\[null\\;deviance = 2(LL(saturated\\;model) - LL(null\\;model))\\] \\[residual\\;deviance = 2(LL(saturated\\;model) - LL(proposed\\;model))\\]\nTrong đó:\n\nSaturated Model: Mỗi quan sát có 1 tham số riêng cần phải ước lượng (n tham số)\nNull Model: Tất cả quan sát chỉ có 1 tham số\nProposed Model: Mô hình có p tham số ứng với p biến và 1 tham số ứng với intercept (p+1 tham số)\n\nNếu Null deviance thấp, nghĩa là mô hình có thể giải thích bằng mô hình đơn giản"
  },
  {
    "objectID": "p03-03-logistic-regression.html#đánh-giá-chất-lượng-mô-hình",
    "href": "p03-03-logistic-regression.html#đánh-giá-chất-lượng-mô-hình",
    "title": "20  Mô hình hồi quy logistic",
    "section": "20.5 Đánh giá chất lượng mô hình",
    "text": "20.5 Đánh giá chất lượng mô hình\nKhi xây dựng mô hình dự báo, kết quả dự báo sẽ được trả ra dưới dạng 1 dãy các điểm xác suất xảy ra sự kiện. Khi đó, ta sẽ phải lựa chọn điểm mà tại đó, nếu xác suất dự báo cao hơn điểm này, các quan sát sẽ được đánh dấu là nợ xấu. Xem ví dụ sau.\n\n\n\n\n\n\n\nCustomer ID\nXác suất nợ xấu\n\n\n\n\n1\n0.7\n\n\n2\n0.5\n\n\n3\n0.4\n\n\n4\n0.3\n\n\n\n\nNếu cutoff là 0.6: Khách hàng 1 được dự báo sẽ có nợ xấu\nNếu cutoff là 0.45: Khách hàng 1, 2 được dự báo sẽ có nợ xấu\n\nNhư vậy, với mỗi điểm cutoff khác nhau, ta sẽ có số lượng khách hàng bị nợ xấu khác nhau\nR bình phương\nKhông giống như mô hình OLS, trong mô hình OLS không có \\(R^2\\) để giải thích độ biến động của dữ liệu khi sử dụng mô hình. Tuy nhiên, mô hình logistic có chỉ số pseudo \\(R^2\\) được McFadden’s đưa ra. Chỉ số này có thể được dùng tương tự như \\(R^2\\) trong OLS.\n\\[R^2 = 1−\\frac{ln(LM_1)}{ln(LM_0)}\\]\nTrong đó:\n\n\\(ln(LM_1)\\) là log likelihood của mô hình\n\\(ln(LM_0)\\) là log likelihood của mull model, nghĩa là mô hình chỉ có hệ số tự do intercept.\n\nTrong mô hình logistic, nếu \\(R^2\\) có giá trị trên 0.4 đã được coi là mô hình tương đối tốt.\n\n20.5.1 Confusion Matrix\n\nConfusion Matrix: Bảng mô tả kết quả dự báo vs. thực tế ứng với 1 điểm cut-off xác định trước.\n\n\n\nTính toán các chỉ số:\n\nAccuracy (acc): Tỷ lệ dự báo chính xác trên cả hai class. Acc \\[Accuracy = \\frac{TP+TN}{TP+TN+FN+FP}\\].\nTrue Negative Rate (tnr): Tỷ lệ dự đoán Negative chính xác trong toàn bộ quan sát thực tế là Negative \\[tnr = Specificity = \\frac{TN}{TN + FP}\\]\nFalse Positive Rate (fpr - fall out): Tỷ lệ dự đoán Positive sai, hay tỷ lệ các quan sát thực tế là Negative nhưng lại được dự báo thành Positive \\[fpr = 1- Specificity = \\frac{FP}{TN + FP}\\]\nTrue Positive Rate (tpr - Sensitivity - hay còn gọi là recall): Tỷ lệ dự báo Positive chính xác trên toàn bộ số lượng quan sát thực tế là Positive \\[tpr = Sensitivity = \\frac{TP}{TP + FN}\\]\nFalse Negative Rate (fnr - Sensitivity): Tỷ lệ dự báo negative sai - tỷ lệ positive nhưng dự báo nhầm thành Negative \\[fnr = 1- Sensitivity = \\frac{FN}{FN + TP}\\]\nSupport: Tỷ lệ dự báo Positive trong toàn mẫu \\[sup = \\frac{TP + FP}{N} = \\frac{predicted pos}{total}\\]\nPrecision: Trong tổng số quan sát dự báo là Positive, bao nhiêu quan sát thực sự là Positive \\[Precision = \\frac{TP}{TP+FP}\\]\nRate of positive predictions (RPP): tỷ lệ tổng số quan sát được dự báo là Y=1 (có nợ xấu) trên tổng số quan sát.\n\n\nTrong các chỉ số trên, các chỉ số True Positive Rate, Precision và Accuracy là quan trọng nhất. Để hiểu hơn về các chỉ số trên, giả sử ta có bảng confusion matrix như sau:\n\n\n\n\n\n\n\n\nPredicted/Actual\n1\n0\n\n\n\n\n1\n0.4\n0.1\n\n\n0\n0.2\n0.3\n\n\n\nCác chỉ số trong bảng trên được tính như sau:\n\n\\(accurracy = 0.4 + 0.3 = 0.7\\): Tỷ lệ dự báo chính xác trên cả hai class là 70%. Nói cách khác, trong 100 quan sát, mô hình dự báo chính xác 70%\n\\(True \\; Positive \\; Rate = TPR = \\frac{0.4}{0.4 + 0.2} = 0.67\\) Trong 100 khách hàng thực sự bị nợ xấu, mô hình dự báo chính xác được 67 khách hàng.\n\\(Precision = \\frac{0.4}{0.4 + 0.1} = 0.8\\) Trong 100 khách hàng được dự báo là nợ xấu, có 80 khách hàng thực sự bị nợ xấu\n\\(True \\;Negative \\;Rate = Specificity = \\frac{0.3}{0.3+0.1}=0.75\\) Trong 100 khách hàng tốt, mô hình dự đoán chính xác 75 khách hàng\n\nVới phương pháp sử dụng Confusion Matrix, kết quả dự báo của mô hình trở nên đơn giản, dễ hiểu, dễ giải thích. Tuy nhiên, để có được confusion matrix, ta phải xác định trước giá trị của điểm cut-off. Điều này không hề dễ dàng bởi lẽ mỗi điểm cut-off sẽ cho ta 1 giá trị của confusion matrix hoàn toàn khác nhau. Để khắc phục điểm yếu này, một chỉ số khác thường được sử dụng là đường ROC với chỉ số AUC.\n\n\n20.5.2 ROC\n\nROC Curve: Receiver Operating Characteristic. ROC cho ta đánh giá hiệu quả của mô hình thông qua sự đánh đổi giữa TPR và FPR.\nÝ nghĩa: Mô tả tỷ lệ đánh đổi giữa 2 tỷ lệ: i, Tỷ lệ dự báo chính xác sự kiện A và ii, Tỷ lệ dự báo rằng sự kiện đó không diễn ra\n\nDự báo mang tính ngẫu nhiên (50 - 50), ROC là đường 45 độ\nDự báo hoàn toàn chính xác thì ROC là đường có dạng tam giác\nDự báo kém hơn kỳ vọng thì ROC ở dưới đường 45 độ\nAUC (Area Under Curve): Cho phép so sánh độ chính xác của mô hình. AUC càng cao (ROC ở vị trí càng cao) thì mô hình càng chính xác\n\n\n\n20.5.2.1 Lý thuyết\nĐể xây dựng đường ROC trong, ta có các ký hiệu sau.\n\nN: số lượng quan sát\nNEG: số lượng NEGATIVE, neg: tỷ lệ negative (\\(neg = \\frac{NEG}{N}\\))\nPOS: số lượng POSITIVE, pos: tỷ lệ positive (\\(pos = \\frac{POS}{N}\\))\n\nQuan hệ giữa acc và fpr, tpr:\n\\[acc =  \\frac{TP}{N} \\frac{TN}{N}= \\frac{TP}{POS}\\frac{POS}{N} + \\frac{NEG-FP}{N}\n= \\frac{TP}{POS}\\frac{POS}{N} + \\frac{NEG}{N} - \\frac{FP}{NEG}\\frac{NEG}{N}\\] \\[acc = tpr * pos + neg - neg * fpr\\]\nhay: \\[tpr = \\frac{acc-neg}{pos} + \\frac{neg}{pos}fpr\\] công thức trên giúp ta vẽ được đồ thị đường acc theo tpr và fpr\n\nĐồ thị acc với 1 số trường hợp:\n\n\n\nLưu ý:\n\nMỗi điểm trên ROC thể hiện kết quả của mô hình ứng vơi MỘT điểm cut-off. Do đó khi vẽ đường ROC, máy tính sẽ thay đổi các điểm cutoff từ 0-1, sau đó tính tpr & fpr với từng điểm cutoff. Tập hợp tất cả các điểm này tạo thành ROC\nĐể tìm điểm cutoff tốt nhất, nghia là với mô hình đã đưa ra, chọn cutoff \\(\\alpha\\) sao cho \\(acc\\) đạt max.\nThuật toán:\n\nVẽ đường acc theo tpr và fpr\nTịnh tiến đường acc cho đến khi acc tiếp tuyến với ROC\nTiếp điểm chính là điểm \\(\\alpha\\) cần tìm\n\n\n\n\n\n\n20.5.2.2 Cách vẽ ROC thực tế\nTrên thực tế, ROC được vẽ với thuật toán như sau:\n\nGọi F là hàm tính xác suất của Y trong GLM\n\n\\[ F(X) ~ \\frac{P(X)}{1-P(X)} = e^{\\beta X}\\]\n\nTính F(X) với tập dữ liệu phân tích, sắp xếp F(X) theo giá trị từ cao xuống thấp. Tập hợp này tạo thành tập hợp A của biến score = x\nBắt đầu từ điểm (0,0)\nVới mỗi giá trị của \\(x \\in A\\), thực hiện như sau:\n\nNếu x positive, di chuyển lên trên 1/pos\nNếu x negative, di chuyển sang phải 1/neg\n\n\n\nĐường ROC nằm càng hướng về phía đỉnh góc bên trái thì độ chính xác của dự báo càng cao, điều đó thể hiện true positive rate cao và false positive rate thấp; còn nếu càng tiến tới đường chéo 45 độ so với trục hoành thì độ chính xác của dự báo càng kém. Đường chéo 45 độ so với trục hoành thể hiện sự phân loại là không có ý nghĩa.\nAUC (Area under ROC curve) - diện tích phía dưới đường cong ROC, là thước đo độ chính xác của sự phân loại. AUC càng lớn thì độ chính xác càng cao.\n\nAUC từ 0.5-0.6: Mô hình chỉ tương đương với việc chọn ngẫu nhiên\n0.6-0.7: Chất lượng mô hình kém\n0.7-0.8: Tương đối tốt\n0.8-0.9: Tốt\ntrên 0.9: Rất tốt\n\nNói cách khác, chỉ số AUC cho phép ta biết được chất lượng phân loại của mô hình. AUC càng cao, cho ta thấy các quan sát xảy ra positive class (nợ xấu), càng có điểm score cao (tương đối) và ngược lại.\n\n\n\n20.5.3 Gain và Lift\nGain và Lift là hai biểu đồ vô cùng quan trọng với các nhà phân tích dữ liệu thực tiễn nhưng lại ít được các nhà lý thuyết sử dụng. Hai đường này đặc biệt quan trọng khi xây dựng các chiến lược marketing.\nĐường gain là tổ hợp giữa TPR (True Positive Rate) và RPP (Rate of Positive Prediction). Vị trí các điểm trên đường gain cho ta biết nên chọn điểm cut-off như thế nào.\n\nlibrary(scales)\ndf <- data.frame(tpr = c(0, 0.4, 0.7, 0.8, 0.85, 0.88, 0.93, 0.94, 0.97, 0.99, 1),\n                 rpp = c(0,seq(0.1,1, by = 0.1))) %>% \n  mutate(lift = tpr/rpp) %>% \n  mutate(lift = replace_na(lift, 0))\n\ndf %>% \n  ggplot(aes(rpp, tpr)) +\n  geom_line(col = \"darkred\", alpha = 0.7) +\n  geom_point(col = \"darkred\", size = 2) +\n  geom_text(aes(label = tpr), vjust = -0.5) +\n  theme_classic() +\n  scale_x_continuous(breaks = seq(0.1, 1, by = 0.1)) +\n  geom_line(aes(rpp, rpp), col = \"black\", alpha = 1, linetype = \"dashed\") +\n  annotate(geom = \"text\", x = 0.5, y = 0.45, label = \"Base line (No model)\", hjust = 0) +\n  geom_vline(xintercept = seq(0.1, 1, by = 0.1), linetype = \"dashed\", col = \"darkgrey\", alpha = 0.3) +\n  labs(x = \"Rate of Positive Prediction\",\n       y = \"True Positive Rate\",\n       title = \"Gain chart\") \n\n\n\n\nGiải thích:\n\nVới top 20% khách hàng có điểm score cao nhất sẽ phủ được 70% khách hàng có nợ xấu trong thực tế.\nVới top 30% khách hàng có điểm score cao nhất sẽ phủ được 80% khách hàng có nợ xấu trong thực tế.\n\nThông thường, đường gain được chia thành các decile (10%) để marketing/risk manager quyết định điểm cut-off trong mô hình. Nếu số lượng khách hàng ít, 1000 khách hàng, đôi khi ta có thể quyết định gửi email marketing cho cả 1000 khách hàng này.\nTrong khi đó, đường lift thể hiện khía cạnh khác trong việc xây dựng mô hình. Đường lift cho ta biết, ứng với từng decile, chất lượng mô hình cao hơn bao nhiêu lần so với việc không sử dụng mô hình (ngẫu nhiên). Đường lift luôn giảm về 1 khi ta dự báo 100% tất cả quan sát thuộc về nhóm positive.\n\ndf %>% \n  filter(lift > 0) %>% \n  ggplot(aes(rpp, lift)) +\n  geom_line() +\n  geom_point() +\n  theme_classic() +\n  geom_line(col = \"darkred\", alpha = 0.7) +\n  geom_point(col = \"darkred\", size = 2) +\n  geom_text(aes(label = round(lift, 1)), vjust = -0.5, hjust = 0) +\n  theme_classic() +\n  scale_x_continuous(breaks = seq(0.1, 1, by = 0.1)) +\n  annotate(geom = \"text\", x = 0.1, y = 1.1, label = \"Base line (No model)\", hjust = 0) +\n  geom_vline(xintercept = seq(0.1, 1, by = 0.1), linetype = \"dashed\", col = \"darkgrey\", alpha = 0.3) +\n  geom_hline(yintercept = 1, linetype = \"dashed\", col = \"darkblue\", alpha = 1, size = 0.7) +\n  labs(x = \"Rate of Positive Prediction\",\n       y = \"Lift ratio\",\n       title = \"Lift chart\") \n\n\n\n\nÝ nghĩa: Trong đồ thị trên, với top 10% khách hàng có điểm score mua hàng cao nhất, tỷ lệ khách hàng mua hàng cao hơn 4 lần so với việc không sử dụng mô hình (hay ngẫu nhiên)\n\n\n20.5.4 Kolmogorov–Smirnov statistics\nChỉ số thống kê Kolmogorov-Smirnov là chỉ số khác để đo lường chất lượng mô hình. Chỉ số này cho ta biết được liệu phân phối của của nhóm positive có thực sự khác nhóm negative hay không.\nChỉ số KS được đo lường bằng khoảng cách lớn nhất giữa hai đường phân phối lũy kế xác suất trên hai class. Khi xây dựng mô hình, KS có thể được tính theo công thức sau.\n\\[KS = max(abs(tpr - fpr))\\]\n\npos_class <- c(0.8, 0.75, 0.7, 0.55, 0.4)\npos_class\n\n[1] 0.80 0.75 0.70 0.55 0.40\n\nneg_class <- c(0.7, 0.4, 0.3, 0.2, 0.15)\nneg_class\n\n[1] 0.70 0.40 0.30 0.20 0.15\n\nks.test(pos_class, neg_class, alternative = \"greater\")\n\n\n    Exact two-sample Kolmogorov-Smirnov test\n\ndata:  pos_class and neg_class\nD^+ = 0, p-value = 1\nalternative hypothesis: the CDF of x lies above that of y\n\n\n\ndata.frame(pos_class, neg_class) %>% \n  ggplot() +\n  stat_ecdf(aes(pos_class), col = \"darkred\", geom = \"line\") +\n  stat_ecdf(aes(neg_class), col = \"darkblue\", geom = \"line\") +\n  annotate(geom = \"text\", x = 0.5, y = 0.5, label = \"KS = 0.6\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nresult$ksTable\n\n  dec total responders nonresponders cumResponders cumNonresponders\n1   1     1          1             0             1                0\n2   2     1          1             0             2                0\n3   4     2          1             1             3                1\n4   5     1          1             0             4                1\n5   7     2          1             1             5                2\n6   8     1          0             1             5                3\n7   9     1          0             1             5                4\n8  10     1          0             1             5                5\n  perCumResponders perCumNonresponders split\n1               20                   0    20\n2               40                   0    40\n3               60                  20    40\n4               80                  20    60\n5              100                  40    60\n6              100                  60    40\n7              100                  80    20\n8              100                 100     0\n\nresult$ks %>% print\n\n[1] 60\n\n\n\nTrong bốn nhóm chỉ số đánh giá chất lượng mô hình đã trình bày ở trên, nhóm chỉ số AUC, Gain và Lift là quan trọng nhất khi dự báo trong thực tế, đặc biệt là khi score khách hàng trong chiến dịch marketing, bán chéo, giảm thiểu khách hàng churn. Khi xây dựng các mô hình dự báo nợ xấu, phê duyệt tín dụng, ta cần sử dụng cả 4 nhóm chỉ số trên để đánh giá hiệu quả mô hình. Đặc biệt, nếu xây dựng mô hình score-card, ta còn phải sử dụng 1 số kỹ thuật khác để có thể xây dựng hệ thống score-card hiệu quả. Chi tiết phần score card sẽ được trình bày trong chương tiếp theo."
  },
  {
    "objectID": "p03-03-logistic-regression.html#thực-hành",
    "href": "p03-03-logistic-regression.html#thực-hành",
    "title": "20  Mô hình hồi quy logistic",
    "section": "20.6 Thực hành",
    "text": "20.6 Thực hành\n\n20.6.1 Dữ liệu\nDùng gói dữ liệu Default có sẵn trong R, bao gồm thông tin của 10,000 khách hàng sử dụng thẻ tín dụng.\nGiải thích các biến:\n\nDefault: Yes/No (khách hàng có nợ xấu/không có nợ xấu)\nStudent: Yes/No (khách hàng là sinh viên/không phải là sinh viên)\nBalance: Dư nợ TB còn lại của KH sau khi trả tiền hàng tháng.\nIncome: Thu nhập của khách hàng\n\n\ndata <- Default\ndata %>% head\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\n\n\n\n20.6.2 Xây dựng mô hình trên tập dữ liệu gốc\n\nglm.fit1 <- glm(default ~ ., \n                data = data, \n                family = binomial)\nglm.fit1 %>% summary\n\n\nCall:\nglm(formula = default ~ ., family = binomial, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4691  -0.1418  -0.0557  -0.0203   3.7383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.087e+01  4.923e-01 -22.080  < 2e-16 ***\nstudentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** \nbalance      5.737e-03  2.319e-04  24.738  < 2e-16 ***\nincome       3.033e-06  8.203e-06   0.370  0.71152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1579.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nglm.fit2 <- step(glm.fit1)\n\nStart:  AIC=1579.54\ndefault ~ student + balance + income\n\n          Df Deviance    AIC\n- income   1   1571.7 1577.7\n<none>         1571.5 1579.5\n- student  1   1579.0 1585.0\n- balance  1   2907.5 2913.5\n\nStep:  AIC=1577.68\ndefault ~ student + balance\n\n          Df Deviance    AIC\n<none>         1571.7 1577.7\n- student  1   1596.5 1600.5\n- balance  1   2908.7 2912.7\n\nglm.fit2 %>% summary\n\n\nCall:\nglm(formula = default ~ student + balance, family = binomial, \n    data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4578  -0.1422  -0.0559  -0.0203   3.7435  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.075e+01  3.692e-01 -29.116  < 2e-16 ***\nstudentYes  -7.149e-01  1.475e-01  -4.846 1.26e-06 ***\nbalance      5.738e-03  2.318e-04  24.750  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.7  on 9997  degrees of freedom\nAIC: 1577.7\n\nNumber of Fisher Scoring iterations: 8\n\n\nGiải thích:\n\nBeta(balance) > 0: Balance và XS có nợ xấu của KH có quan hệ cùng chiều (cái này tăng thì cái kia tăng).\n\nBeta(balance) = 0.0057: Balance tăng 1 đơn vị -> Ln(Odds) tăng 0.0057 đơn vị.\n\nBeta(studentYes) < 0: KH là sinh viên có XS có nợ xấu thấp hơn so với KH ko là sinh viên.\n\nGiả sử KH B có balance = 1000$:\n\\[Pr(default=Yes|student=Yes)=\\frac{e^{-10.75-0.7149*1+0.0057*1000}}{1+e^{-10.75-0.7149*1+0.0057*1000}}= 0.003\\] \\[Pr(default=Yes|student=No)=\\frac{e^{-10.75-0.7149*0+0.0057*1000}}{1+e^{-10.75-0.7149*0+0.0057*1000}}= 0.006\\]\n\n\n20.6.3 Xây dựng mô hình trên hai tập train và test\n\n20.6.3.1 Chia tập train/test\n\n#chia tập train/test\nset.seed(1)\ntrain <- sample(nrow(data), 0.7 * nrow(data))  \ndf.train <- data[train,]            # tập train\ndf.test <- data[-train,]        # tập test\n\ndf.train %>% summary\n\n default    student       balance           income     \n No :6769   No :4925   Min.   :   0.0   Min.   : 1498  \n Yes: 231   Yes:2075   1st Qu.: 477.9   1st Qu.:21151  \n                       Median : 819.4   Median :34428  \n                       Mean   : 836.6   Mean   :33387  \n                       3rd Qu.:1167.9   3rd Qu.:43780  \n                       Max.   :2654.3   Max.   :71239  \n\ndf.test %>% summary\n\n default    student       balance           income     \n No :2898   No :2131   Min.   :   0.0   Min.   :  772  \n Yes: 102   Yes: 869   1st Qu.: 487.0   1st Qu.:21743  \n                       Median : 826.3   Median :34849  \n                       Mean   : 832.5   Mean   :33820  \n                       3rd Qu.:1161.5   3rd Qu.:43878  \n                       Max.   :2321.9   Max.   :73554  \n\n\n\n\n20.6.3.2 Xây dựng mô hình\n\nfit.logit <- glm(default~., data = df.train, family = binomial)\n\n#giảm biến ko có ý nghĩa thống kê\nlogit.fit.reduced <- step(fit.logit)\n\nStart:  AIC=1081.54\ndefault ~ student + balance + income\n\n          Df Deviance    AIC\n- income   1   1073.9 1079.9\n<none>         1073.5 1081.5\n- student  1   1079.7 1085.7\n- balance  1   2022.6 2028.6\n\nStep:  AIC=1079.94\ndefault ~ student + balance\n\n          Df Deviance    AIC\n<none>         1073.9 1079.9\n- student  1   1098.0 1102.0\n- balance  1   2025.9 2029.9\n\nlogit.fit.reduced %>% summary\n\n\nCall:\nglm(formula = default ~ student + balance, family = binomial, \n    data = df.train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4261  -0.1377  -0.0542  -0.0203   3.4070  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.070e+01  4.395e-01 -24.349  < 2e-16 ***\nstudentYes  -8.591e-01  1.815e-01  -4.733 2.21e-06 ***\nbalance      5.684e-03  2.739e-04  20.748  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2030.3  on 6999  degrees of freedom\nResidual deviance: 1073.9  on 6997  degrees of freedom\nAIC: 1079.9\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n20.6.3.3 Dự báo xác suất có nợ xấu trên tập test\n\nprob.test <- predict(logit.fit.reduced, df.test, type = \"response\")\n\n#phân loại (chọn cutoff = 0.5)\nclass.pred.test <- factor(prob.test > 0.5, \n                          levels = c(FALSE, TRUE), \n                          labels = c(\"No\", \"Yes\"))\n\n#kết quả trên tập test\nprob.test.df <- prob.test %>% as.data.frame()\nnames(prob.test.df)[1] <- \"prob.default\"\n\n#Lụa chọn điểm cutoff là 0.5\nprob.test.df <- prob.test.df %>% \n  mutate(class = as.factor(ifelse(prob.default > 0.5, \"Yes\", \"No\")))\nprob.test.df$student <- df.test$student\nprob.test.df$balance <- df.test$balance\nprob.test.df$income <- df.test$income\n\nprob.test.df %>% \n  select(3:5,1,2) %>% \n  sample_n(10)\n\n     student  balance   income prob.default class\n6476     Yes 528.5857 19621.63 1.925160e-04    No\n971       No   0.0000 43346.62 2.253954e-05    No\n6027     Yes 969.3972 17686.56 2.352766e-03    No\n1615      No 196.3740 57397.28 6.880709e-05    No\n651       No 133.7433 30457.02 4.820048e-05    No\n8235      No 657.0888 31724.63 9.428268e-04    No\n3265     Yes 687.0644 14059.46 4.737141e-04    No\n2720      No 339.5293 58747.39 1.552189e-04    No\n5721     Yes 645.3388 20122.20 3.737394e-04    No\n3063     Yes 969.1435 22425.41 2.349384e-03    No\n\n\n\n\n20.6.3.4 Đánh giá chất lượng dự báo trên tập test\n\n20.6.3.4.1 Confusion matrix\n\n#conf.matrix\nlogit.perf <- table(df.test$default, class.pred.test)\nlogit.perf\n\n     class.pred.test\n        No  Yes\n  No  2896    2\n  Yes   80   22\n\n###performance\n#function to perform model\nmodel.performance <- function(confusion_matrix) {\n  a <- confusion_matrix[1,1]\n  b <- confusion_matrix[1,2]\n  c <- confusion_matrix[2,2]\n  d <- confusion_matrix[2,1]\n  \n  recall <- c/(c+d)\n  precision <- c/(b+c)\n  accuracy <- (a+c)/(a+b+c+d)\n  \n  print(paste('recall :',round(recall,2)))\n  print(paste('precision :',round(precision,2)))\n  print(paste('accuracy :',round(accuracy,2)))\n}\n\nmodel.performance(logit.perf)\n\n[1] \"recall : 0.22\"\n[1] \"precision : 0.92\"\n[1] \"accuracy : 0.97\"\n\n\n\n\n20.6.3.4.2 ROC & AUC\nTất cả các chỉ số về chất lượng mô hình đều có thể sử dụng với package ROCR thông qua hai bước\n\nBước 1: Tạo object dự báo pred <- prediction(prob_value, actual_value). Object này sẽ chứa 1 loạt các chỉ số của confusion matrix với từng điểm cut-off\nBước 2: Tính toán các chỉ số với hàm performance với giá trị trục x và y mong muốn\n\nLưu ý: Object pred được lưu dưới dạng S4 class. Để chiết xuất dữ liệu cần dùng toán tử @ thay cho $ như S3\n\n# Bước 1: Tạo object pred\npred <- prediction(prob.test, df.test$default)\npred %>% str\n\nFormal class 'prediction' [package \"ROCR\"] with 11 slots\n  ..@ predictions:List of 1\n  .. ..$ : Named num [1:3000] 1.42e-03 2.25e-05 9.55e-06 9.74e-03 8.67e-05 ...\n  .. .. ..- attr(*, \"names\")= chr [1:3000] \"1\" \"10\" \"11\" \"12\" ...\n  ..@ labels     :List of 1\n  .. ..$ : Ord.factor w/ 2 levels \"No\"<\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n  ..@ cutoffs    :List of 1\n  .. ..$ : Named num [1:2851] Inf 0.837 0.76 0.755 0.749 ...\n  .. .. ..- attr(*, \"names\")= chr [1:2851] \"\" \"3856\" \"2890\" \"3377\" ...\n  ..@ fp         :List of 1\n  .. ..$ : num [1:2851] 0 0 0 0 0 0 0 0 0 0 ...\n  ..@ tp         :List of 1\n  .. ..$ : num [1:2851] 0 1 2 3 4 5 6 7 8 9 ...\n  ..@ tn         :List of 1\n  .. ..$ : num [1:2851] 2898 2898 2898 2898 2898 ...\n  ..@ fn         :List of 1\n  .. ..$ : num [1:2851] 102 101 100 99 98 97 96 95 94 93 ...\n  ..@ n.pos      :List of 1\n  .. ..$ : int 102\n  ..@ n.neg      :List of 1\n  .. ..$ : int 2898\n  ..@ n.pos.pred :List of 1\n  .. ..$ : num [1:2851] 0 1 2 3 4 5 6 7 8 9 ...\n  ..@ n.neg.pred :List of 1\n  .. ..$ : num [1:2851] 3000 2999 2998 2997 2996 ...\n\n# Bước 2: Tạo object tính toán prediction cho AUC\nroc <- performance(pred, x.measure = \"fpr\", measure = \"tpr\")\n\n# Xây dựng hàm để convert giá trị sang data.frame\nget_df <- function(pred){\n  pred_new <- data.frame(x = pred@x.values %>% unlist,\n                         y = pred@y.values %>% unlist)\n  return(pred_new)\n}\n\nget_df(roc) %>% \n  ggplot(aes(x, y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"FPR\",\n       y = \"TPR\",\n       title = \"ROC curve\")\n\n\n\n# Tính chỉ số AUC\nauc_test <- performance(pred, \"auc\", \"cutoff\")\nauc_test@y.values %>% unlist\n\n[1] 0.9379389\n\n\n\n\n20.6.3.4.3 Gain và Lift\n\n# GAIN charts on testing set\ngain_df <- performance(pred, \"tpr\", \"rpp\")\n\nget_df(gain_df) %>% \n  ggplot(aes(x, y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Gain chart\",\n       x = \"RPP\",\n       y = \"TPR\")\n\n\n\n\n\n# LIFT charts on testing set\nlift_df <- performance(pred, \"lift\", \"rpp\")\nget_df(lift_df) %>% \n  ggplot(aes(x, y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Lift chart\",\n       x = \"RPP\",\n       y = \"Lift\")\n\n\n\n\n\n\n20.6.3.4.4 KS statistics\n\nlift_df <- performance(pred, \"tpr\", \"fpr\")\nlift_df %>% \n  get_df %>% \n  mutate(ks = abs(x - y)) %>% \n  pull(ks) %>% max\n\n[1] 0.7432915\n\ndata.frame(y = (df.test$default %>% as.numeric) - 1, \n           prob = prob.test) %>% \n  ggplot(aes(prob, col = as.factor(y))) +\n  stat_ecdf() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(tittle = \"CDF & KS statistics\")"
  },
  {
    "objectID": "p03-03-logistic-regression.html#xây-dựng-mô-hình-logistics-theo-machine-learning",
    "href": "p03-03-logistic-regression.html#xây-dựng-mô-hình-logistics-theo-machine-learning",
    "title": "20  Mô hình hồi quy logistic",
    "section": "20.7 Xây dựng mô hình Logistics theo Machine Learning",
    "text": "20.7 Xây dựng mô hình Logistics theo Machine Learning\nCost function: Đối với mô hình logistic, nếu sử dụng cost function tương tự như như hồi quy tuyến tính, chúng ta sẽ gặp vấn đề về bài toán tối ưu. Hàm tối ưu sẽ không thể tìm được điểm nhỏ nhất do tồn tại vô số các điểm “local optimization”. Do đó, ta phải xây dựng một hàm tối ưu mới cho bài toán phân loại của logistic như sau.\n\\[Cost(h_\\theta(x), y)\\cases{\n-log(h_\\theta(x) \\text{ với } y = 1 \\\\\n-log(1-h_\\theta(x)) \\text{ với } y = 0)\n}\\]\nVới hàm tối ưu trên, có 1 số điểm lưu ý như sau.\n\nNếu y = 1 và \\(h_\\theta = 1\\) nghĩa là mô hình dự báo chính xác với trường hợp xảy ra sự kiện, cost = 0\nTuy nhiên, nếu y = 1 và \\(h_\\theta \\rightarrow 0\\), nghĩa là mô hình dự báo không xảy ra sự kiện (y = 0) trong khi thực tế xảy ra, \\(cost \\rightarrow \\infty\\).\n\nTrong thực tế, hàm tối ưu được đơn giản hóa như sau.\n\\[Cost(h_\\theta, y) = -y log(h_\\theta(x)) - (1-y)log(1-h_\\theta(y))\\]\nVới từng giá trị y = 0 hoặc y = 1, hàm tối ưu quay trở lại như bình thường.\nĐối với hàm tối ưu như trên, thuật toán gradient descent quay trở lại tương tự như hàm hồi quy tuyến tính."
  },
  {
    "objectID": "p03-03-logistic-regression.html#tài-liệu-tham-khảo",
    "href": "p03-03-logistic-regression.html#tài-liệu-tham-khảo",
    "title": "20  Mô hình hồi quy logistic",
    "section": "20.8 Tài liệu tham khảo",
    "text": "20.8 Tài liệu tham khảo\n\nChapter 4 - Classification - Introduction to Statistical Learning with R"
  },
  {
    "objectID": "p03-04-decision-tree.html#giới-thiệu",
    "href": "p03-04-decision-tree.html#giới-thiệu",
    "title": "21  Cây quyết định",
    "section": "21.1 Giới thiệu",
    "text": "21.1 Giới thiệu\nPhương pháp decision trees (cây quyết định) gồm tập hợp các nguyên tắc phân nhóm (spliting rules) được sử dụng để nhóm các quan sát vào một số vùng (regions) nhất định mà được thống kê trong một cây.\nPhương pháp decision trees có thể áp dụng đối với cả hai bài toán hồi quy (regression) và phân loại (classification)."
  },
  {
    "objectID": "p03-04-decision-tree.html#regression-trees",
    "href": "p03-04-decision-tree.html#regression-trees",
    "title": "21  Cây quyết định",
    "section": "21.2 Regression Trees",
    "text": "21.2 Regression Trees\nPhương pháp cây quyết định hồi quy (regression trees) dùng cho trường hợp khi biến đầu ra chúng ta muốn dự báo là biến liên tục hay biến định lượng\nChúng ta sẽ sử dụng dữ liệu có sẵn trong R - Hitters trong packge ISLR để dự báo thu nhập của cầu thủ bóng chày (salary) dựa vào số năm chơi bóng tại các giải đấu lớn (years) và số điểm ghi được trong mùa giải trước (hits).\n\nlibrary(ISLR)\nlibrary(tidyverse)\n\ndata(\"Hitters\")\nnames(Hitters) <- names(Hitters) %>% tolower\n\n# Lấy 3 biến: `years`, `hits`, `salary`\ndata <- Hitters %>% \n  select(years,hits,salary)\n\nĐầu tiên chúng ta sẽ loại bỏ những quan sát bị missing ở biến salary, và lấy log của biến salary để biến này tiệm cận với phân phối chuẩn hơn (phân phối hình chuông). Biến salary đơn vị là nghìn USD.\n\ndata_new <- data %>%\n  # Loại bỏ giá trị missing ở biến `salary`\n  filter(!is.na(salary)) %>% \n  # log-transform\n  mutate(salary = log(salary))\n\n# Biểu đồ phân phối biến `salary` của `data`\ndata %>% \n  ggplot(aes(salary))+\n  geom_density()+\n  theme_minimal()\n\n\n\n# Biểu đồ phân phối biến `salary` của `data_new`\ndata_new %>% \n  ggplot(aes(salary))+\n  geom_density()+\n  theme_minimal()\n\n\n\n\nGiả sử chúng ta xây dựng một cây hồi quy (regression tree) với dữ liệu nói trên và thu được kết quả như sau:\n\n\n\n\n\nVới tập dữ liệu này, trung bình log-transform salary là 5.9, tức thu nhâp trung bình của các cầu thủ bóng chày là exp(5.9) = 375 (nghìn USD).\nCây quyết định trên bao gồm tập hợp các nguyên tắc phân nhóm (spliting rules), bắt đầu từ trên xuống dưới.\nVới nguyên tắc phân nhóm đầu tiên Years < 4.5 thì ở nhánh bên trái là những quan sát thỏa mãn điều kiện trên, và trung bình log-transform salary của nhóm này là 5.1. Nghĩa là, nhóm cầu thủ có số năm chơi bóng tại các giải đấu lớn ít hơn 4.5 năm có mức thu nhập trung bình là exp(5.1) = 164 (nghìn USD).\nCòn đối với những cầu thủ mà chơi bóng từ 4.5 năm trở lên tại các giải đấu lớn thì có thu nhập trung bình là exp(6.4) = 601 (nghìn USD). Tuy nhiên nhóm này còn chia thành 2 nhóm nhỏ hơn với nguyên tắc phân nhóm Hits < 118.\n\nNhóm thỏa mãn điều kiện Hits < 118 (số điểm ghi được trong mùa giải trước ít hơn 118) có mức thu nhập trung bình là exp(6) = 403 (nghìn USD)\nNhóm ghi được từ 118 điểm trở lên tại mùa giải trước có mức thu nhập trung bình là exp(6.7) = 812 (nghìn USD).\n\nNhư vậy, cây quyết định nói trên đã phân loại các cầu thủ và 3 vùng/nhóm (regions of predictor space):\n\nR1: years < 4.5\nR2: years >= 4.5, hits < 118\nR3: years >= 4.5, hits >= 118\n\n\ndata_new %>% \n  ggplot(aes(years,hits)) +\n  geom_point(col = \"gold\")+\n  theme_bw()+\n  geom_vline(aes(xintercept = 4.5), col = \"blue\", size = 1)+\n  geom_segment(aes(x = 4.5, \n                   y = 118, \n                   xend = Inf, \n                   yend = 118\n                   ), \n               col = \"blue\",\n               size = 1)+\n  theme(panel.grid = element_blank())+\n  scale_x_continuous(breaks = 4.5)+\n  scale_y_continuous(breaks = 118)+\n  theme(axis.text = element_text(face = \"bold\", size = 10))+\n  annotate(\"text\", x = 4.5/2, y = 118, label = \"R1\", col = \"red\", size = 6)+\n  annotate(\"text\", x = (32-4.5)/2, y = 118/2, label = \"R2\", col = \"red\", size = 6)+\n  annotate(\"text\", x = (32-4.5)/2, y = 118*1.5, label = \"R3\", col = \"red\", size = 6)\n\n\n\n\nMức thu nhập dự báo của 3 nhóm này lần lượt là 164,000 USD, 403,000 USD, 812,000 USD.\nTrong ví dụ này R1, R2, R3 ở đây được gọi là terminal nodes hoặc leaves (lá cây). years < 4.5 và hits < 118 được gọi là internal nodes hoặc decision nodes.\nChúng ta có thể diễn giải cây quyết định hồi quy nói trên như sau: Biến years là quan trọng nhất ảnh hưởng tới việc dự báo salary. Nhóm cầu thủ có số năm chơi bóng tại các giải đấu lớn nhiều hơn sẽ có mức thu nhập cao hơn. Nhóm cầu thủ có số năm chơi bóng dưới 4.5 năm thì số điểm họ ghi được trong mùa giải trước đấy không ảnh hưởng đến thu nhập của họ, trong khi đó nhóm cầu thủ có số năm chơi bóng từ 4.5 năm trở lên tại các giải đấu lớn thì ngược lại. Số điểm ghi được trong mùa giải trước đó của nhóm cầu thủ này có ảnh hưởng đến thu nhập của họ, nhóm cầu thủ ghi được từ 118 điểm trở lên có mức thu nhập cao hơn nhóm còn lại.\nBây giờ, chúng ta sẽ cùng tìm hiểu quá trình xây dựng cây quyết định hồi quy (regression tree). Bao gồm 2 bước sau:\n\nBước 1: Chia các quan sát (tức tìm tập hợp những giá trị phù hợp cho các biến X1, X2,…, Xp) vào j vùng/nhóm khác nhau (non-overlapping) R1, R2,…, Rj.\nBước 2: Với mỗi quan sát mà thuộc về vùng/nhóm Rj, chúng ta sẽ dự báo chúng cùng một giá trị, bằng với trung bình các giá trị quan sát ở tập dữ liệu training trong vùng/nhóm Rj.\n\nNhư ví dụ về dự báo thu nhập của cầu thủ bóng chày nói trên, ở bước 1 chúng ta chia các cầu thủ thành 3 nhóm: R1, R2, R3. Thu nhập trung bình của 3 nhóm trên lần lượt là 164,000 USD, 403,000 USD, 812,000 USD. Vậy nếu một cầu thủ bất kỳ mà thuộc về R1, thì chúng ta sẽ dự báo thu nhập của cầu thủ này là 164,000 USD, nếu thuộc về R2, thì dự báo là 403,000 USD, và nếu thuộc R3 - 812,000 USD.\nChúc ta sẽ cùng tìm hiểu sâu hơn về bước 1 nói trên. Làm thế nào để xây dựng được các vùng/nhóm R1,.., Rj? Về lý thuyết, các vùng này có thể có hình dạng bất kỳ. Tuy nhiên, chúng ta sẽ chia các quan sát (predictor space) thành boxes (các hộp) để cho đơn giản và dễ giải thích kết quả dự báo của mô hình. Mục đích là đi tìm boxes R1,…, Rj sao cho tối thiểu hóa RSS (Residual Sum of Squares):\n\\[\\sum_{j=1}^{J}\\sum_{i \\epsilon R_j}^{J}(y_i - \\widehat{y}_{R_j})^2\\]\nTrong đó yRj là trung bình kết quả của những quan sát trên tập dữ liệu training trong box thứ j.\nTuy nhiên việc tính toán biểu thức trên sẽ rất phức tạp, do vậy chúng ta sẽ dùng cách tiếp cận top-down, greedy hay còn được gọi là recursive binary splitting. Sở dĩ cách tiếp cận trên là top-down vì nó bắt đầu từ phần đỉnh của cây (nơi mà tất cả các quan sát thuộc một nhóm ban đầu), sau đó sẽ phân nhóm các quan sát, mỗi sự phân nhóm sẽ chia làm 2 nhánh (branches) mới xuống phía dưới. Còn việc nói cách tiếp cận trên là greedy vì tại mỗi bước trong quá trình xây dựng cây, sự phân nhóm tốt nhất (best split) sẽ được sử dụng.\nĐể có thể thực hiện cách tiếp cận recursive binary splitting nêu trên, chúng ta đầu tiên sẽ lấy những biến Xj và cutpoint s để chia các quan sát vào các vùng/nhóm {X|Xj < s} và {X|Xj >= s} sao cho tối thiểu hóa RSS.\n{X|Xj < s} ở đây được hiểu là vùng bao gồm các quan sát mà thỏa mãn điều kiện Xj < s.\nNghĩa là, chúng ta sẽ xem xét tất cả các biến X1,…, Xp và tất cả các cutpoint s cho mỗi biến, rồi sau đó sẽ lựa chọn những biến và cutpoint để sao cho cây quyết định cuối cùng có RSS nhỏ nhất. Nói một cách tổng quát hơn, với mọi giá trị j và s, chúng ta xác định cặp nửa mặt phẳng sau:\n\\[R_1(j,s) = \\lbrace X|X_j < s \\rbrace , R_2(j,s) = \\lbrace X|X_j >= s \\rbrace\\]\nVà chúng ta sẽ đi tìm giá trị j và s để tối thiểu hóa biểu thức sau:\n\\[\\sum_{i: x_i \\epsilon R_1(j,s)}^{J}(y_i - \\widehat{y}_{R_1})^2 + \\sum_{i: x_i \\epsilon R_2(j,s)}^{J}(y_i - \\widehat{y}_{R_2})^2\\]\nTrong đó:\n\nyR1 - trung bình kết quả các quan sát trên tập dữ liệu training trong R1(j,s)\nyR2 - trung bình kết quả các quan sát trên tập dữ liệu training trong R2(j,s)\n\nViệc tìm j và s sẽ khá nhanh, đặc biệt đối với trường hợp khi số lượng biến p ít.\nTiếp theo đó, chúng ta sẽ lặp lại quá trình nói trên, lựa chọn biến tốt nhất và cutpoint tốt nhất để tiếp tục split dữ liệu sao để tối thiểu hóa RSS trong mỗi vùng/nhóm kết quả. Tuy nhiên, lần này thay vì việc chúng ta split toàn bộ các quan sát, chúng ta chỉ split 1 trong 2 vùng/nhóm đã được xác định trước đó. Như vậy, bây giờ chúng ta có 3 vùng/nhóm. Cứ tiếp tục như vậy, chúng ta lại split 1 trong 3 vùng/nhóm này để tối thiểu hóa RSS. Quá trình cứ tiếp tục diễn ra cho đến khi nó dừng lại theo tiêu chí đặt ra của chúng ta, ví dụ chúng ta đặt điều kiện là sẽ tiếp tục quá trình cho đến khi không có vùng/nhóm nào bao gồm nhiều hơn 10 quan sát.\nNhư vậy, các vùng R1, R2,…, Rj được tạo ra, chúng ta dự báo kết quả của các quan sát mới bằng việc sử dụng giá trị trung bình kết quả các quan sát trên tập dữ liệu training ở vùng/nhóm mà quan sát mới đó thuộc về.\nTree Pruning\nQuá trình mô tả bên trên có thể dự báo tương đối chính xác trên tập dữ liệu training, nhưng có thể thiếu chính xác trên tập dữ liệu testing, vấn đề này được gọi là overfitting. Đó là bởi vì cây quyết định được xây dựng quá phức tạp (nhiều splits). Cây quyết định nhỏ hơn với ít splits hơn (ít vùng/nhóm hơn) có thể dẫn đến việc variance thấp hơn và tính giải thích cao hơn. Để khắc phục vấn đề overfitting, chúng ta có thể dùng kỹ thuật “tỉa” cây (pruning).\nCách tiếp cận tốt nhất là chúng ta sẽ xây dựng một cây lớn To, sau đó sẽ “tỉa” (prune) cây để thành subtree (cây con). Nhưng làm thế nào để xác định được cách “tỉa” cây tốt nhất? Theo trực giác, chúng ta sẽ chọn subtree mà có tỷ lệ sai số thấp nhất trên tập dữ liệu mới (test error rate). Khi subtree được xác định, chúng ta có thể ước lượng sai số trên tập dữ liệu mới của subtree đó bằng việc sử dụng cross-validation hoặc dữ liệu mới (validation set). Tuy nhiên việc ước lượng cross-validation error cho từng subtree có thể có sẽ rất phức tạp và tốn nhiều công sức, vì số lượng subtree có thể sẽ rất nhiều. Thay vì việc đó, chúng ta cân nhắc việc lựa chọn một tập hợp nhỏ các subtree để xem xét.\nCost complexity pruning, còn gọi là weakest link pruning là một cách để chúng ta thực hiện việc nói trên. Thay vì việc xem xét tất cả các subtree có thể, chúng ta có thể xem xét một chuỗi các cây được indexed bởi một tham số không âm α.\nThuật toán xây dựng cây quyết định hồi quy (regression tree):\n\nBước 1: Sử dụng recursive binary splitting để xây dựng một cây lớn từ tập dữ liệu training, và chỉ dừng lại khi mỗi terminal node có ít hơn một số lượng quan sát tối thiểu nhất định.\nBước 2: Áp dụng cost complexity pruning để có được một chuỗi các subtree tốt nhất, as a function of α.\nBước 3: Sử dụng K-fold cross-validation để lựa chọn α. Tức là, chia các quan sát của tập dữ liệu training vào K-fold. Với k = 1,…, K:\n\nLập lại bước 1 và 2 nhưng với k-th fold của tập dữ liệu training.\nTính toán mean squared prediction error on the data in the left-out kth fold, as a function of α.\n\n\nTính trung bình các kết quả cho từng giá trị α, và chọn α để tối thiểu hóa sai số trung bình (average error).\n\nBước 4: Lựa chọn subtree từ bước 2 mà tương ứng với việc lựa chọn α.\n\n\nVới mỗi giá trị α sẽ tương ứng với subtree T thuộc To mà:\n\\[\\sum_{m=1}^{|T|}\\sum_{i: x_i \\epsilon R_m}^{J}(y_i - \\widehat{y}_{R_m})^2 + α|T|\\]\nnhỏ nhất có thể. Ở đây, T - số lượng terminal nodes của cây, Rm - rectangle (the subset of predictor space) tương ứng với terminal node thứ m, yRm - giá trị dự báo (tức là trung bình các giá trị quan sát trong Rm).\nTham số α sẽ kiểm soát sự đánh đổi giữa độ phức tạp của subtree và chất lượng dự báo trên tập dữ liệu train. Nếu α = 0, thì subtree T = To, vì biểu thức nói trên trở thành training error. Biểu thức trên làm chúng ta liên tưởng đến lasso - một biểu thức tương tự dùng để kiểm soát độ phức tạp của mô hình hồi quy tuyến tính."
  },
  {
    "objectID": "p03-04-decision-tree.html#classification-trees",
    "href": "p03-04-decision-tree.html#classification-trees",
    "title": "21  Cây quyết định",
    "section": "21.3 Classification Trees",
    "text": "21.3 Classification Trees\nCây quyết định phân loại (classification tree) tương tự như cây quyết định hồi quy (regression tree), chỉ khác điểm đối với classification tree thì biến đầu ra muốn dự báo là biến rời rạc hay biến định tính (categorical variable) thay vì là biến liên tục như đối với regression tree.\nĐối với regression tree thì kết quả dự báo cho một quan sát mới chính là trung bình kết quả của các giá trị quan sát trong tập dữ liệu training tại vùng (region) mà quan sát đó thuộc về. Nhưng đối với classification tree, kết quả dự báo cho một quan sát mới sẽ là giá trị mà có tần suất xuất hiện nhiều nhất trong số các quan sát của tập dữ liệu training tại vùng mà quan sát đó thuộc về.\nGiả sử, chúng ta đã xây được một cây quyết định dự báo xem khách hàng có trả được hết nợ hoặc không (tức biến đầu ra có 2 class là trả được nợ và không trả được nợ). Và kết quả là cây quyết định đó chia thành khách hàng vào 3 vùng (regions) khác nhau: A,B,C.\nBây giờ chúng ta đang muốn dự báo xem một khách hàng mới Nguyễn Văn T có trả được nợ hay không dựa vào mô hình đã được xây dựng. Giả sử, với những nguyên tắc phân nhóm (spliting rules) của cây quyết định, chúng ta đã phân nhóm được khách hàng nói trên vào vùng A nhất định. Như vậy, nếu trong các khách hàng thuộc vùng A trước đó, đa số là khách hàng trả được nợ (tức số lượng khách hàng trả được nợ nhiều hơn số lượng khách hàng không trả được nợ), thì chúng ta có thể dự báo rằng khách hàng mới nói trên có trả được nợ.\nKhi diễn giải kết quả của classification tree, chúng ta không chỉ quan tâm đến class được dự báo tại mỗi vùng (region) nhất định mà còn quan tâm đến class propotions giữa các quan sát trong tập dữ liệu training mà thuộc về vùng đó. Như trong ví dụ trên, ngoài việc dự báo được khách hàng thuộc vùng A sẽ có khả năng trả được nợ, chúng ta còn muốn biết cụ thể tỷ lệ khách hàng trả được nợ và không trả được nợ tại vùng A là bao nhiêu, và tỷ lệ khách hàng thuộc vùng A trên tổng số khách hàng là bao nhiêu.\nCũng giống như regression tree, việc xây dựng classification tree cũng sử dụng recursive binary splitting. Tuy nhiên đối với classification tree, RSS không được sử dụng như một tiêu chí để làm phân nhóm nhị phân (binary splits), thay vào đó là classification error rate. Khi chúng ta đã xác định được quan sát mới vào class mà tần suất xuất hiện nhiều nhất trong vùng mà quan sát đó thuộc về, classification error rate sẽ chính là tỷ lệ số quan sát trên tập dữ liệu training trong vùng đó mà không thuộc vào class đa số:\n\\[E = 1 - \\max_k(\\widehat{p}_{mk})\\]\nỞ đây pmk - tỷ trọng quan sát trên tập dữ liệu training trong vùng thứ m từ class thứ k. Tuy nhiên, classification error đôi khi sẽ không hiệu quả đối với một số trường hợp trong thực tế, vì vậy, có 2 thước đo khác mà chúng ta nên sử dụng:\nChỉ số Gini:\n\\[G = \\sum_{k=1}^{K}\\widehat{p}_{mk}(1-\\widehat{p}_{mk})\\]\nĐây là một chỉ số về tổng variance qua K classes. Dễ nhận thấy rằng chỉ số Gini sẽ càng nhỏ nếu pmk tiếp cận 0 hoặc 1. Vì lý do đó chỉ số Gini được coi là thước đo độ đồng nhất của node (node purity).\nMột chỉ số khác thay thế cho chỉ số Gini là cross-entropy:\n\\[D = -\\sum_{k=1}^{K}\\widehat{p}_{mk}log(\\widehat{p}_{mk})\\]\npmk nhận giá trị từ 0 đến 1, nên -pmk*log(pmk) sẽ không âm. Cross-entropy sẽ tiếp cận đến 0 nếu tất cả các giá trị pmk tiệm cận đến 0 hoặc 1. Vì vậy, cũng giống như chỉ số Gini, cross-entropy sẽ mang giá trị nhỏ nếu như node thứ m đồng nhất.\nKhi xây dựng classification tree, cả chỉ số Gini và cross-entropy đều thường dùng để đánh giá chất lượng của split cụ thể nào đó. Hai chỉ số trên more sensitive với node purity hơn là classification error rate. Cả 3 chỉ số trên đều có thể sử dụng để “tỉa” (prune) cây, nhưng classification error rate sẽ được ưu tiên sử dụng khi chúng ta muốn tính độ chính xác của dự báo (prediction accuracy) của cây quyết định cuối cùng sau khi đã được “tỉa”.\n\nNhư vậy, classification tree thiết lập một tập hợp các nguyên tắc phân nhóm nhị phân (binary splits) với các biến đầu vào (predictor variables) để xây dựng một cây quyết định sao cho có thể phân loại (classify) những quan sát mới vào một trong hai nhóm xác định.\nĐể thực hành xây dựng classification tree trên R, chúng ta sẽ sử dụng dữ liệu Default trong package ISLR có sẵn trong R để dự báo khách hàng nào sẽ không trả được nợ thẻ thẻ tín dụng.\nDữ liệu bao gồm thông tin trả nợ thẻ tín dụng của 10,000 khách hàng:\n\ndefault: Đánh dấu khách hàng có khả năng trả nợ hay không\n\nYes: Không trả được nợ\nNo: Có trả được nợ\n\nstudent: Đánh dấu khách hàng là sinh viên hay không\n\nYes: Sinh viên\nNo: Không phải sinh viên\n\nbalance: Trung bình dư nợ còn lại khách hàng phải trả\nincome: Thu nhập của khách hàng\n\nThuật toán gồm các bước sau:\n\nBước 1: Chọn biến đầu vào phân nhánh tốt nhất (best splits) để chia dữ liệu thành 2 nhóm sao cho tính đồng nhất (the purity/homogeneity) của outcome trong 2 nhóm là maximized (tức là càng nhiều trường hợp khách hàng không trả được nợ vào một nhóm, và các nhiều trường hợp khách hàng trả được nợ vào nhóm còn lại).\n\nNếu biến đầu vào là biến liên tục (continuous), chọn một điểm cut-off để maximize purity của 2 nhóm được tạo ra.\nNếu biến đầu vào là biến rời rạc (categorical), combine the categories to obtain two groups with maximum purity.\n\nBước 2: Chia dữ liệu thành 2 nhóm nói trên và tiếp tục quá trình cho từng nhóm con (subgroup)\nBước 3: Lặp lại bước 1 và bước 2 cho đến khi a subgroup bao gồm số quan sát ít hơn một số lượng nhất định mà chúng ta đặt ra hoặc không thể phân nhóm tiếp để làm tăng tính đồng nhất\n\nThe subgroups in the final set được gọi là terminal nodes. Mỗi terminal node được phân loại vào một category of the outcome or the other dựa vào the most frequent value of the outcome for the sample in that node.\n\nBước 4: To classify a case, run it down the tree to a terminal node, and assign it the modal outcome value assigned in step 3.\n\nQuá trình trên có thể dẫn đến việc cây được xây dựng rất là lớn và sẽ dẫn đến overfitting (chỉ “chính xác” trên tập dữ liệu huấn luyện - training data, nhưng thiếu “chính xác” trên tập dữ liệu kiểm tra - testing data). Kết quả là, những quan sát mới sẽ không được phân loại chính xác. Để khắc phục vấn đề trên chúng ta có thể dùng kỹ thuật “tỉa” cây (pruning) bằng việc lựa chọn cây với 10-fold cross-validated prediction error nhỏ nhất. Cây đã được tỉa sẽ được sử dụng cho việc dự báo các quan sát trong tương lai.\nTrong R, việc xây dựng cây quyết định (decision trees) cũng như “tỉa cành” chúng ta có thể dùng hàm rpart() và prune() trong package rpart.\nDưới đây sẽ là ví dụ về việc xây dựng cây quyết định để phân loại các quan sát là khách hàng sử dụng thẻ tín dụng vào 2 nhóm: Trả được nợ hay không trả được nợ.\n\n# Data\nlibrary(ISLR)\ndata(\"Default\")\n\n# Chia dữ liệu thành 2 tập: train/test\nset.seed(1234)\ntrain <- sample(nrow(Default), 0.7*nrow(Default))\ndf.train <- Default[train,]\ndf.validate <- Default[-train,]\n\n# Biến đầu ra của dữ liệu huấn luyện\ntable(df.train$default)\n\n\n  No  Yes \n6769  231 \n\n# Biến đầu ra của dữ liệu kiểm tra\ntable(df.validate$default)\n\n\n  No  Yes \n2898  102 \n\n\nTrước tiên, chúng ta sẽ xây dựng một cây quyết định với tập dữ liệu huấn luyện, sử dụng hàm rpart().\n\n# Classical decision tree\nlibrary(rpart)\nset.seed(1234)\n# 1. Grows the tree\ndtree <- rpart(default ~ ., \n               data = df.train,\n               method = \"class\",  # biến đầu ra - rời rạc\n               parms=list(split=\"information\")\n)\n\nĐể xem và thống kê kết quả của mô hình vừa xây dựng, chúng ta có thể sử dụng hàm print() và summary(). Cây quyết định có thể sẽ rất lớn và cần phải sử dụng kỹ thuật “tỉa cành”.\n\nprint(dtree)\n\nn= 7000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 7000 231 No (0.96700000 0.03300000)  \n    2) balance< 1472.479 6302  46 No (0.99270073 0.00729927) *\n    3) balance>=1472.479 698 185 No (0.73495702 0.26504298)  \n      6) balance< 1856.76 556  98 No (0.82374101 0.17625899)  \n       12) balance< 1704.503 382  48 No (0.87434555 0.12565445) *\n       13) balance>=1704.503 174  50 No (0.71264368 0.28735632)  \n         26) income< 38153.8 132  32 No (0.75757576 0.24242424) *\n         27) income>=38153.8 42  18 No (0.57142857 0.42857143)  \n           54) balance< 1747.783 19   3 No (0.84210526 0.15789474) *\n           55) balance>=1747.783 23   8 Yes (0.34782609 0.65217391) *\n      7) balance>=1856.76 142  55 Yes (0.38732394 0.61267606)  \n       14) balance< 2113.324 105  51 Yes (0.48571429 0.51428571)  \n         28) income< 17702.99 26   7 No (0.73076923 0.26923077) *\n         29) income>=17702.99 79  32 Yes (0.40506329 0.59493671)  \n           58) income< 24355.19 27  13 No (0.51851852 0.48148148)  \n            116) balance< 1978.492 18   6 No (0.66666667 0.33333333) *\n            117) balance>=1978.492 9   2 Yes (0.22222222 0.77777778) *\n           59) income>=24355.19 52  18 Yes (0.34615385 0.65384615) *\n       15) balance>=2113.324 37   4 Yes (0.10810811 0.89189189) *\n\n\nĐể lựa chọn kích thước của cây, chúng ta có thể kiểm tra phần cptable trong kết quả model dtree như sau:\n\n# Data about the prediction error for various tree sizes\ndtree$cptable\n\n          CP nsplit rel error    xerror       xstd\n1 0.06926407      0 1.0000000 1.0000000 0.06470044\n2 0.02597403      2 0.8614719 0.9047619 0.06164232\n3 0.01298701      4 0.8095238 0.8701299 0.06048665\n4 0.01010101      6 0.7835498 0.8658009 0.06034044\n5 0.01000000      9 0.7532468 0.8658009 0.06034044\n\n\nKết quả cho chúng ta thấy thông tin về sai số dự báo đối với mỗi kích thước khác nhau của cây.\n\nCP - tham số đánh giá độ phức tạp của mô hình (complexity parameter) dùng để penalize larger trees\nnsplit - số lần phân nhóm (number of branch splits) mô tả kích thước của cây. Một cây với n splits sẽ có n+1 terminal nodes\nrel error - tỷ lệ sai số (error rate) của cây đối với từng kích thước được xác định trên tập dữ liệu training.\nxerror - cross-validated error dựa trên 10-fold cross validation (cũng dựa trên dữ liệu training)\nxstd - sai số chuẩn (standard error) của cross validation error\n\nChúng ta có thể sử dụng hàm plotcp() để xem mối quan hệ giữa cross-validated error (xerror) và tham số complexity (CP). Một sự lựa chọn tốt cho kích thước của cây là cây nhỏ nhất mà cross-validated error nằm trong khoảng giá trị 1 standard error của cross-validated error nhỏ nhất\n\n# Plot the cross-validated error against the complexity parameter\nplotcp(dtree)\n\n\n\n\nTrong trường hợp này, min xerror là 0.8255319 và tương ứng với xstd = 0.058442666, như vậy cây nhỏ nhất với xerror nằm trong khoảng giá trị (0.8255319-0.058442666, 0.8255319+0.058442666), hay (0.7670892, 0.8839746). Xem bảng kết quả cptable có thể thấy cây với 3 splits (xerror = 0.8553191, CP = 0.05106383) thỏa mãn điều kiện nói trên.\nCòn nếu dựa vào plot mối quan hệ giữa xerror và CP, chúng ta sẽ lựa chọn cây với giá trị CP ngoài cùng bên trái (CP cao nhất) nằm dưới đường line. Nếu nhìn vào plot này chúng ta có thể chọn CP = 0.055 để tiến hành “tỉa” cây.\nNhư vậy, trong trường hợp này, để “tỉa” cây chúng ta có thể lựa chọn CP = 0.051 hoặc CP = 0.055 (chênh lệch không đáng kể), nên chúng ta sẽ ưu tiên lựa chọn cây với ít “phức tạp” (ít split) hơn - trường hợp này lựa chọn cây với 3 splits (4 terminal nodes).\nHàm prune() sử dụng tham số complexity để tỉa cây theo kích thước mong muốn. Cụ thể hơn, cách này sẽ cắt bỏ đi các splits mà ít quan trọng nhất của cây theo giá trị của tham số complexity mà chúng ta mong muốn.\nTrong trường hợp này, chúng ta lựa chọn cây với 3 splits, tương ứng với complexity (CP = ), do đó để lựa chọn cây với kích thước mong muốn, chúng ta sử dụng câu lệnh sau:\n\n# 2. Prunes the tree\ndtree.pruned <- prune(dtree, cp = 0.05106383)\n\nprint(dtree.pruned)\n\nn= 7000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 7000 231 No (0.96700000 0.03300000)  \n  2) balance< 1472.479 6302  46 No (0.99270073 0.00729927) *\n  3) balance>=1472.479 698 185 No (0.73495702 0.26504298)  \n    6) balance< 1856.76 556  98 No (0.82374101 0.17625899) *\n    7) balance>=1856.76 142  55 Yes (0.38732394 0.61267606) *\n\n\nHàm prp() trong package rpart.plot sử dụng để trực quan hóa cây quyết định cuối cùng. Để phân loại một quan sát, bắt đầu từ phần đỉnh của cây, sau đó di chuyển sang nhánh bên trái nếu một điều kiện là đúng hoặc sang phải trường hợp ngược lại. Tiếp tục di chuyển xuống phía dưới cho đến khi gặp terminal node (lá), tức đã được phân loại.\n\nlibrary(rpart.plot)\nprp(dtree.pruned,\n    type = 2, # Draws the split labels below each node\n    extra = 104, # Includes the probabilities for each class, along with the percentage of observations in each node\n    fallen.leaves = T, # Displays the terminal nodes at the bottom of the graph\n    main = \"Decision Tree\"\n    )\n\n\n\n\nCách đọc kết quả từ cây quyết định từ trên xuống dưới như sau:\n\nTập dữ liệu huấn luyện ban đầu của chúng ta (tức bao gồm tất cả quan sát - 100%) có 97% là default = No (khách hàng trả được nợ), 3% là default = Yes (không trả được nợ)\nVới điều kiện phân nhánh đầu tiên balance < 1472 (tức dư nợ còn lại phải trả < 1472):\n\n90% khách hàng thỏa mãn điều kiện balance < 1472 (sẽ được phân loại về nhóm default = No - trả được nợ), trong đó :\n\n99% là default = No (trả được nợ)\n1% là default = Yes (không trả được nợ)\n\n10% khách hàng không thỏa mãn điều kiện balance < 1472, trong đó:\n\n73% là default = No (trả được nợ)\n27% là default = Yes (không trả được nợ)\n\n\nSau đó, đối với 10% khách hàng không thỏa mãn điều kiện balance < 1472 lại được tiếp tục phân nhánh nhỏ hơn với các điều kiện phân nhánh: balance < 1800 và balance < 1972 (tương tự cách giải thích như trên)\n\n…\n\nKết quả cuối cùng, các quan sát trong tập training được chia thành 4 vùng:\n\nR1: balance < 1472 -> thuộc về nhóm default = No\nR2: balance >= 1472 & balance < 1800 -> thuộc về nhóm default = No\nR3: balance >= 1800 & balance < 1972 -> thuộc về nhóm default = No\nR4: balance >= 1972 -> thuộc về nhóm default = Yes\n\n\nVà sau khi đã build được mô hình cây quyết định hoàn chỉnh, chúng ta sẽ phân loại những quan sát mới - tập dữ liệu validation bằng việc sử dụng hàm predict(), sau đó só sánh kết quả dự báo và thực tế như sau:\n\n# Phân loại những quan sát mới\ndtree.pred <- predict(dtree.pruned,\n                      df.validate,\n                      type = \"class\"\n                      )\n\n# So sánh kết quả dự báo và thực tế\ndtree.perf <- table(df.validate$default,\n                    dtree.pred,\n                    dnn = c(\"Actual\", \"Predicted\")\n                    )\ndtree.perf\n\n      Predicted\nActual   No  Yes\n   No  2877   21\n   Yes   55   47\n\n\nKết quả cho chúng ta thấy trong số 3000 quan sát của tập dữ liệu kiểm tra (quan sát mới) có 2892 khách hàng trả được nợ (default = No) và 23 khách hàng không trả được nợ (default = Yes) được dự báo đúng. Như vậy, tổng số quan sát được dự báo đúng là 2915 (2892+23), tức tỷ lệ quan sát dự báo đúng (accuracy) là 97% (2915/3000). Lưu ý rằng phương pháp cây quyết định có thể bị biased đối với những biến đầu vào có nhiều giá trị hoặc nhiều giá trị bị missing.\nMô hình cây và mô hình tuyến tính\nMô hình cây quyết định là một cách tiếp cận khác so với mô hình tuyết tính, cụ thể là hồi quy tuyến tính có dạng:\n\\[f(X) = \\beta_0 + \\sum_{j=1}^{P}X_jB_j\\]\nNhư vậy, câu hỏi đặt ra là mô hình nào tốt hơn? Câu trả lời là: Tùy thuộc vào bài toán mà chúng ta muốn giải quyết là gì. Nếu bài toán chúng ta muốn giải quyết bao gồm biến đầu ra và các biến đầu vào có mối quan hệ gần giống với tuyến tính thì chúng ta nên sử dụng hồi quy tuyến tính. Còn nếu giữa các biến đầu vào và biến đầu ra có mối quan hệ phi tuyến tính hoặc phức tạp thì chúng ta nên sử dụng mô hình cây quyết định. Để đánh giá chất lượng dự báo của mô hình cây quyết định và mô hình tuyến tính, chúng ta có thể tính toán sai số trên tập dữ liệu validation hoặc tính toán sai số sử dụng cross-validation.\nƯu điểm và nhược điểm của mô hình cây quyết định\nMô hình cây quyết định có những ưu điểm vượt trội so với mô hình tuyến tính như sau: Thứ nhất, mô hình cây quyết định có thể mô tả bằng graphic, vì vậy rất dễ dàng diễn giải kết quả mô hình, thậm chí là dễ dàng hơn so với mô hình tuyến tính. Thứ hai, mô hình cây quyết định có thể sử dụng với những biến đầu vào là biến rời rạc mà không cần phải biến đổi về dạng dummy (0-1). Tuy nhiên, nhược điểm của mô hình cây quyết định là thiếu ổn định (non-robust). Hay nói cách khác, với một sự thay đổi nhỏ trong tập dữ liệu, có thể dẫn đến sự thay đổi lớn trong kết quả dự báo.\nTuy nhiên, những phương pháp mà tổng hợp nhiều cây quyết định như: bagging, random forests, boosting có thể khắc phục được nhược điểm nêu trên và cải thiện chất lượng dự báo. Chúng ta sẽ cùng tìm hiểu trong các phần tiếp theo."
  },
  {
    "objectID": "p03-05-bagging-boosting.html#bagging",
    "href": "p03-05-bagging-boosting.html#bagging",
    "title": "22  Bagging, RandomForest và Boosting",
    "section": "22.1 Bagging",
    "text": "22.1 Bagging\n\n\n\n\n\nMô hình cây quyết định như đã đề cập, sẽ gặp phải vấn đề high variance. Nghĩa là nếu chúng ta chia ngẫu nhiên tập dữ liệu training làm 2 tập dữ liệu con, và sau đó xây dựng mô hình trên 2 tập dữ liệu con đó, kết quả nhận được sẽ có thể khá khác nhau. Boostrap aggregation hay bagging sẽ có thể làm giảm variance, đây là một phương pháp rất hiệu quả và phổ biến khi chúng ta sử dụng các phương pháp liên quan đến cây quyết định.\nGiả sử, có tập hợp n quan sát độc lập Z1,…, Zn, mỗi quan sát với variance \\(σ^2\\), variance của giá trị trung bình các quan sát Z là \\(σ^2/n\\). Nói cách khác, trung bình tập hợp các quan sát sẽ làm giảm variance. Vì thế cách tự nhiên nhất để làm giảm variance và tăng độ chính xác của dự báo là lấy thật nhiều các dữ liệu training khác nhau, rồi xây dựng các mô hình dự báo sử dụng các tập dữ liệu training đó, sau đó lấy trung bình các kết quả dự báo. Nói cách khác, chúng ta sẽ tính toán \\(fˆ1(x), fˆ2(x)\\), . . . , fˆB(x) sử dụng B tập dữ liệu training khác nhau, sau đó lấy trung bình để nhận được một mô hình đơn với variance thấp như sau:\n\\[\\widehat{f}_{avg}(x) = \\frac{1}{B}\\sum_{b=1}^{B}\\widehat{f}^{b}(x)\\]\nTuy nhiên, điều đó là không thực tế vì chúng ta không thể có nhiều dữ liệu training khác nhau, dữ liệu là hữu hạn. Vì vậy, chúng ta sẽ sử dụng boostrap, tức phương pháp lấy mẫu ngẫu nhiên có hoàn lại từ tập dữ liệu training duy nhất của chúng ta. Theo đó, chúng ta sẽ tạo ra B boostrapped dữ liệu training khác nhau để có được các giá trị dự báo khác nhau, và sau đó sẽ lấy trung bình tất cả các kết quả dự báo, thu được kết quả cuối cùng:\n\\[\\widehat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^{B}\\widehat{f^{*}}^{b}(x)\\]\nĐây gọi là bagging. Bagging có thể cải thiện chất lượng dự báo cho rất nhiều các mô hình hồi quy, đặc biệt hiệu quả đối với mô hình cây quyết định. Để áp dụng bagging với regression trees, chúng ta đơn giản chỉ cần xây dựng B regression trees sử dụng B boostrapped dữ liệu training, sau đó lấy trung bình các kết quả dự báo. Những cây quyết định này được xây dựng rất sâu (nhiều tầng) và không được “tỉa”. Vì thế mỗi cây quyết định trên sẽ có variance cao, nhưng bias thấp. Lấy trung bình kết quả B cây quyết định này sẽ làm giảm variance. Bagging có thể cải thiện chất lượng dự báo một cách đáng kể khi kết hợp hàng trăm hoặc thậm chí hàng nghìn cây quyết định lại với nhau.\nCho đến thời điểm hiện tại, chúng ta đã mô tả phương pháp bagging đối với regression trees, tức dự báo biến đầu ra là biến liên tục. Vậy phương pháp bagging có thể sử dụng với bài toán mà biến đầu ra là biến rời rạc không? Trong trường hợp này, giả sử khi chúng ta muốn phân loại một quan sát mới, chúng ta có thể dự báo được quan sát mới trên thuộc class nào trong B cây quyết định khác nhau, rồi sau đó, lấy majority vote - tức là, quan sát mới trên sẽ rơi vào class mà tần suất xuất hiện của nó nhiều nhất trong B kết quả dự báo khác nhau.\nTrong phương pháp bagging, tham số về số lượng cây quyết định B nói trên mà càng lớn thì cũng không thể dẫn đến overfitting. Trong thực tế, chúng ta sẽ chọn số lượng cây quyết định đủ lớn để sao cho sai số đủ nhỏ.\nOut-of-Bag Error Estimation\nCó một cách khá đơn giản để ước lượng sai số dự báo (test error) của mô hình bagging mà không cần dùng cross-validation hoặc tập dữ liệu validation. Nhắc lại key của phương pháp bagging là việc các cây quyết định sẽ được xây dựng nhiều lần sử dụng những tập dữ liệu bootstrapped khác nhau. Mỗi một cây quyết định được xây đều sử dụng khoảng 2/3 số quan sát, còn 1/3 quan sát còn lại không được sử dụng trong quá trình xây dựng mô hình sẽ được gọi là những quan sát out-of-bag (OOB). Chúng ta có thể dự báo kết quả cho quan sát thứ i sử dụng từng cây quyết định mà các quan sát là OOB. Điều này sẽ mang lại khoảng B/3 giá trị dự báo cho quan sát thứ i này. Để dự báo giá trị cuối cùng của quan sát đó, chúng ta sẽ lấy trung bình các kết quả dự báo (đối với bài toán hồi quy - regression) hoặc lấy theo số đông - majority vote (đối với bài toán phân loại - classification). Đó chính là kết quả dự báo OOB (OOB prediction) cho quan sát i. OOB prediction có thể sử dụng cho từng n quan sát, và có thể tính toán được OOB MSE (đối với bài toán hồi quy) hoặc classification error (đối với bài toán phân loại). Cách sử dụng OOB để ước lượng sai số dự báo (test error) sẽ thuận tiện hơn so với cách cross-validation khi sử dụng bagging đối với tập dữ liệu lớn.\nVariable Importance Measures\nNhư vừa tìm hiểu, bagging sẽ cải thiện được độ chính xác của dự báo so với mô hình cây quyết định đơn lẻ. Tuy nhiên, bagging lại rất khó để giải thích kết quả mô hình do việc tổng hợp rất nhiều cây quyết định khác nhau.\nMặc dù đối với bagging rất khó để giải thích kết quả mô hình, nhưng chúng ta vẫn có thể xem được thống kê tổng quát về mức độ quan trọng của các biến đầu vào trong mô hình bằng việc sử dụng RSS (đối với bagging regression trees) hoặc chỉ số Gini (đối với bagging classification trees)."
  },
  {
    "objectID": "p03-05-bagging-boosting.html#random-forests",
    "href": "p03-05-bagging-boosting.html#random-forests",
    "title": "22  Bagging, RandomForest và Boosting",
    "section": "22.2 Random Forests",
    "text": "22.2 Random Forests\nCũng giống như bagging, random forests cũng xây dựng một tập hợp các cây quyết định sử dụng các tập dữ liệu con được chia theo phương pháp boostrap (lấy mẫu ngẫu nhiên có hoàn lại) từ tập dữ liệu training ban đầu. Tuy nhiên, với phương pháp random forests thì những tập dữ liệu con đó sẽ không bao gồm tất cả các biến đầu vào (p - tổng số lượng biến đầu vào) trong tập dữ liệu training ban đầu như bagging mà chỉ bao gồm m biến nhất định (thông thường m ~ sqrt(p)).\nĐối với bagging các cây quyết định có thể tương quan chặt chẽ với nhau (highly correlated) do các cây đều lấy cùng một số lượng là tất cả các biến đầu vào trong tập dữ liệu training ban đầu. Điều đó sẽ dẫn đến việc variance sẽ cao. Trong khi đó, random forests có thể khắc phục được vấn đề trên khi mỗi cây quyết định được xây dựng chỉ lấy ngẫu nhiên m biến đầu vào ngẫu nhiên. Quá trình đó được gọi là decorrelating. Quá trình này sẽ giúp kết quả dự báo đáng tin cậy hơn.\nNhư vậy, điểm khác biệt quan trọng nhất giữa bagging và random forests là việc chọn số lượng biến đầu vào:\n\nbagging lấy tất cả các biến đầu vào (p)\nrandom forests lấy m biến nhất định (m ~ sqrt(p))\n\nVì thế, nếu xây dựng mô hình random forests với số lượng biến đầu vào m = p (tức lấy tất cả các biến đầu vào) thì mô hình trở thành bagging. Do đó, có thể nói bagging là một trường hợp đặc biệt của random forests.\nThuật toán của Random Forest sẽ bao gồm việc lấy mẫu và chọn biến để xây dựng một số lượng lớn các cây quyết định khác nhau. Kết quả dự báo cuối cùng của một quan sát sẽ lấy trung bình các kết quả dự báo của các cây quyết định (đối với bài toán hồi quy - regression) hoặc lấy majority vote từ các kết quả dự báo từ các cây quyết định để xác định quan sát đó thuộc class nào (đối với bài toán phân loại - classification).\nGiả định rằng N là số lượng quan sát của tập dữ liệu training và p là số lượng biến đầu vào. Thuật toán sẽ diễn ra theo các bước như sau:\n\nBước 1: Xây dựng số lượng lớn các cây quyết định bằng việc lấy mẫu ngẫu nhiên có hoàn lại N quan sát từ tập dữ liệu training\nBước 2: Với mỗi cây quyết định lựa chọn m < p biến nhất định. Những biến này được cân nhắc lựa chọn để phân nhánh, với mỗi cây quyết định thì đều có số lượng biến là m.\nBước 3: Xây dựng các cây (không thực hiện tỉa cây)\nBước 4: Dự báo các quan sát mới bằng việc lấy trung bình các kết quả dự báo của các cây quyết định khác nhau đã được xây dựng (đối với bài toán hồi quy), hoặc lấy theo kết quả số đông của các cây quyết định khác nhau đã được xây dựng (đối với bài toán phân loại).\n\nOut-of-bag (OOB) error estimate được tính toán bằng việc phân loại quan sát mới mà không có trong tập dữ liệu training khi xây dựng cây quyết định. Việc này sẽ hữu ích khi mà chúng ta không có dữ liệu validation.\nTrong R để xây dựng mô hình random forests chúng ta có thể sử dụng hàm randomForest() trong package randomForest. Số lượng cây mặc định là 500, số lượng biến tại mỗi cây mặc định là sqrt(tổng số biến), và kích thước nhỏ nhất của cây mặc định là 1.\n\nlibrary(ISLR)\nlibrary(randomForest) # Package sử dụng cho Random Forests\nset.seed(1234)\n\n# Data\ndata(\"Default\")\n\n# Chia dữ liệu thành 2 tập: train/test\nset.seed(1234)\ntrain <- sample(nrow(Default), 0.7*nrow(Default))\ndf.train <- Default[train,]\ndf.validate <- Default[-train,]\n\n# Biến đầu ra của dữ liệu huấn luyện\ntable(df.train$default)\n\n\n  No  Yes \n6769  231 \n\n# Biến đầu ra của dữ liệu kiểm tra\ntable(df.validate$default)\n\n\n  No  Yes \n2898  102 \n\n# Xây dựng mô hình trên tập training\nfit_rf <- randomForest(default ~ .,\n                       data = df.train,\n                       # na.action = na.roughfix, # Xử lý giá trị missing (nếu có)\n                       importance = T\n                       )\n\nfit_rf\n\n\nCall:\n randomForest(formula = default ~ ., data = df.train, importance = T) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n        OOB estimate of  error rate: 3%\nConfusion matrix:\n      No Yes class.error\nNo  6746  23 0.003397843\nYes  187  44 0.809523810"
  },
  {
    "objectID": "p03-05-bagging-boosting.html#boosting",
    "href": "p03-05-bagging-boosting.html#boosting",
    "title": "22  Bagging, RandomForest và Boosting",
    "section": "22.3 Boosting",
    "text": "22.3 Boosting\nBây giờ chúng ta sẽ cùng thảo luận về boosting, một phương pháp khác để cải thiện chất lượng dự báo từ việc sử dụng cây quyết định. Giống như các phương pháp trước, boosting có thể áp dụng được đối với cả 2 bài toán: Hồi quy (regression) và phân loại (classification).\nNhắc lại một chút, boosting và random forests sử dụng phương pháp boostrap (lấy mẫu ngẫu nhiên có hoàn lại) để tạo các tập dữ liệu con từ dữ liệu training ban đầu, sau đó xây dựng các cây quyết định đối với từng tập dữ liệu con đó (các cây quyết định được xây dựng độc lập với nhau). Cuối cùng, sẽ tổng hợp lại các cây quyết định để ra được kết quả dự báo cuối cùng bằng cách lấy trung bình các kết quả dự báo của các cây quyết định (đối với bài toán hồi quy), hoặc lấy theo số đông các kết quả dự báo của các cây quyết định (đối với bài toán phân loại).\nBoosting cũng hoạt động theo cách tương tự, nhưng khác ở chỗ là việc xây dựng các cây quyết định từ những tập dữ liệu con khác nhau không phải là độc lập hoàn toàn với nhau như bagging hay random forests. Thay vào đó, boosting xây dựng các cây quyết định một cách có trình tự (sequentially): Mỗi cây kế tiếp được xây dựng bằng cách sử dụng kết quả từ những cây trước đó. Boosting tập trung nhiều hơn vào những quan sát bị dự báo sai từ những cây trước để góp phần cải thiện kết quả dự báo cuối cùng. Boosting không dùng boostrap để chia tập dữ liệu training ban đầu, mà thay vào đó là việc dùng các phiên bản đã được modified từ tập dữ liệu train ban đầu để xây dựng các cây quyết định.\nChúng ta sẽ cùng xem xét bài toán hồi quy. Giống như bagging và randomforests, boosting kết hợp nhiều cây quyết định lại với nhau: f1,…, fB.\nThuật toán boosting đối với bài toán hồi quy:\n\nBước 1: Đặt fˆ(x) = 0 và ri = yi với mọi i trên tập dữ liệu training\nBước 2: Với b = 1,2.., B, lặp lại:\n\nXây dựng một cây fˆb với d splits (d+1 terminal nodes) với dữ liệu training (X,r).\nUpdate fˆ bằng việc adding in a shrunken version of the new tree, và update residuals:\n\n\n\\[\\widehat{f}(x) = \\widehat{f}(x) + λ\\widehat{f}^{b}(x)\\]\n\\[r_i = r_i - λ\\widehat{f}^{b}(x_i)\\]\n\nBước 3: Kết quả của boosted model:\n\n\\[\\widehat{f}(x) = \\sum_{b=1}^{P}λ\\widehat{f}^{b}(x)\\]\nThay vì việc xây dựng các cây quyết định đơn lẻ với kích thước lớn có thể dẫn đến vấn đề overfitting, phương pháp boosting sẽ “học” chậm (learn slowly). Khi xây dựng xong cây quyết định đầu tiên, chúng ta sẽ xây dựng cây quyết định tiếp theo sử dụng biến đầu ra là phần dư (residuals) của cây trước đó. Sau đó, sẽ xây dựng các cây quyết định tiếp theo để update residuals. Mỗi cây có thể có kích thước nhỏ, với chỉ một vài terminal nodes được quyết định bởi tham số d trong thuật toán. Bằng việc xây dựng những cây nhỏ với residuals, chúng ta có thể dần dần cải thiện fˆ. Tham số shrinkage hay learning rate(tốc độ học của mô hình) λ sẽ làm mô hình “học” chậm và kỹ hơn nữa giúp cải thiện chất lượng mô hình. Lưu ý rằng khác với bagging, đối với boosting thì việc xây dựng các cây quyết định tiếp theo sẽ phụ thuộc vào kết quả của các cây trước đó.\nNhư vậy, chúng ta đã vừa cùng tìm hiểu về boosting regression trees. Bây giờ chúng sẽ cùng tìm hiểu về 2 thuật toán trong boosting là Adaboost và Gradient Boosting.\nAdaBoost\nAdaBoost kết hợp các “weak learners” để tạo thành “strong learner” (“weak learners” được hiểu là các cây phân loại chỉ tốt hơn một chút so với việc đoán ngẫu nhiên). Sau mỗi bước lặp, những quan sát bị phân loại sai sẽ được đánh trọng số cao hơn, những quan sát được phân loại đúng sẽ đánh trọng số thấp hơn. Mỗi cây tiếp theo được xây dựng với mục tiêu phân loại đúng những quan sát đã bị phân loại sai ở cây trước đó.\nChúng ta sẽ mô tả thuật toán Adaboost thông qua việc sử dụng ví dụ sau đây: Phân loại các quan sát vào 2 nhóm + hoặc -. Chúng ta sẽ thực hiện các bước sau:\n\n\n\nDiễn giải:\n\nBox 1: Đánh trọng số bằng nhau đối với tất cả quan sát và xây dựng một decision stump - D1 (cây chỉ gồm 1 split hay 1 tầng) để phân loại các quan sát thành 2 nhóm + và -. Kết quả cho thấy có 3 quan sát bị phân loại sai (là + nhưng lại bị cho vào nhóm -), 3 quan sát này sẽ được đánh trọng số cao hơn và tiếp tục xây dựng decision stump khác - D2.\nBox 2: D2 được xây dựng với mục đích phân loại đúng 3 quan sát bị phân loại sai ở D1. Kết quả cho thấy, lại có 3 quan sát bị phân loại sai (là - nhưng bị cho vào nhóm +). Lại tiếp tục đánh trọng số cao hơn đối với những quan sát này và tiếp tục xây dựng decision stump - D3.\nBox 3: D3 được xây dựng với mục đích phân loại đúng 3 quan sát bị phân loại sai ở D2. Kết quả cho thấy vẫn có những quan sát bị phân loại sai.\nBox 4: Kết hợp D1, D2, D3 để tạo thành D4 - phân loại tốt hơn so với D1, D2, D3 (nhóm + và - đã được phân loại hoàn toàn).\n\nGradient Boosting\nGradient Boosting = Gradient Descent + Boosting\nCả AdaBoost và Gradient Boosting đều kết hợp các “weak learners” để tạo thành một “strong learner” và đều tập trung vào những quan sát bị dự báo sai. AdaBoost thì đánh trong số cao hơn vào những quan sát bị dự báo sai tại mỗi cây trước, và cố gắng dự báo đúng những quan sát đó tại cây tiếp theo. Trong khi đó, với Gradient Boosting, mỗi một cây mới sẽ được xây dựng với mục tiêu tối thiểu hóa dần tổng loss của cây trước đó bằng việc sử dụng phương pháp gradient descent.\nTrong Gradient Boosting, việc tính tổng loss dựa vào việc lựa chọn loại “loss function” nào, ví dụ như: square loss, absolute loss, huber loss. Mỗi loại đều có những ưu/nhược điểm riêng.\n\nSquare loss:\n\n\\[L(y,F) = (y-F)^2/2\\]\n\nAbsolute loss:\n\n\\[L(y,F) = |y-F|\\]\n\nHuber loss:\n\n\\[\\begin{cases}(y-F)^2/2 & |x-F| \\leqslant \\delta\\\\\\delta(|y-F| - \\delta/2) & |x-F| > \\delta\\end{cases}\\]\nVí dụ:\n\n\nBoosting có 3 tham số cơ bản để tối ưu hóa mô hình (tuning parameters):\n\nSố lượng cây quyết định (B): Với boosting khi số lượng cây quá nhiều có thể dẫn đến overfitting, nên chúng ta sẽ sử dụng cross-validation để lựa chọn số lượng cây\nTốc độ học λ (learning rate hoặc shrinkage): Giá trị nhỏ, dương. λ có thể nhận các giá trị như: 0.1, 0.01, hay 0.001 tùy từng trường hợp. λ càng nhỏ thì mô hình sẽ “học” càng chậm, càng lâu.\nSố lần splits, hay phân nhánh (d) của mỗi cây: Tham số này dùng để kiểm soát độ phức tạp của mô hình. Tham số này còn có thể gọi là số tầng cây (interactive depth). Nếu d = 1 (tức cây chỉ có 1 tầng hay 1 split) thì cây quyết định đó được gọi là stump.\n\nĐể thực hành xây dựng mô hình boosting trên R, chúng ta sẽ sử dụng dữ liệu có sẵn trong R - GermanCredit.\nĐây là dữ liệu ghi nhận về lịch sử vay của khách hàng, với các 61 biến đầu vào cùng với biến đầu ra Class ghi nhận thực tế là các khoản vay đó có phải là khoản nợ xấu hay không.\nĐể xây dựng mô hình boosting trên R, chúng ta sẽ sử dụng hàm gbm() trong package gbm.\n\nlibrary(caret)\ndata(\"GermanCredit\")\ndata <- GermanCredit\nrm(GermanCredit)\n\n# Hàm tính toán các chỉ số đo lường chất lượng dự báo của mô hình\nmodel.performance <- function(confusion_matrix) {\n  a <- confusion_matrix[1,1]\n  b <- confusion_matrix[1,2]\n  c <- confusion_matrix[2,2]\n  d <- confusion_matrix[2,1]\n  \n  tpr <- c/(b+c)\n  precision <- c/(c+d)\n  accuracy <- (a+c)/(a+b+c+d)\n  \n  print(paste('recall :',round(tpr,2)))\n  print(paste('precision :', round(precision,2)))\n  print(paste('accuracy :',round(accuracy,2)))\n}\n\n# Chia data: training/testing tỷ lệ 8/2\nset.seed(123) \nindxTrain <- createDataPartition(y = data$Class,p = 8/10,list = FALSE) \ntraining <- data[indxTrain,] \ntesting <- data[-indxTrain,] \n\ndf.train <- training\ndf.train$Status[df.train$Class == \"Good\"] <- 1\ndf.train$Status[df.train$Class == \"Bad\"] <- 0\n\ndf.test <- testing\ndf.test$Status[df.test$Class == \"Good\"] <- 1\ndf.test$Status[df.test$Class == \"Bad\"] <- 0\n\nrm(training, testing)\n\n# Gradient Boosting\nset.seed(9999)\n\n# Xây dựng mô hình trên tập train\nlibrary(gbm)\ngbm.train <- gbm(Status ~ . - Class,\n                 data = df.train,\n                 distribution = \"bernoulli\", \n                 n.trees = 1000,\n                 shrinkage = 0.01,\n                 interaction.depth = 4)\n\n# Dự báo quan sát trên tập test\ngbm.result <- predict(gbm.train,\n                      newdata = df.test,\n                      n.trees = 1000,\n                      type = \"response\")\n\n\n# Confusion matrix\ngbm.conf  <-  rep (\"Bad\", 200) \ngbm.conf[gbm.result > 0.5] = \"Good\" \ngbm.confusion <- table(gbm.conf, df.test$Class)\ngbm.confusion \n\n        \ngbm.conf Bad Good\n    Bad   25   16\n    Good  35  124\n\nmodel.performance(gbm.confusion)\n\n[1] \"recall : 0.89\"\n[1] \"precision : 0.78\"\n[1] \"accuracy : 0.74\"\n\n\n\nKết quả confusion matrix trên tập dữ liệu testing cho chúng ta thấy:\n\nTỷ lệ dự báo đúng trên tổng quan sát là 80% (tức 35 khách hàng có nợ xấu và 124 khách hàng không có nợ xấu được dự báo đúng trên tổng số 200 khách hàng trên tập dữ liệu testing)\nTrong số 140 khách hàng thực tế không có nợ xấu thì chúng ta dự báo chính xác 124 khách hàng (tỷ lệ 124/140 = 89%)\nTrong số 149 khách hàng mà chúng ta dự báo là không có nợ xấu, có 124 khách hàng được dự báo chính xác (tỷ lệ 124/149 = 83%)\n\n\n\n# ROC\nlibrary(ROCR) # Dùng để vẽ đường ROC và tính toán AUC\ngbm.ROC <- prediction(gbm.result, df.test$Class)\ngbm.ROCperf_test <- performance(gbm.ROC, \"tpr\", \"fpr\")\n\n# Vẽ ROC\nplot(gbm.ROCperf_test)\n\n\n\n# Tính toán AUC\ngbm.auc_test <- performance(gbm.ROC, \"auc\", \"cutoff\")\ngbm.auc_test@y.values\n\n[[1]]\n[1] 0.7639286\n\n\n\nKết quả AUC ~ 82% trên tập testing cho thấy chất lượng dự báo của mô hình đạt chất lượng khá tốt\n\n\n22.3.1 Xây dựng boosting tree với R\nĐể tìm hiểu kỹ hơn thuật toán boosting, ta sẽ phân tích kỹ thuật toán gradient boosting với hàm loss là tổng bình phương sai số. Thuật toán diễn ra như sau.\n\nCho tập training d=(X,y). Số lượng cây quyết định (bước) B, tốc độ thuật toán thay đổi (shrinkage factor/learning rate) ϵ, số tầng của mỗi cây là d đã được xác định trước. Đặt mô hình đầu tiên \\(\\widehat{G_0} \\equiv 0\\) và sai số ban đầu r=y.\nVới b=1,2,…,B thực hiện các bước sau: > a) Xây mô hình cây quyết định \\(\\widetilde{g}_b\\) với dữ liệu (X,r), với số tầng d > b) Cập nhật mô hình dự báo mới nhất \\(\\widetilde{g}_b\\): \\(\\widetilde{G}_b=\\widetilde{G}_{b−1} + \\epsilon*\\widetilde{g}_b\\) > c) Cập nhật sai số mô hình để xây dựng mô hình cho bước tiếp theo: \\(r_i=r_i−\\epsilon*\\widetilde{g}_b\\), i=1,2,…,n\nTrả ra kết quả của $\\(\\widehat{G}_b\\), b=1,2,…,B.\nTrong đó, B, ϵ, và d là các tham số cần tối ưu\n\n\n# Color pallete\ncols <- RColorBrewer::brewer.pal(9, \"Set1\")\n\n# Function to implement gradient boosting with squared-error loss. Based on algorithm 17.2 on page 333 of Computer Age Statistical Inference, by Bradley, Efron amd Trevor Hastie\n\nrpartBoost <- function(X, y, data, num_trees = 100, learn_rate = 0.1, \n                       tree_depth = 6, verbose = FALSE) {\n  require(rpart)\n  # Tạo ma trận chứa kết quả\n  G_b_hat <- matrix(0, nrow = length(y), ncol = num_trees + 1)\n  r <- y\n  for (tree in seq_len(num_trees)) {\n    if (verbose) {\n      message(\"iter \", tree, \" of \", num_trees)\n    }\n    # Step 1: Xây mô hình r ~ X\n    g_b_tilde <- rpart(r ~ X, control = list(cp = 0, maxdepth = tree_depth))\n    # Step 2.1: Tính toán e*g_b\n    g_b_hat <- learn_rate * predict(g_b_tilde)\n    # Step 3: Kết quả dự báo của tree + 1\n    G_b_hat[, tree + 1] <- G_b_hat[, tree] + matrix(g_b_hat)\n    # Update r\n    r <- r - g_b_hat\n  }\n  colnames(G_b_hat) <- paste0(\"tree_\", c(0, seq_len(num_trees)))\n  G_b_hat\n}\n\n# Xây dựng hàm vẽ kết quả mô hình ở bước thứ i\nplotIter <- function(object, iter, show_legend = FALSE, ...) {\n  plot(x, y, ...)\n  lines(x, sin(x), lwd = 3, col = cols[2L])\n  lines(x, object[, iter + 1], lwd = 3, col = cols[1L])\n  if (show_legend) {\n    legend(\"topright\", legend = c(\"Boosted prediction\", \"True function\"),\n           lty = 1L, lwd = 3L, col = cols[1L:2L], inset = 0.01)\n  }\n}\n\n# Simulate some sine wave data\nset.seed(101)\nx <- seq(from = 0, to = 2 * pi, length = 500)\ny <- sin(x) + rnorm(length(x), sd = 0.3)\nplot(x, y)\n\n\n\n# gradient boosted decision trees\nbst <- rpartBoost(X = x, y = y, num_trees = 1000, learn_rate = 0.1, \n                  tree_depth = 3, verbose = TRUE)\n\n# Plot first 15 iterations\npar(mfrow = c(3, 3))\nfor (i in c(0, 5, 10, 15, 25, 50, 100, 500, 1000)) {\n  plotIter(bst, iter = i, main = paste(\"Iter:\", i))\n}\n\n\n\n# plotIter(bst, iter = 10)\n# # # Tạo gif file\n# png(file = \"gifs/boosted-stumps%02d.png\", width = 500, height = 500)\n# for (i in c(0:100)){\n#   plotIter(bst, iter = i, show_legend = TRUE)\n# }\n# dev.off()\n# system(\"magick convert -delay 10 gifs/*.png gifs/boosted_stumps.gif\")\n# file.remove(list.files(path = \"gifs\", pattern = \".png\", full.names = TRUE))"
  },
  {
    "objectID": "p03-05-bagging-boosting.html#tài-liệu-tham-khảo",
    "href": "p03-05-bagging-boosting.html#tài-liệu-tham-khảo",
    "title": "22  Bagging, RandomForest và Boosting",
    "section": "22.4 Tài liệu tham khảo",
    "text": "22.4 Tài liệu tham khảo\n\nhttps://bgreenwell.github.io/MLDay18/MLDay18.html#59"
  },
  {
    "objectID": "p03-07-xay-dung-mo-hinh-voi-h2o.html#giới-thiệu",
    "href": "p03-07-xay-dung-mo-hinh-voi-h2o.html#giới-thiệu",
    "title": "23  Xây dựng mô hình với H2O",
    "section": "23.1 Giới thiệu",
    "text": "23.1 Giới thiệu\n\nH2O là một phần mềm dựa trên nền tảng của Java dùng để mô hình hóa dữ liệu (data modeling) và tính toán nói chung (general computing).\nH2O là một platform mã nguồn mở hàng đầu thế giới hiện nay về Machine Learning và AI.\nHỗ trợ nhiều phần mềm khác nhau như: R, Python, Azure, Spark…"
  },
  {
    "objectID": "p03-07-xay-dung-mo-hinh-voi-h2o.html#cài-đặt-và-khởi-động",
    "href": "p03-07-xay-dung-mo-hinh-voi-h2o.html#cài-đặt-và-khởi-động",
    "title": "23  Xây dựng mô hình với H2O",
    "section": "23.2 Cài đặt và khởi động",
    "text": "23.2 Cài đặt và khởi động\nĐể cài đặt và khởi động H2O, chúng ta cần thực hiện các bước sau:\n\nBước 1: Cài đặt Java\nBước 2: Cài đặt R package h2o\n\n\ninstall.packages(\"h2o\", method = \"wininet\")\n\n\nBước 3: Khởi động H2O từ R - sử dụng câu lệnh h2o.init()\n\n\nlibrary(h2o)\nh2o.init(ip = \"localhost\", \n         port = 54321, \n         nthreads= -1,\n         max_mem_size = \"4g\") # Đặt mức RAM tối đa\n\nLưu ý: Trong trường hợp chúng ta muốn tắt H2O trên R, chúng ta có thể sử dụng câu lệnh sau:\n\nh2o.shutdown(prompt = T)"
  },
  {
    "objectID": "p03-07-xay-dung-mo-hinh-voi-h2o.html#thực-hành-với-r",
    "href": "p03-07-xay-dung-mo-hinh-voi-h2o.html#thực-hành-với-r",
    "title": "23  Xây dựng mô hình với H2O",
    "section": "23.3 Thực hành với R",
    "text": "23.3 Thực hành với R\nĐể thực hành xây dựng mô hình dự báo với H2O, chúng ta sẽ sử dụng dữ liệu có sẵn trong R - GermanCredit trong package caret.\nĐây là dữ liệu ghi nhận về lịch sử vay của khách hàng, với các 61 biến đầu vào cùng với biến đầu ra Class ghi nhận thực tế là các khoản vay đó có phải là khoản nợ xấu hay không. Chúng ta sẽ dự báo khoản vay nào không có nợ xấu.\n\n23.3.1 Lấy dữ liệu\n\n# Bước 1: Lấy dữ liệu\nlibrary(caret)\ndata(\"GermanCredit\")\n\n\n\n23.3.2 Kiểm tra dữ liệu\n\n# Bước 2: Kiểm tra dữ liệu\n# Kiểm tra định dạng các biến trong tập dữ liệu\nlibrary(dplyr)\nGermanCredit %>% str\n\n'data.frame':   1000 obs. of  62 variables:\n $ Duration                              : int  6 48 12 42 24 36 24 36 12 30 ...\n $ Amount                                : int  1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ...\n $ InstallmentRatePercentage             : int  4 2 2 2 3 2 3 2 2 4 ...\n $ ResidenceDuration                     : int  4 2 3 4 4 4 4 2 4 2 ...\n $ Age                                   : int  67 22 49 45 53 35 53 35 61 28 ...\n $ NumberExistingCredits                 : int  2 1 1 1 2 1 1 1 1 2 ...\n $ NumberPeopleMaintenance               : int  1 1 2 2 2 2 1 1 1 1 ...\n $ Telephone                             : num  0 1 1 1 1 0 1 0 1 1 ...\n $ ForeignWorker                         : num  1 1 1 1 1 1 1 1 1 1 ...\n $ Class                                 : Factor w/ 2 levels \"Bad\",\"Good\": 2 1 2 2 1 2 2 2 2 1 ...\n $ CheckingAccountStatus.lt.0            : num  1 0 0 1 1 0 0 0 0 0 ...\n $ CheckingAccountStatus.0.to.200        : num  0 1 0 0 0 0 0 1 0 1 ...\n $ CheckingAccountStatus.gt.200          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ CheckingAccountStatus.none            : num  0 0 1 0 0 1 1 0 1 0 ...\n $ CreditHistory.NoCredit.AllPaid        : num  0 0 0 0 0 0 0 0 0 0 ...\n $ CreditHistory.ThisBank.AllPaid        : num  0 0 0 0 0 0 0 0 0 0 ...\n $ CreditHistory.PaidDuly                : num  0 1 0 1 0 1 1 1 1 0 ...\n $ CreditHistory.Delay                   : num  0 0 0 0 1 0 0 0 0 0 ...\n $ CreditHistory.Critical                : num  1 0 1 0 0 0 0 0 0 1 ...\n $ Purpose.NewCar                        : num  0 0 0 0 1 0 0 0 0 1 ...\n $ Purpose.UsedCar                       : num  0 0 0 0 0 0 0 1 0 0 ...\n $ Purpose.Furniture.Equipment           : num  0 0 0 1 0 0 1 0 0 0 ...\n $ Purpose.Radio.Television              : num  1 1 0 0 0 0 0 0 1 0 ...\n $ Purpose.DomesticAppliance             : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Purpose.Repairs                       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Purpose.Education                     : num  0 0 1 0 0 1 0 0 0 0 ...\n $ Purpose.Vacation                      : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Purpose.Retraining                    : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Purpose.Business                      : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Purpose.Other                         : num  0 0 0 0 0 0 0 0 0 0 ...\n $ SavingsAccountBonds.lt.100            : num  0 1 1 1 1 0 0 1 0 1 ...\n $ SavingsAccountBonds.100.to.500        : num  0 0 0 0 0 0 0 0 0 0 ...\n $ SavingsAccountBonds.500.to.1000       : num  0 0 0 0 0 0 1 0 0 0 ...\n $ SavingsAccountBonds.gt.1000           : num  0 0 0 0 0 0 0 0 1 0 ...\n $ SavingsAccountBonds.Unknown           : num  1 0 0 0 0 1 0 0 0 0 ...\n $ EmploymentDuration.lt.1               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ EmploymentDuration.1.to.4             : num  0 1 0 0 1 1 0 1 0 0 ...\n $ EmploymentDuration.4.to.7             : num  0 0 1 1 0 0 0 0 1 0 ...\n $ EmploymentDuration.gt.7               : num  1 0 0 0 0 0 1 0 0 0 ...\n $ EmploymentDuration.Unemployed         : num  0 0 0 0 0 0 0 0 0 1 ...\n $ Personal.Male.Divorced.Seperated      : num  0 0 0 0 0 0 0 0 1 0 ...\n $ Personal.Female.NotSingle             : num  0 1 0 0 0 0 0 0 0 0 ...\n $ Personal.Male.Single                  : num  1 0 1 1 1 1 1 1 0 0 ...\n $ Personal.Male.Married.Widowed         : num  0 0 0 0 0 0 0 0 0 1 ...\n $ Personal.Female.Single                : num  0 0 0 0 0 0 0 0 0 0 ...\n $ OtherDebtorsGuarantors.None           : num  1 1 1 0 1 1 1 1 1 1 ...\n $ OtherDebtorsGuarantors.CoApplicant    : num  0 0 0 0 0 0 0 0 0 0 ...\n $ OtherDebtorsGuarantors.Guarantor      : num  0 0 0 1 0 0 0 0 0 0 ...\n $ Property.RealEstate                   : num  1 1 1 0 0 0 0 0 1 0 ...\n $ Property.Insurance                    : num  0 0 0 1 0 0 1 0 0 0 ...\n $ Property.CarOther                     : num  0 0 0 0 0 0 0 1 0 1 ...\n $ Property.Unknown                      : num  0 0 0 0 1 1 0 0 0 0 ...\n $ OtherInstallmentPlans.Bank            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ OtherInstallmentPlans.Stores          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ OtherInstallmentPlans.None            : num  1 1 1 1 1 1 1 1 1 1 ...\n $ Housing.Rent                          : num  0 0 0 0 0 0 0 1 0 0 ...\n $ Housing.Own                           : num  1 1 1 0 0 0 1 0 1 1 ...\n $ Housing.ForFree                       : num  0 0 0 1 1 1 0 0 0 0 ...\n $ Job.UnemployedUnskilled               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Job.UnskilledResident                 : num  0 0 1 0 0 1 0 0 1 0 ...\n $ Job.SkilledEmployee                   : num  1 1 0 1 1 0 1 0 0 0 ...\n $ Job.Management.SelfEmp.HighlyQualified: num  0 0 0 0 0 0 0 1 0 1 ...\n\n# Kiểm tra biến đầu ra (biến muốn dự báo)\nGermanCredit$Class %>% \n  table %>% \n  prop.table\n\n.\n Bad Good \n 0.3  0.7 \n\n\nNhư vậy, chúng ta thấy trong tập dữ liệu này:\n\n70% là các khoản vay không có nợ xấu\n30% là các khoản vay có nợ xấu\n\n\n\n23.3.3 Khởi động H2O\n\n# Bước 3: Khởi động H2O\nlibrary(h2o)\nh2o.init(ip = \"localhost\", \n         port = 54321, \n         nthreads= -1,\n         max_mem_size = \"4g\") # Đặt mức RAM tối đa\n\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         3 minutes 28 seconds \n    H2O cluster timezone:       UTC \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.40.0.1 \n    H2O cluster version age:    2 months and 9 days \n    H2O cluster name:           H2O_started_from_R_r110786_ocv184 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   3.80 GB \n    H2O cluster total cores:    1 \n    H2O cluster allowed cores:  1 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.2.3 (2023-03-15) \n\n\n\n\n23.3.4 Đưa dữ liệu vào H2O\nĐể đưa dữ liệu vào H2O, chúng ta sử dụng câu lệnh: as.h2o().\n\n# Bước 4: Đưa dữ liệu vào h2o\ndata_h2o <- as.h2o(GermanCredit)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\n23.3.5 Chia dữ liệu train/test\nChia tập dữ liệu thành 2 tập dữ liệu train và test (tỷ lệ 80-20)\n\n# Bước 5: Chia tập dữ liệu thành: train/test (tỷ lệ 80/20) \nset.seed(1234)\ndata_split = h2o.splitFrame(data = data_h2o, \n                            ratios = 0.8)\ntrain = data_split[[1]]\ntest = data_split[[2]]\n\n# Tập dữ liệu train (data frame)\ntrain_df <- as.data.frame(train)\ntrain_df$Class %>% table\n\n.\n Bad Good \n 235  551 \n\n# Tập dữ liệu test (data frame)\ntest_df <- as.data.frame(test)\ntest_df$Class %>% table\n\n.\n Bad Good \n  65  149 \n\n\n\n\n23.3.6 Xây dựng mô hình trên tập train\nĐể xây dựng mô hình dự báo với H2O, chúng ta có thể sử dụng những câu lệnh khác nhau tương ứng với từng phương pháp/thuật toán như: Logistic regression, Random Forests, Boosting…\n\nh2o.glm() - Logistic regression\nh2o.randomForest() - Random Forests\nh2o.gbm() - Boosting\n\n\n# Bước 6: Xây dựng mô hình trên train\nstart_time <- Sys.time()\nset.seed(1)\ngbm_model1 <- h2o.gbm(training_frame = train, # Tập dữ liệu train\n                      x = setdiff(names(GermanCredit), \"Class\"),  # Các biến đầu vào\n                      y = \"Class\" # Biến đầu ra (biến muốn dự báo)\n                      )\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%\n\nend_time <- Sys.time()\ntime <- end_time - start_time\ntime\n\nTime difference of 2.223689 secs\n\n\nChúng ta thấy việc xây dựng mô hình với H2O có tốc độ rất nhanh, xây dựng mô hình với các tham số cơ bản với dữ liệu này chỉ mất khoảng 2.22 giây. Đương nhiên trong trường hợp này dữ liệu nhỏ nên chúng ta chưa nhìn thấy rõ ưu điểm vượt trội của H2O về tốc độ, nhưng trong trường hơp dữ liệu lớn, việc sử dụng H2O sẽ rút ngắn thời gian xây dựng mô hình dự báo của chúng ta đi rất nhiều.\n\n# Summary model\ngbm_model1 %>% summary\n\nModel Details:\n==============\n\nH2OBinomialModel: gbm\nModel Key:  GBM_model_R_1681781433609_248 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              50                       50               13981         5\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1         5    5.00000          9         26    17.58000\n\nH2OBinomialMetrics: gbm\n** Reported on training data. **\n\nMSE:  0.07258735\nRMSE:  0.2694204\nLogLoss:  0.2676266\nMean Per-Class Error:  0.07134031\nAUC:  0.9794455\nAUCPR:  0.990769\nGini:  0.958891\nR^2:  0.6536728\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n       Bad Good    Error     Rate\nBad    210   25 0.106383  =25/235\nGood    20  531 0.036298  =20/551\nTotals 230  556 0.057252  =45/786\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.585928   0.959350 231\n2                       max f2  0.528659   0.973467 251\n3                 max f0point5  0.641146   0.964326 215\n4                 max accuracy  0.585928   0.942748 231\n5                max precision  0.985738   1.000000   0\n6                   max recall  0.301839   1.000000 324\n7              max specificity  0.985738   1.000000   0\n8             max absolute_mcc  0.585928   0.862683 231\n9   max min_per_class_accuracy  0.650274   0.932849 211\n10 max mean_per_class_accuracy  0.641146   0.936919 215\n11                     max tns  0.985738 235.000000   0\n12                     max fns  0.985738 550.000000   0\n13                     max fps  0.050169 235.000000 399\n14                     max tps  0.301839 551.000000 324\n15                     max tnr  0.985738   1.000000   0\n16                     max fnr  0.985738   0.998185   0\n17                     max fpr  0.050169   1.000000 399\n18                     max tpr  0.301839   1.000000 324\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n\n\nScoring History: \n            timestamp   duration number_of_trees training_rmse training_logloss\n1 2023-04-18 01:34:07  0.002 sec               0       0.45781          0.61000\n2 2023-04-18 01:34:07  0.069 sec               1       0.44489          0.58254\n3 2023-04-18 01:34:07  0.078 sec               2       0.43324          0.55877\n4 2023-04-18 01:34:07  0.085 sec               3       0.42313          0.53861\n5 2023-04-18 01:34:07  0.093 sec               4       0.41501          0.52240\n  training_auc training_pr_auc training_lift training_classification_error\n1      0.50000         0.70102       1.00000                       0.29898\n2      0.82314         0.91958       1.42650                       0.22774\n3      0.83729         0.92585       1.42650                       0.21374\n4      0.85373         0.93406       1.42650                       0.20738\n5      0.86354         0.93792       1.42650                       0.19338\n\n---\n             timestamp   duration number_of_trees training_rmse\n46 2023-04-18 01:34:08  0.990 sec              45       0.27706\n47 2023-04-18 01:34:08  0.997 sec              46       0.27555\n48 2023-04-18 01:34:08  1.006 sec              47       0.27383\n49 2023-04-18 01:34:08  1.073 sec              48       0.27225\n50 2023-04-18 01:34:08  1.081 sec              49       0.27105\n51 2023-04-18 01:34:08  1.089 sec              50       0.26942\n   training_logloss training_auc training_pr_auc training_lift\n46          0.27988      0.97668         0.98933       1.42650\n47          0.27727      0.97715         0.98965       1.42650\n48          0.27464      0.97787         0.99002       1.42650\n49          0.27199      0.97879         0.99049       1.42650\n50          0.27012      0.97924         0.99069       1.42650\n51          0.26763      0.97945         0.99077       1.42650\n   training_classification_error\n46                       0.06234\n47                       0.06107\n48                       0.05980\n49                       0.05980\n50                       0.05852\n51                       0.05725\n\nVariable Importances: (Extract with `h2o.varimp`) \n=================================================\n\nVariable Importances: \n                    variable relative_importance scaled_importance percentage\n1 CheckingAccountStatus.none           84.350624          1.000000   0.156410\n2                     Amount           82.419136          0.977102   0.152829\n3                   Duration           68.733696          0.814857   0.127452\n4                        Age           31.725632          0.376116   0.058828\n5             Purpose.NewCar           19.282278          0.228597   0.035755\n\n---\n                           variable relative_importance scaled_importance\n54                  Housing.ForFree            0.132180          0.001567\n55        Purpose.DomesticAppliance            0.000000          0.000000\n56               Purpose.Retraining            0.000000          0.000000\n57                    Purpose.Other            0.000000          0.000000\n58 Personal.Male.Divorced.Seperated            0.000000          0.000000\n59          Job.UnemployedUnskilled            0.000000          0.000000\n   percentage\n54   0.000245\n55   0.000000\n56   0.000000\n57   0.000000\n58   0.000000\n59   0.000000\n\n\nNhìn vào kết quả mô hình vừa được xây dựng trên tập train, chúng ta có thể thấy một số thông tin quan trọng sau:\n\nCác tham số (parameters) cơ bản của mô hình:\n\nnumber_of_trees = 50: Số lượng cây là 50\nmax_depth = 5: Mỗi cây có 5 tầng (mức độ phức tạp của mô hình)\n\nAUC trên tập train: 0.9794455\nDựa vào mục Confusion Matrix (tối ưu hóa F1-score) có thể tính toán được:\n\nerror rate (tỷ lệ quan sát dự báo sai): 45/786 = 0.057 (5.7%)\naccuracy (tỷ lệ quan sát dự báo đúng): 1 - 0.057 = 0.943 (94.3%)\nprecision: 531/551 = 0.96 (96%)\nrecall: 531/556 = 0.96 (96%)\n\nDựa vào mục Maximum Metrics, chúng ta có thể biết được các ngưỡng cutoff xác suất để tối ưu hóa các chỉ số như accuracy, precision, recall, f1-score…\n\nmax f1: Điểm cutoff xác suất là 0.585928\nmax accuracy: 0.585928\nmax recall: 0.301839\nmax precision: 0.985738\n\nDựa vào mục Variable Importances, chúng ta có thể biết được những biến nào có ảnh hưởng nhiều nhất đến biến đầu ra mà chúng ta muốn dự báo (có thể sử dụng câu lệnh h2o.varimp() để xem chi tiết). Trong trường hợp này, các biến có ảnh hưởng nhiều nhất tới biến Class theo thứ tự là:\n\nCheckingAccountStatus.none\nAmount\nDuration\nAge\nPurpose.NewCar\n\n\n\n# Kiểm tra chi tiết các biến có ảnh hướng nhiều tới biến dự báo trong mô hình\ngbm_model1 %>% \n  h2o.varimp() %>% \n  as.data.frame %>% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\nvariable\nrelative_importance\nscaled_importance\npercentage\n\n\n\n\nCheckingAccountStatus.none\n84.3506241\n1.0000000\n0.1564102\n\n\nAmount\n82.4191360\n0.9771017\n0.1528287\n\n\nDuration\n68.7336960\n0.8148570\n0.1274520\n\n\nAge\n31.7256317\n0.3761161\n0.0588284\n\n\nPurpose.NewCar\n19.2822781\n0.2285967\n0.0357549\n\n\nInstallmentRatePercentage\n15.5161829\n0.1839486\n0.0287714\n\n\nOtherInstallmentPlans.None\n15.2438316\n0.1807198\n0.0282664\n\n\nCreditHistory.Critical\n14.5524130\n0.1725229\n0.0269843\n\n\nCheckingAccountStatus.lt.0\n12.5769672\n0.1491034\n0.0233213\n\n\nProperty.RealEstate\n12.3567972\n0.1464933\n0.0229130\n\n\nSavingsAccountBonds.lt.100\n10.7499561\n0.1274437\n0.0199335\n\n\nOtherDebtorsGuarantors.Guarantor\n9.6545200\n0.1144570\n0.0179022\n\n\nProperty.Insurance\n9.3979931\n0.1114158\n0.0174266\n\n\nJob.SkilledEmployee\n8.7692375\n0.1039617\n0.0162607\n\n\nCheckingAccountStatus.gt.200\n8.7212677\n0.1033930\n0.0161717\n\n\nResidenceDuration\n8.4237700\n0.0998661\n0.0156201\n\n\nOtherDebtorsGuarantors.None\n7.9725552\n0.0945168\n0.0147834\n\n\nOtherInstallmentPlans.Bank\n7.7707009\n0.0921238\n0.0144091\n\n\nCreditHistory.NoCredit.AllPaid\n7.2156229\n0.0855432\n0.0133798\n\n\nPersonal.Male.Single\n7.0852146\n0.0839972\n0.0131380\n\n\nNumberExistingCredits\n6.5925212\n0.0781562\n0.0122244\n\n\nPurpose.UsedCar\n6.2361321\n0.0739311\n0.0115636\n\n\nPersonal.Female.NotSingle\n5.6832266\n0.0673762\n0.0105383\n\n\nEmploymentDuration.lt.1\n5.2887440\n0.0626995\n0.0098068\n\n\nEmploymentDuration.4.to.7\n5.2262311\n0.0619584\n0.0096909\n\n\nTelephone\n5.1227264\n0.0607313\n0.0094990\n\n\nProperty.Unknown\n4.8568892\n0.0575798\n0.0090061\n\n\nHousing.Rent\n4.7812662\n0.0566832\n0.0088658\n\n\nCreditHistory.Delay\n4.5384483\n0.0538046\n0.0084156\n\n\nSavingsAccountBonds.Unknown\n3.7545331\n0.0445110\n0.0069620\n\n\nEmploymentDuration.1.to.4\n3.6860566\n0.0436992\n0.0068350\n\n\nJob.Management.SelfEmp.HighlyQualified\n3.5472615\n0.0420538\n0.0065776\n\n\nPurpose.Education\n3.3089063\n0.0392280\n0.0061357\n\n\nEmploymentDuration.gt.7\n3.2702508\n0.0387697\n0.0060640\n\n\nHousing.Own\n3.0387659\n0.0360254\n0.0056347\n\n\nCreditHistory.PaidDuly\n2.8158870\n0.0333831\n0.0052215\n\n\nOtherDebtorsGuarantors.CoApplicant\n2.5130854\n0.0297933\n0.0046600\n\n\nNumberPeopleMaintenance\n2.4034374\n0.0284934\n0.0044567\n\n\nCreditHistory.ThisBank.AllPaid\n2.3900807\n0.0283351\n0.0044319\n\n\nCheckingAccountStatus.0.to.200\n2.0607431\n0.0244307\n0.0038212\n\n\nPurpose.Radio.Television\n2.0184691\n0.0239295\n0.0037428\n\n\nProperty.CarOther\n1.9168139\n0.0227244\n0.0035543\n\n\nPurpose.Furniture.Equipment\n1.8031244\n0.0213765\n0.0033435\n\n\nPurpose.Business\n1.5692081\n0.0186034\n0.0029098\n\n\nSavingsAccountBonds.gt.1000\n1.4338670\n0.0169989\n0.0026588\n\n\nEmploymentDuration.Unemployed\n1.3670616\n0.0162069\n0.0025349\n\n\nPurpose.Repairs\n1.2434046\n0.0147409\n0.0023056\n\n\nSavingsAccountBonds.100.to.500\n1.1172017\n0.0132447\n0.0020716\n\n\nOtherInstallmentPlans.Stores\n1.0155718\n0.0120399\n0.0018832\n\n\nSavingsAccountBonds.500.to.1000\n0.7910167\n0.0093777\n0.0014668\n\n\nForeignWorker\n0.5909842\n0.0070063\n0.0010959\n\n\nJob.UnskilledResident\n0.3838074\n0.0045501\n0.0007117\n\n\nPersonal.Male.Married.Widowed\n0.2746936\n0.0032566\n0.0005094\n\n\nHousing.ForFree\n0.1321795\n0.0015670\n0.0002451\n\n\nPurpose.DomesticAppliance\n0.0000000\n0.0000000\n0.0000000\n\n\nPurpose.Retraining\n0.0000000\n0.0000000\n0.0000000\n\n\nPurpose.Other\n0.0000000\n0.0000000\n0.0000000\n\n\nPersonal.Male.Divorced.Seperated\n0.0000000\n0.0000000\n0.0000000\n\n\nJob.UnemployedUnskilled\n0.0000000\n0.0000000\n0.0000000\n\n\n\n\n\n\n\n23.3.7 Dự báo trên tập test\n\n# Bước 7: Dự báo trên tập test\npred_test1 <- h2o.predict(gbm_model1, newdata = test) %>% \n  as.data.frame()\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npred_test1 %>% head\n\n  predict        Bad      Good\n1     Bad 0.71166405 0.2883359\n2     Bad 0.82454629 0.1754537\n3     Bad 0.49154935 0.5084507\n4     Bad 0.79388195 0.2061180\n5    Good 0.03446872 0.9655313\n6     Bad 0.76942563 0.2305744\n\n\nNhìn vào kết quả trên, ta có thể biết được những thông tin sau:\n\nCột predict: Là Class của khoản vay được dự báo\n\nGood: Khoản vay không phải là khoản nợ xấu\nBad: Khoản vay là khoản nợ xấu\n\nBad: Xác suất khoản vay là khoản nợ xấu\nGood: Xác suất khoản vay là khoản không có nợ xấu\n\n\n\n23.3.8 Đánh giá chất lượng dự báo trên test\n\n# Bước 8: Đánh giá chất lượng dự báo trên test\n# Confusion matrix\ntable(as.vector(pred_test1$predict), \n      test_df$Class,\n      dnn = c(\"Predicted\", \"Actual\")\n      )\n\n         Actual\nPredicted Bad Good\n     Bad   40   30\n     Good  25  119\n\n\nDựa vào confusion matrix trên tập test, chúng ta có thể tính toán được:\n\nAccuracy: (40+119)/214 = 0.74 (74%)\nError rate: (25+30)/186 = 1 - 0.74 = 0.26 (26%)\nPrecision: 119/(119+25) = 0.83 (83%)\nRecall: 119/(119+30) = 0.80 (80%)\n\n\nNote: Với H2O, chúng ta hoàn toàn có thể xây dựng mô hình trên tập train, rồi sau đó đánh giá chất lượng trên tập train và test đồng thời bằng một câu lệnh sau:\n\nset.seed(2)\ngbm_model2 <- h2o.gbm(training_frame = train, # Xây dựng trên train\n                      x = setdiff(names(GermanCredit), \"Class\"),  # Các biến đầu vào\n                      y = \"Class\", # Biến đầu ra (biến muốn dự báo)\n                      validation_frame = test # Đánh giá trên test\n                      )\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================================| 100%\n\ngbm_model2 %>% summary\n\nModel Details:\n==============\n\nH2OBinomialModel: gbm\nModel Key:  GBM_model_R_1681781433609_300 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              50                       50               13979         5\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1         5    5.00000          9         26    17.58000\n\nH2OBinomialMetrics: gbm\n** Reported on training data. **\n\nMSE:  0.07258735\nRMSE:  0.2694204\nLogLoss:  0.2676266\nMean Per-Class Error:  0.07134031\nAUC:  0.9794455\nAUCPR:  0.990769\nGini:  0.958891\nR^2:  0.6536728\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n       Bad Good    Error     Rate\nBad    210   25 0.106383  =25/235\nGood    20  531 0.036298  =20/551\nTotals 230  556 0.057252  =45/786\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.585928   0.959350 231\n2                       max f2  0.528659   0.973467 251\n3                 max f0point5  0.641146   0.964326 215\n4                 max accuracy  0.585928   0.942748 231\n5                max precision  0.985738   1.000000   0\n6                   max recall  0.301839   1.000000 324\n7              max specificity  0.985738   1.000000   0\n8             max absolute_mcc  0.585928   0.862683 231\n9   max min_per_class_accuracy  0.650274   0.932849 211\n10 max mean_per_class_accuracy  0.641146   0.936919 215\n11                     max tns  0.985738 235.000000   0\n12                     max fns  0.985738 550.000000   0\n13                     max fps  0.050169 235.000000 399\n14                     max tps  0.301839 551.000000 324\n15                     max tnr  0.985738   1.000000   0\n16                     max fnr  0.985738   0.998185   0\n17                     max fpr  0.050169   1.000000 399\n18                     max tpr  0.301839   1.000000 324\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nH2OBinomialMetrics: gbm\n** Reported on validation data. **\n\nMSE:  0.1745567\nRMSE:  0.4177998\nLogLoss:  0.5180404\nMean Per-Class Error:  0.4451729\nAUC:  0.7696438\nAUCPR:  0.8859651\nGini:  0.5392876\nR^2:  0.1746002\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n       Bad Good    Error     Rate\nBad      8   57 0.876923   =57/65\nGood     2  147 0.013423   =2/149\nTotals  10  204 0.275701  =59/214\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.230574   0.832861 203\n2                       max f2  0.175454   0.924318 209\n3                 max f0point5  0.643835   0.829662 132\n4                 max accuracy  0.579584   0.757009 148\n5                max precision  0.983556   1.000000   0\n6                   max recall  0.175454   1.000000 209\n7              max specificity  0.983556   1.000000   0\n8             max absolute_mcc  0.643835   0.427333 132\n9   max min_per_class_accuracy  0.705292   0.707692 124\n10 max mean_per_class_accuracy  0.643835   0.725348 132\n11                     max tns  0.983556  65.000000   0\n12                     max fns  0.983556 148.000000   0\n13                     max fps  0.082487  65.000000 213\n14                     max tps  0.175454 149.000000 209\n15                     max tnr  0.983556   1.000000   0\n16                     max fnr  0.983556   0.993289   0\n17                     max fpr  0.082487   1.000000 213\n18                     max tpr  0.175454   1.000000 209\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n\nScoring History: \n            timestamp   duration number_of_trees training_rmse training_logloss\n1 2023-04-18 01:34:09  0.001 sec               0       0.45781          0.61000\n2 2023-04-18 01:34:09  0.069 sec               1       0.44489          0.58254\n3 2023-04-18 01:34:09  0.077 sec               2       0.43324          0.55877\n4 2023-04-18 01:34:09  0.086 sec               3       0.42313          0.53861\n5 2023-04-18 01:34:09  0.168 sec               4       0.41501          0.52240\n  training_auc training_pr_auc training_lift training_classification_error\n1      0.50000         0.70102       1.00000                       0.29898\n2      0.82314         0.91958       1.42650                       0.22774\n3      0.83729         0.92585       1.42650                       0.21374\n4      0.85373         0.93406       1.42650                       0.20738\n5      0.86354         0.93792       1.42650                       0.19338\n  validation_rmse validation_logloss validation_auc validation_pr_auc\n1         0.45990            0.61405        0.50000           0.69626\n2         0.45392            0.60102        0.66598           0.80764\n3         0.45093            0.59421        0.66004           0.80140\n4         0.44796            0.58744        0.66324           0.80763\n5         0.44516            0.58122        0.67491           0.81993\n  validation_lift validation_classification_error\n1         1.00000                         0.30374\n2         1.10480                         0.30374\n3         1.07718                         0.29907\n4         1.07718                         0.28037\n5         0.95749                         0.26636\n\n---\n             timestamp   duration number_of_trees training_rmse\n46 2023-04-18 01:34:11  1.280 sec              45       0.27706\n47 2023-04-18 01:34:11  1.293 sec              46       0.27555\n48 2023-04-18 01:34:11  1.365 sec              47       0.27383\n49 2023-04-18 01:34:11  1.375 sec              48       0.27225\n50 2023-04-18 01:34:11  1.386 sec              49       0.27105\n51 2023-04-18 01:34:11  1.396 sec              50       0.26942\n   training_logloss training_auc training_pr_auc training_lift\n46          0.27988      0.97668         0.98933       1.42650\n47          0.27727      0.97715         0.98965       1.42650\n48          0.27464      0.97787         0.99002       1.42650\n49          0.27199      0.97879         0.99049       1.42650\n50          0.27012      0.97924         0.99069       1.42650\n51          0.26763      0.97945         0.99077       1.42650\n   training_classification_error validation_rmse validation_logloss\n46                       0.06234         0.41681            0.51647\n47                       0.06107         0.41697            0.51699\n48                       0.05980         0.41727            0.51749\n49                       0.05980         0.41718            0.51717\n50                       0.05852         0.41795            0.51830\n51                       0.05725         0.41780            0.51804\n   validation_auc validation_pr_auc validation_lift\n46        0.76964           0.88500         1.43624\n47        0.76944           0.88436         1.43624\n48        0.76820           0.88405         1.43624\n49        0.76923           0.88526         1.43624\n50        0.76758           0.88486         1.43624\n51        0.76964           0.88597         1.43624\n   validation_classification_error\n46                         0.28037\n47                         0.28037\n48                         0.28037\n49                         0.27570\n50                         0.27570\n51                         0.27570\n\nVariable Importances: (Extract with `h2o.varimp`) \n=================================================\n\nVariable Importances: \n                    variable relative_importance scaled_importance percentage\n1 CheckingAccountStatus.none           84.350624          1.000000   0.156410\n2                     Amount           82.419136          0.977102   0.152829\n3                   Duration           68.733696          0.814857   0.127452\n4                        Age           31.725632          0.376116   0.058828\n5             Purpose.NewCar           19.282278          0.228597   0.035755\n\n---\n                           variable relative_importance scaled_importance\n54                  Housing.ForFree            0.132180          0.001567\n55        Purpose.DomesticAppliance            0.000000          0.000000\n56               Purpose.Retraining            0.000000          0.000000\n57                    Purpose.Other            0.000000          0.000000\n58 Personal.Male.Divorced.Seperated            0.000000          0.000000\n59          Job.UnemployedUnskilled            0.000000          0.000000\n   percentage\n54   0.000245\n55   0.000000\n56   0.000000\n57   0.000000\n58   0.000000\n59   0.000000\n\n\n\n\n23.3.9 Tuning\nTuning là quá trình tìm kiếm tổ hợp các tham số để tối ưu hóa mô hình (chất lượng dự báo tốt nhất có thể).\n\n# Bước 9: Tuning\n# 9.1. Set hyper parameters\nhyper_params = list( \n  ## restrict the search to the range of max_depth established above\n  max_depth = c(5:9),                                      \n  \n  ## search a large space of row sampling rates per tree\n  sample_rate = seq(0.5,1,0.05),                                             \n  \n  ## search a large space of column sampling rates per split\n  col_sample_rate = seq(0.5,1,0.05),                                  \n\n  ## search a large space of the number of min rows in a terminal node\n  min_rows = 2^seq(0,log2(nrow(train))-1,1),                                 \n  \n  ## search a large space of the number of bins for split-finding for continuous and integer columns\n  nbins = 2^seq(4,10,1),                                                     \n  \n  ## search a large space of the number of bins for split-finding for categorical columns\n  nbins_cats = 2^seq(4,12,1),                                                \n  \n  ## search a few minimum required relative error improvement thresholds for a split to happen\n  min_split_improvement = c(0,1e-8,1e-6,1e-4),                               \n  \n  ## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)\n  histogram_type = c(\"UniformAdaptive\",\"QuantilesGlobal\",\"RoundRobin\")      \n)\n\n# 9.2. Set search criteria\nsearch_criteria = list(\n  ## Random grid search\n  strategy = \"RandomDiscrete\",      \n  \n  ## limit the runtime to xx minutes\n  max_runtime_secs = 10,         \n  \n  ## build no more than xx models\n  max_models = 5,                  \n  \n  ## random number generator seed to make sampling of parameter combinations reproducible\n  seed = 1234,                        \n  \n  ## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference\n  stopping_rounds = 5,                \n  stopping_metric = \"AUC\",\n  stopping_tolerance = 1e-3\n)\n\n# 9.3. Grid search\ngrid <- h2o.grid(\n  ## hyper parameters\n  hyper_params = hyper_params,\n  \n  ## hyper-parameter search configuration (see above)\n  search_criteria = search_criteria,\n  \n  ## which algorithm to run\n  algorithm = \"gbm\",\n  \n  ## identifier for the grid, to later retrieve it\n  grid_id = \"model_id\", \n\n  ## standard model parameters\n  x = setdiff(names(GermanCredit), \"Class\"), \n  y = \"Class\", \n  training_frame = train, \n  validation_frame = test,\n  #balance_classes = T, #Option to solve imbalanced classification\n  \n  ## more trees is better if the learning rate is small enough\n  ## use \"more than enough\" trees - we have early stopping\n  ntrees = 200,                                                            \n  \n  ## smaller learning rate is better\n  ## since we have learning_rate_annealing, we can afford to start with a bigger learning rate\n  learn_rate = 0.05,                                                         \n  \n  ## learning rate annealing: learning_rate shrinks by 1% after every tree \n  ## (use 1.00 to disable, but then lower the learning_rate)\n  learn_rate_annealing = 0.99,                                               \n  \n  ## early stopping based on timeout (no model should take more than 1 hour - modify as needed)\n  max_runtime_secs = 30,                                                 \n  \n  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events\n  stopping_rounds = 5, stopping_tolerance = 1e-3, stopping_metric = \"AUC\", \n  \n  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)\n  score_tree_interval = 10,                                                \n  \n  ## base random number generator seed for each model (automatically gets incremented internally for each model)\n  seed = 1234                                                             \n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\n\n# 9.4. Get models\nsortedGrid <- h2o.getGrid(\"model_id\", sort_by = \"auc\", decreasing = TRUE)    \nsortedGrid\n\nH2O Grid Details\n================\n\nGrid ID: model_id \nUsed hyper parameters: \n  -  col_sample_rate \n  -  histogram_type \n  -  max_depth \n  -  min_rows \n  -  min_split_improvement \n  -  nbins \n  -  nbins_cats \n  -  sample_rate \nNumber of models: 7 \nNumber of failed models: 0 \n\nHyper-Parameter Search Summary: ordered by decreasing auc\n  col_sample_rate  histogram_type max_depth min_rows min_split_improvement\n1         0.80000 UniformAdaptive   7.00000  1.00000               0.00000\n2         0.80000 UniformAdaptive   7.00000  8.00000               0.00010\n3         0.80000 UniformAdaptive   7.00000  1.00000               0.00000\n4         0.75000 QuantilesGlobal   5.00000 32.00000               0.00000\n5         0.75000 QuantilesGlobal   5.00000 32.00000               0.00000\n6         0.85000      RoundRobin   8.00000  1.00000               0.00010\n7         0.85000      RoundRobin   8.00000  1.00000               0.00010\n      nbins nbins_cats sample_rate        model_ids     auc\n1 512.00000 2048.00000     0.55000 model_id_model_6 0.77656\n2  16.00000   16.00000     1.00000 model_id_model_7 0.77408\n3 512.00000 2048.00000     0.55000 model_id_model_3 0.77315\n4  64.00000  512.00000     0.50000 model_id_model_2 0.77233\n5  64.00000  512.00000     0.50000 model_id_model_5 0.77233\n6  16.00000  256.00000     0.55000 model_id_model_1 0.76386\n7  16.00000  256.00000     0.55000 model_id_model_4 0.76386\n\n# 9.5. Get best model\ngbm <- h2o.getModel(sortedGrid@model_ids[[1]])\n\n# Parameters of the best model\ngbm@parameters\n\n$model_id\n[1] \"model_id_model_6\"\n\n$training_frame\n[1] \"RTMP_sid_9333_3\"\n\n$validation_frame\n[1] \"RTMP_sid_9333_5\"\n\n$score_tree_interval\n[1] 10\n\n$ntrees\n[1] 190\n\n$max_depth\n[1] 7\n\n$min_rows\n[1] 1\n\n$nbins\n[1] 512\n\n$nbins_cats\n[1] 2048\n\n$stopping_rounds\n[1] 5\n\n$stopping_metric\n[1] \"AUC\"\n\n$max_runtime_secs\n[1] 3.775\n\n$seed\n[1] 1234\n\n$learn_rate\n[1] 0.05\n\n$learn_rate_annealing\n[1] 0.99\n\n$distribution\n[1] \"bernoulli\"\n\n$sample_rate\n[1] 0.55\n\n$col_sample_rate\n[1] 0.8\n\n$min_split_improvement\n[1] 1e-08\n\n$histogram_type\n[1] \"UniformAdaptive\"\n\n$categorical_encoding\n[1] \"Enum\"\n\n$calibration_method\n[1] \"PlattScaling\"\n\n$x\n [1] \"Duration\"                              \n [2] \"Amount\"                                \n [3] \"InstallmentRatePercentage\"             \n [4] \"ResidenceDuration\"                     \n [5] \"Age\"                                   \n [6] \"NumberExistingCredits\"                 \n [7] \"NumberPeopleMaintenance\"               \n [8] \"Telephone\"                             \n [9] \"ForeignWorker\"                         \n[10] \"CheckingAccountStatus.lt.0\"            \n[11] \"CheckingAccountStatus.0.to.200\"        \n[12] \"CheckingAccountStatus.gt.200\"          \n[13] \"CheckingAccountStatus.none\"            \n[14] \"CreditHistory.NoCredit.AllPaid\"        \n[15] \"CreditHistory.ThisBank.AllPaid\"        \n[16] \"CreditHistory.PaidDuly\"                \n[17] \"CreditHistory.Delay\"                   \n[18] \"CreditHistory.Critical\"                \n[19] \"Purpose.NewCar\"                        \n[20] \"Purpose.UsedCar\"                       \n[21] \"Purpose.Furniture.Equipment\"           \n[22] \"Purpose.Radio.Television\"              \n[23] \"Purpose.DomesticAppliance\"             \n[24] \"Purpose.Repairs\"                       \n[25] \"Purpose.Education\"                     \n[26] \"Purpose.Retraining\"                    \n[27] \"Purpose.Business\"                      \n[28] \"Purpose.Other\"                         \n[29] \"SavingsAccountBonds.lt.100\"            \n[30] \"SavingsAccountBonds.100.to.500\"        \n[31] \"SavingsAccountBonds.500.to.1000\"       \n[32] \"SavingsAccountBonds.gt.1000\"           \n[33] \"SavingsAccountBonds.Unknown\"           \n[34] \"EmploymentDuration.lt.1\"               \n[35] \"EmploymentDuration.1.to.4\"             \n[36] \"EmploymentDuration.4.to.7\"             \n[37] \"EmploymentDuration.gt.7\"               \n[38] \"EmploymentDuration.Unemployed\"         \n[39] \"Personal.Male.Divorced.Seperated\"      \n[40] \"Personal.Female.NotSingle\"             \n[41] \"Personal.Male.Single\"                  \n[42] \"Personal.Male.Married.Widowed\"         \n[43] \"OtherDebtorsGuarantors.None\"           \n[44] \"OtherDebtorsGuarantors.CoApplicant\"    \n[45] \"OtherDebtorsGuarantors.Guarantor\"      \n[46] \"Property.RealEstate\"                   \n[47] \"Property.Insurance\"                    \n[48] \"Property.CarOther\"                     \n[49] \"Property.Unknown\"                      \n[50] \"OtherInstallmentPlans.Bank\"            \n[51] \"OtherInstallmentPlans.Stores\"          \n[52] \"OtherInstallmentPlans.None\"            \n[53] \"Housing.Rent\"                          \n[54] \"Housing.Own\"                           \n[55] \"Housing.ForFree\"                       \n[56] \"Job.UnemployedUnskilled\"               \n[57] \"Job.UnskilledResident\"                 \n[58] \"Job.SkilledEmployee\"                   \n[59] \"Job.Management.SelfEmp.HighlyQualified\"\n\n$y\n[1] \"Class\"\n\n# Summary best model\ngbm %>% summary\n\nModel Details:\n==============\n\nH2OBinomialModel: gbm\nModel Key:  model_id_model_6 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1             190                      190              154044         7\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1         7    7.00000         37         90    59.70526\n\nH2OBinomialMetrics: gbm\n** Reported on training data. **\n\nMSE:  0.02755389\nRMSE:  0.1659936\nLogLoss:  0.1418825\nMean Per-Class Error:  0.003035101\nAUC:  0.9998533\nAUCPR:  0.9999372\nGini:  0.9997065\nR^2:  0.8685355\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n       Bad Good    Error    Rate\nBad    234    1 0.004255  =1/235\nGood     1  550 0.001815  =1/551\nTotals 235  551 0.002545  =2/786\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.562943   0.998185 224\n2                       max f2  0.562943   0.998185 224\n3                 max f0point5  0.562943   0.998185 224\n4                 max accuracy  0.562943   0.997455 224\n5                max precision  0.989264   1.000000   0\n6                   max recall  0.519195   1.000000 231\n7              max specificity  0.989264   1.000000   0\n8             max absolute_mcc  0.562943   0.993930 224\n9   max min_per_class_accuracy  0.600199   0.995745 223\n10 max mean_per_class_accuracy  0.562943   0.996965 224\n11                     max tns  0.989264 235.000000   0\n12                     max fns  0.989264 550.000000   0\n13                     max fps  0.044299 235.000000 399\n14                     max tps  0.519195 551.000000 231\n15                     max tnr  0.989264   1.000000   0\n16                     max fnr  0.989264   0.998185   0\n17                     max fpr  0.044299   1.000000 399\n18                     max tpr  0.519195   1.000000 231\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nH2OBinomialMetrics: gbm\n** Reported on validation data. **\n\nMSE:  0.1704193\nRMSE:  0.4128187\nLogLoss:  0.5140953\nMean Per-Class Error:  0.3946825\nAUC:  0.7765617\nAUCPR:  0.8842813\nGini:  0.5531234\nR^2:  0.1941639\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n       Bad Good    Error     Rate\nBad     15   50 0.769231   =50/65\nGood     3  146 0.020134   =3/149\nTotals  18  196 0.247664  =53/214\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.294319   0.846377 195\n2                       max f2  0.265576   0.928482 200\n3                 max f0point5  0.629027   0.820896 146\n4                 max accuracy  0.572423   0.757009 158\n5                max precision  0.986755   1.000000   0\n6                   max recall  0.132550   1.000000 210\n7              max specificity  0.986755   1.000000   0\n8             max absolute_mcc  0.629027   0.408635 146\n9   max min_per_class_accuracy  0.742643   0.692308 123\n10 max mean_per_class_accuracy  0.731343   0.708570 127\n11                     max tns  0.986755  65.000000   0\n12                     max fns  0.986755 148.000000   0\n13                     max fps  0.091295  65.000000 213\n14                     max tps  0.132550 149.000000 210\n15                     max tnr  0.986755   1.000000   0\n16                     max fnr  0.986755   0.993289   0\n17                     max fpr  0.091295   1.000000 213\n18                     max tpr  0.132550   1.000000 210\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n\nScoring History: \n             timestamp   duration number_of_trees training_rmse\n1  2023-04-18 01:34:18  6.228 sec               0       0.45781\n2  2023-04-18 01:34:18  6.423 sec              10       0.38021\n3  2023-04-18 01:34:18  6.542 sec              20       0.33012\n4  2023-04-18 01:34:18  6.641 sec              30       0.29559\n5  2023-04-18 01:34:18  6.735 sec              40       0.27103\n6  2023-04-18 01:34:19  6.832 sec              50       0.25164\n7  2023-04-18 01:34:19  6.933 sec              60       0.23528\n8  2023-04-18 01:34:19  7.036 sec              70       0.22403\n9  2023-04-18 01:34:19  7.145 sec              80       0.21344\n10 2023-04-18 01:34:19  7.235 sec              90       0.20578\n11 2023-04-18 01:34:19  7.325 sec             100       0.19796\n12 2023-04-18 01:34:19  7.418 sec             110       0.19256\n13 2023-04-18 01:34:19  7.542 sec             120       0.18836\n14 2023-04-18 01:34:19  7.638 sec             130       0.18327\n15 2023-04-18 01:34:19  7.737 sec             140       0.17954\n16 2023-04-18 01:34:20  7.832 sec             150       0.17647\n17 2023-04-18 01:34:20  7.923 sec             160       0.17356\n18 2023-04-18 01:34:20  8.022 sec             170       0.17036\n19 2023-04-18 01:34:20  8.121 sec             180       0.16797\n20 2023-04-18 01:34:20  8.214 sec             190       0.16599\n   training_logloss training_auc training_pr_auc training_lift\n1           0.61000      0.50000         0.70102       1.00000\n2           0.45947      0.96683         0.98507       1.42650\n3           0.37361      0.98343         0.99259       1.42650\n4           0.31846      0.99070         0.99579       1.42650\n5           0.28075      0.99421         0.99734       1.42650\n6           0.25232      0.99597         0.99820       1.42650\n7           0.22911      0.99682         0.99859       1.42650\n8           0.21353      0.99772         0.99901       1.42650\n9           0.19968      0.99841         0.99931       1.42650\n10          0.18953      0.99878         0.99947       1.42650\n11          0.17986      0.99908         0.99961       1.42650\n12          0.17317      0.99919         0.99965       1.42650\n13          0.16761      0.99928         0.99969       1.42650\n14          0.16165      0.99944         0.99976       1.42650\n15          0.15721      0.99957         0.99981       1.42650\n16          0.15372      0.99963         0.99984       1.42650\n17          0.15041      0.99970         0.99987       1.42650\n18          0.14672      0.99975         0.99989       1.42650\n19          0.14401      0.99980         0.99992       1.42650\n20          0.14188      0.99985         0.99994       1.42650\n   training_classification_error validation_rmse validation_logloss\n1                        0.29898         0.45990            0.61405\n2                        0.08524         0.43380            0.55706\n3                        0.06107         0.42038            0.52747\n4                        0.04453         0.41787            0.52099\n5                        0.03181         0.41491            0.51435\n6                        0.02417         0.41453            0.51243\n7                        0.02163         0.41477            0.51231\n8                        0.01908         0.41367            0.51055\n9                        0.01527         0.41347            0.51122\n10                       0.01527         0.41203            0.50889\n11                       0.01145         0.41213            0.50923\n12                       0.01018         0.41182            0.50903\n13                       0.00891         0.41179            0.50945\n14                       0.00763         0.41150            0.50925\n15                       0.00763         0.41181            0.51021\n16                       0.00636         0.41169            0.51012\n17                       0.00382         0.41213            0.51182\n18                       0.00382         0.41218            0.51246\n19                       0.00382         0.41278            0.51395\n20                       0.00254         0.41282            0.51410\n   validation_auc validation_pr_auc validation_lift\n1         0.50000           0.69626         1.00000\n2         0.74734           0.88451         1.43624\n3         0.76851           0.88894         1.43624\n4         0.76489           0.88515         1.43624\n5         0.77078           0.88528         1.43624\n6         0.77202           0.88675         1.43624\n7         0.77481           0.88866         1.43624\n8         0.77584           0.88846         1.43624\n9         0.77739           0.88715         1.43624\n10        0.77904           0.88849         1.43624\n11        0.77801           0.88722         1.43624\n12        0.78007           0.88798         1.43624\n13        0.78152           0.88848         1.43624\n14        0.78286           0.88893         1.43624\n15        0.78080           0.88712         1.43624\n16        0.78090           0.88723         1.43624\n17        0.77873           0.88623         1.43624\n18        0.77894           0.88555         1.43624\n19        0.77728           0.88529         1.43624\n20        0.77656           0.88428         1.43624\n   validation_classification_error\n1                          0.30374\n2                          0.28972\n3                          0.27103\n4                          0.26636\n5                          0.25234\n6                          0.25234\n7                          0.25701\n8                          0.27570\n9                          0.26636\n10                         0.26168\n11                         0.25701\n12                         0.25701\n13                         0.26168\n14                         0.25701\n15                         0.25701\n16                         0.25234\n17                         0.25234\n18                         0.24766\n19                         0.24766\n20                         0.24766\n\nVariable Importances: (Extract with `h2o.varimp`) \n=================================================\n\nVariable Importances: \n                    variable relative_importance scaled_importance percentage\n1                     Amount          332.317505          1.000000   0.157845\n2                        Age          204.717499          0.616030   0.097237\n3                   Duration          190.941040          0.574574   0.090694\n4 CheckingAccountStatus.none          184.693298          0.555774   0.087726\n5          ResidenceDuration           78.698425          0.236817   0.037380\n\n---\n                    variable relative_importance scaled_importance percentage\n54           Purpose.Repairs            8.946022          0.026920   0.004249\n55 Purpose.DomesticAppliance            6.825329          0.020539   0.003242\n56   Job.UnemployedUnskilled            6.578030          0.019794   0.003124\n57             ForeignWorker            6.199419          0.018655   0.002945\n58        Purpose.Retraining            3.083785          0.009280   0.001465\n59             Purpose.Other            1.568497          0.004720   0.000745\n\n\n\n\n23.3.10 Lưu và load mô hình\nĐể lưu lại mô hình trên H2O, chúng ta sử dụng câu lệnh h2o.saveModel().\n\n# Lưu model\nh2o.saveModel(gbm, # Tên mô hình\n              path = getwd(), # Đường dẫn muốn lưu\n              force = T)\n\nĐể load mô hình trên H2O, chúng ta sử dụng câu lệnh h2o.loadModel().\n\nh2o.shutdown(prompt = F)\n\n\nNhư vậy, trong chương này, chúng ta được học cách xây dựng mô hình dự báo với H2O. Đây là một platform rất mạnh giúp cải thiện chất lượng dự báo, tốc độ xử lý, tối ưu hóa mô hình, cũng như hiển thị kết quả mô hình một cách rất hiệu quả."
  },
  {
    "objectID": "p03-07-xay-dung-mo-hinh-voi-h2o.html#tài-liệu-tham-khảo",
    "href": "p03-07-xay-dung-mo-hinh-voi-h2o.html#tài-liệu-tham-khảo",
    "title": "23  Xây dựng mô hình với H2O",
    "section": "23.4 Tài liệu tham khảo",
    "text": "23.4 Tài liệu tham khảo\n\nhttps://github.com/h2oai/h2o-tutorials"
  },
  {
    "objectID": "p03-08-xgboost.html#giới-thiệu",
    "href": "p03-08-xgboost.html#giới-thiệu",
    "title": "24  Extreme Gradient Boosting",
    "section": "24.1 Giới thiệu",
    "text": "24.1 Giới thiệu\nXGBoost, viết tắt của Extreme Gradient Boosting, là thuật toán thuộc nhóm cây quyết định áp dụng gradient boosting để tối ưu hóa mô hình. xgboost có thuật toán gần tương tự với gbm nhuwnng tập trung nhiều hơn vào regularization. Do đó, mô hình xgboost hạn chế được vấn đề overfitting trong xây dựng mô hình.\nKhác với dữ liệu đầu vào thông thường, xgboost yêu cầu dữ liệu đầu vào như sau.\n\nChỉ dự báo với biến số - do đó các biến factor phải chuyển sang sang số\nDữ liệu đầu vào của XGBoost chia làm các dạng:\n\nDense matrix: Lưu trữ tất cả dữ liệu dưới dạng matrix\nSparse matrix: Chỉ lưu dữ liệu khác 0.\nxgb.DMatrix: Định dạng dữ liệu riêng của xgboost - được khuyến nghị sử dụng trong xây dựng mô hình.\n\n\n\nlibrary(tidyverse)\nlibrary(ISLR)\nlibrary(rsample)\nlibrary(xgboost)\n\n\nx <- c(rep(NA, 3), rnorm(3, 4), rep(0,10))\n# Dense Matrix\nm1 <- matrix(x, nrow = 4, ncol = 4)\n# Sparse Matrix\nm2 <- m1 %>% as(\"sparseMatrix\")\nm2\n\n4 x 4 sparse Matrix of class \"dgCMatrix\"\n                         \n[1,]      NA 2.217692 . .\n[2,]      NA 4.521284 . .\n[3,]      NA .        . .\n[4,] 2.97539 .        . .\n\n# xgb.DMatrix\nm3 <- xgb.DMatrix(data = m1, label = c(1,0,1,0))\nm3 %>% class\n\n[1] \"xgb.DMatrix\"\n\n# dgCMatrix\nm4 <- list(data = m1 %>% as(\"dgCMatrix\"), label = c(1,0,1,0))\nm4\n\n$data\n4 x 4 sparse Matrix of class \"dgCMatrix\"\n                         \n[1,]      NA 2.217692 . .\n[2,]      NA 4.521284 . .\n[3,]      NA .        . .\n[4,] 2.97539 .        . .\n\n$label\n[1] 1 0 1 0\n\n\nKhi xây dựng mô hình, xgboost có 3 nhóm hàm xây dựng mô hình:\n\nxgboost: Xây dựng mô hình đơn giản\nxgb.cv: Xây dựng mô hình với cross-validation, hỗ trợ tuning\nxgb.train: Xây dựng mô hình với các tính năng nâng cao như đánh giá chất lượng trên tập test,… Hàm này bắt buộc dữ liệu đầu vào ở dạng xgb.Dmatrix"
  },
  {
    "objectID": "p03-08-xgboost.html#tham-số-trong-mô-hình",
    "href": "p03-08-xgboost.html#tham-số-trong-mô-hình",
    "title": "24  Extreme Gradient Boosting",
    "section": "24.2 Tham số trong mô hình",
    "text": "24.2 Tham số trong mô hình\n\n24.2.1 Các tham số cơ bản\n\nbooster[default = gbtree]: Thuật toán sử dụng trong xgboost.\n\ngbtree: Sử dụng cây quyết định\ngblinear: Sử dụng hàm tuyến tính\n\n\nVới bài toán phân loại, sử dụng gbtree. Với bài toán hồi quy, có thể sử dụng bất kỳ thuật toán nào.\n\nnthread: Số lượng cores được sử dụng khi chạy parallel computing.\nsilent[default = 1]: Hiển thị kết quả chạy mô hình trên console\n\n\n\n24.2.2 Tham số cho tree booster\n\nnrounds[default=100]: Số lượt iteration khi xây boosting, đây chính là số cây quyết định được tạo trong mô hình. Cần tối ưu qua xgb.cv\neta[default=0.3][range: (0,1)]: Tốc độ learning rate, giá trị càng nhỏ thì thời gian tính toán càng lâu. Khi giá trị của eta thấp, ta cần tăng số lượng nrounds để đạt giá trị tối ưu. eta thường sử dụng trong khoảng 0.01-0.3\ngamma[default=0][range: (0,Inf)]: Tham số hỗ trợ regularization để giảm thiểu overfitting. Giá trị gamma càng lớn, sai số giữa tập train và test càng thấp. Khi sử dụng xgb.cv, nếu sai số giữa train và test lớn, ta cần đưa tham số gamma vào mô hình. gamma hoạt động tốt với các cây quyết định đơn giản (max_depth có giá trị nhỏ). Một node sẽ chỉ được tách tiếp nếu loss function được giảm một lượng tối thiểu gamma\nmax_depth[default=6][range: (0,Inf)]: Tham số quyết định số tầng của cây. Giá trị càng lớn, khả năng overfitting càng lớn. Tham số cần được tối ưu qua xgb.cv\nmin_child_weight[default=1][range:(0,Inf)]: Số lượng quan sát tối thiểu cho mỗi node\nsubsample[default=1][range: (0,1)]: Tỷ lệ quan sát có trong mỗi cây quyết định. Thông thường có giá trị trong khoảng (0.5-0.8)\ncolsample_bytree[default=1][range: (0,1)]: Tỷ lệ số biến được sử dụng trong cây quyết định, thường có giá trị trong khoảng (0.5,0.9)\nlambda[default=0]: L2 regularization\nalpha[default=1]: L1 regularization\n\n\n\n24.2.3 Tham số cho Linear Booster\ngblinear có ít tham số tuning hơn gbtree rất nhiều. Do đó, với các bài toán hồi quy, ta nên sử dụng để tăng tốc độ tuning\n\nnrounds[default=100]\nlambda[default=0]\nalpha[default=1]\n\n\n\n24.2.4 Tham số cho các bài toán khác nhau\nVới các bài toán khác nhau, tham số mục tiêu trong mô hình cũng sẽ thay đổi theo.\n\nobjective = \"binary:logistic\": Phân loại 2 nhóm\nobjective = \"multi:softmax\": Phân loại nhiều nhóm, kết quả trả ra nhãn (label) của mỗi class. Yêu cầu có thêm tham số num_class\nobjective = \"multi:softprob\": Phân loại nhiều nhóm, trả ra xác suất của mỗi nhóm\nobjective = \"reg:linear: Mô hình dự báo regression\n\nCác tham số đánh giá chất lượng mô hình khi tuning\n\nmae – Mean Absolute Error (regression)\nlogloss – Negative loglikelihood (classification)\nauc – Area under curve (classification)\nrmse – Root mean square error (regression)\nerror – Binary classification error rate [#wrong cases/#all cases]\nmlogloss – multiclass logloss (classification)"
  },
  {
    "objectID": "p03-08-xgboost.html#sử-dụng-xgboost-cơ-bản",
    "href": "p03-08-xgboost.html#sử-dụng-xgboost-cơ-bản",
    "title": "24  Extreme Gradient Boosting",
    "section": "24.3 Sử dụng xgboost cơ bản",
    "text": "24.3 Sử dụng xgboost cơ bản\n\n24.3.1 Phân loại 2 nhóm\n\ndata(\"Default\")\nDefault %>% head\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\n\nDữ liệu đầu vào của xgboost cần phải đảm bảo hai yếu tố:\n\nToàn bộ dữ liệu cần có định dạng số\nDữ liệu đầu vào dạng matrix\n\nDo đó, toàn bộ dữ liệu factor phải biến đổi thành matrix như sau\n\nconvert_fct <- function(x){\n  x <- x %>% as.numeric\n  x <- x - 1\n  return(x)\n}\n\nDefault <- Default %>% \n  modify_if(is.factor, convert_fct) \nDefault %>% summary\n\n    default          student          balance           income     \n Min.   :0.0000   Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 481.7   1st Qu.:21340  \n Median :0.0000   Median :0.0000   Median : 823.6   Median :34553  \n Mean   :0.0333   Mean   :0.2944   Mean   : 835.4   Mean   :33517  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1166.3   3rd Qu.:43808  \n Max.   :1.0000   Max.   :1.0000   Max.   :2654.3   Max.   :73554  \n\n\n\nindex <- sample(1:nrow(Default), 0.75 * nrow(Default))\n\n# Chia tập train test\ntrain <- Default[index, ]\ntest <- Default[-index, ]\n\ntrain %>% str\n\n'data.frame':   7500 obs. of  4 variables:\n $ default: num  0 0 0 0 0 0 0 0 0 0 ...\n $ student: num  0 0 0 0 0 0 1 0 1 0 ...\n $ balance: num  1011 1060 153 817 557 ...\n $ income : num  36299 27870 39503 41995 35067 ...\n\n\nTraining model\n\n# Cách 1: Biến đổi dữ liệu trực tiếp matrix từ DataFrame\nxgb_model <- xgboost(data = train %>% select(-default) %>% as.matrix,\n                     label = train %>% pull(default),\n                     max_depth = 2, \n                     eta = 0.1, #step size of each boosting step = control the learning rate\n                     nrounds = 5,\n                     objective = \"binary:logistic\") #binary classification model\n\n[1] train-logloss:0.606049 \n[2] train-logloss:0.534740 \n[3] train-logloss:0.475348 \n[4] train-logloss:0.425241 \n[5] train-logloss:0.382491 \n\nxgb_model\n\n##### xgb.Booster\nraw: 8.5 Kb \ncall:\n  xgb.train(params = params, data = dtrain, nrounds = nrounds, \n    watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, \n    early_stopping_rounds = early_stopping_rounds, maximize = maximize, \n    save_period = save_period, save_name = save_name, xgb_model = xgb_model, \n    callbacks = callbacks, max_depth = 2, eta = 0.1, objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  max_depth = \"2\", eta = \"0.1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.print.evaluation(period = print_every_n)\n  cb.evaluation.log()\n# of features: 3 \nniter: 5\nnfeatures : 3 \nevaluation_log:\n iter train_logloss\n    1     0.6060489\n    2     0.5347401\n    3     0.4753482\n    4     0.4252406\n    5     0.3824908\n\n\n\n# Cách 2: Biến đổi thành Dmatrix\ndtrain <- xgb.DMatrix(data = train %>% \n                        select(-default) %>% as.matrix,\n                      label = train %>% pull(default) %>% as.matrix)\n\ndtest <- xgb.DMatrix(data = test %>% select(-default) %>% as.matrix,\n                      label = test %>% pull(default) %>% as.matrix)\n\ngetinfo(dtrain, \"label\") %>% head\n\n[1] 0 0 0 0 0 0\n\nxgb_model2 <- xgb.train(data = dtrain,\n                        nround = 20,\n                        watchlist = list(train = dtrain, test = dtest),\n                        objective = \"binary:logistic\")\n\n[1] train-logloss:0.456465  test-logloss:0.461732 \n[2] train-logloss:0.325561  test-logloss:0.335189 \n[3] train-logloss:0.243478  test-logloss:0.257027 \n[4] train-logloss:0.188583  test-logloss:0.206339 \n[5] train-logloss:0.151419  test-logloss:0.171470 \n[6] train-logloss:0.124873  test-logloss:0.148238 \n[7] train-logloss:0.106079  test-logloss:0.131726 \n[8] train-logloss:0.092768  test-logloss:0.119828 \n[9] train-logloss:0.083067  test-logloss:0.112201 \n[10]    train-logloss:0.076175  test-logloss:0.107035 \n[11]    train-logloss:0.071114  test-logloss:0.103486 \n[12]    train-logloss:0.066510  test-logloss:0.101269 \n[13]    train-logloss:0.063671  test-logloss:0.099681 \n[14]    train-logloss:0.061429  test-logloss:0.098411 \n[15]    train-logloss:0.059548  test-logloss:0.097536 \n[16]    train-logloss:0.057891  test-logloss:0.097047 \n[17]    train-logloss:0.056649  test-logloss:0.096697 \n[18]    train-logloss:0.055824  test-logloss:0.096437 \n[19]    train-logloss:0.054968  test-logloss:0.096052 \n[20]    train-logloss:0.054453  test-logloss:0.096174 \n\n\n\n#Dự báo\n\npred_result <- predict(xgb_model2, test %>% select(-default) %>% as.matrix)\nresult <- data.frame(pred = pred_result, \n                     actual = test$default)\nresult %>% head\n\n         pred actual\n1 0.001644940      0\n2 0.010656979      0\n3 0.001531548      0\n4 0.002682617      0\n5 0.072526902      0\n6 0.022804208      0\n\n\n\n# Variable importance\nimportant_matrix <- xgb.importance(model = xgb_model2) \nimportant_matrix\n\n   Feature        Gain       Cover Frequency\n1: balance 0.842198921 0.830537424 0.5420354\n2:  income 0.150944068 0.164183260 0.4380531\n3: student 0.006857012 0.005279316 0.0199115\n\nxgb.plot.importance(important_matrix)\n\n\n\n\nxgboost croos validation\n\nxgb_cv_model <- xgb.cv(\n  data = train %>% select(-default) %>% as.matrix,\n  label = train %>% pull(default),\n  nfold = 5,\n  nrounds = 20,\n  objective = \"binary:logistic\"\n)\n\n[1] train-logloss:0.456443+0.000674 test-logloss:0.459251+0.001795 \n[2] train-logloss:0.325654+0.001169 test-logloss:0.331319+0.003312 \n[3] train-logloss:0.243380+0.001407 test-logloss:0.251464+0.004232 \n[4] train-logloss:0.188807+0.001659 test-logloss:0.198382+0.005424 \n[5] train-logloss:0.151177+0.001841 test-logloss:0.162590+0.006475 \n[6] train-logloss:0.124632+0.001843 test-logloss:0.138449+0.007058 \n[7] train-logloss:0.105713+0.001924 test-logloss:0.121676+0.007789 \n[8] train-logloss:0.091863+0.002035 test-logloss:0.109712+0.008631 \n[9] train-logloss:0.081819+0.002160 test-logloss:0.101353+0.009198 \n[10]    train-logloss:0.074494+0.002175 test-logloss:0.095321+0.009602 \n[11]    train-logloss:0.068936+0.002422 test-logloss:0.091329+0.010128 \n[12]    train-logloss:0.064834+0.002410 test-logloss:0.088636+0.010678 \n[13]    train-logloss:0.061750+0.002462 test-logloss:0.086801+0.011021 \n[14]    train-logloss:0.059222+0.002469 test-logloss:0.085496+0.011246 \n[15]    train-logloss:0.057350+0.002357 test-logloss:0.084681+0.011565 \n[16]    train-logloss:0.055885+0.002238 test-logloss:0.083926+0.011783 \n[17]    train-logloss:0.054735+0.002181 test-logloss:0.083789+0.011906 \n[18]    train-logloss:0.053752+0.002185 test-logloss:0.083563+0.011913 \n[19]    train-logloss:0.052808+0.002437 test-logloss:0.083544+0.011947 \n[20]    train-logloss:0.052005+0.002477 test-logloss:0.083590+0.012147 \n\nxgb_cv_model\n\n##### xgb.cv 5-folds\n iter train_logloss_mean train_logloss_std test_logloss_mean test_logloss_std\n    1         0.45644301      0.0006739002        0.45925147      0.001795369\n    2         0.32565427      0.0011686685        0.33131930      0.003312023\n    3         0.24338015      0.0014065094        0.25146354      0.004231658\n    4         0.18880739      0.0016591077        0.19838212      0.005423796\n    5         0.15117685      0.0018410891        0.16259027      0.006475489\n    6         0.12463196      0.0018429609        0.13844943      0.007058480\n    7         0.10571266      0.0019244415        0.12167644      0.007789193\n    8         0.09186278      0.0020353153        0.10971164      0.008630608\n    9         0.08181907      0.0021598219        0.10135303      0.009197664\n   10         0.07449379      0.0021750061        0.09532147      0.009602407\n   11         0.06893600      0.0024216393        0.09132926      0.010127932\n   12         0.06483435      0.0024103745        0.08863641      0.010677939\n   13         0.06174974      0.0024623895        0.08680146      0.011021055\n   14         0.05922165      0.0024687004        0.08549554      0.011246297\n   15         0.05735000      0.0023574078        0.08468061      0.011564853\n   16         0.05588468      0.0022383212        0.08392588      0.011782927\n   17         0.05473464      0.0021811777        0.08378939      0.011905952\n   18         0.05375206      0.0021849861        0.08356349      0.011913141\n   19         0.05280787      0.0024371889        0.08354444      0.011947281\n   20         0.05200490      0.0024765581        0.08359039      0.012146617\n\n\n\n\n24.3.2 Phân loại nhiều nhóm\n\niris <- iris %>% mutate(label = as.numeric(Species) - 1)\n\n#Chia tập train/test\nset.seed(1)\ntrain <- sample(nrow(iris), 0.7 * nrow(iris))  \niris_train <- iris[train,]            \niris_test <- iris[-train,]      \n\n#Đổi định dạng sang xgb.DMatrix\ndtrain <- xgb.DMatrix(data = iris_train[,1:4] %>% as.matrix, \n                      label = iris_train[,6])\ndtest <- xgb.DMatrix(data = iris_test[,1:4] %>% as.matrix, \n                     label = iris_test[,6])\n\n#Build model trên train\nxgb_model <- xgb.train(data=dtrain, \n                 nround = 20, \n                 watchlist = list(train=dtrain, test=dtest),\n                 eval.metric = \"merror\", \n                 objective = \"multi:softmax\", #multiclass\n                 num_class = 3)               # số lượng class\n\n[1] train-merror:0.019048   test-merror:0.000000 \n[2] train-merror:0.019048   test-merror:0.000000 \n[3] train-merror:0.019048   test-merror:0.000000 \n[4] train-merror:0.019048   test-merror:0.000000 \n[5] train-merror:0.019048   test-merror:0.000000 \n[6] train-merror:0.009524   test-merror:0.022222 \n[7] train-merror:0.009524   test-merror:0.022222 \n[8] train-merror:0.000000   test-merror:0.044444 \n[9] train-merror:0.000000   test-merror:0.044444 \n[10]    train-merror:0.000000   test-merror:0.044444 \n[11]    train-merror:0.000000   test-merror:0.044444 \n[12]    train-merror:0.000000   test-merror:0.044444 \n[13]    train-merror:0.000000   test-merror:0.044444 \n[14]    train-merror:0.000000   test-merror:0.044444 \n[15]    train-merror:0.000000   test-merror:0.044444 \n[16]    train-merror:0.000000   test-merror:0.044444 \n[17]    train-merror:0.000000   test-merror:0.044444 \n[18]    train-merror:0.000000   test-merror:0.044444 \n[19]    train-merror:0.000000   test-merror:0.044444 \n[20]    train-merror:0.000000   test-merror:0.044444 \n\npred <- predict(xgb_model, dtest)\n# Confusion matrix\ntable(pred, getinfo(dtest, \"label\"))\n\n    \npred  0  1  2\n   0 15  0  0\n   1  0 17  2\n   2  0  0 11\n\n\n\n\n24.3.3 Mô hình hồi quy\n\nlibrary(mlbench)\ndata(\"BostonHousing\")\n\ndf <- BostonHousing %>% \n  mutate(chas = as.numeric(chas)) # Fixing factor\nindex <- sample(1:nrow(BostonHousing), 0.75*nrow(BostonHousing))\ntrain <- df[index, ]\ntest <- df[-index, ]\n\n# Tạo hàm convert\nxgb_matrix_convert <- function(df, y){\n  y <- enquo(y)\n  xgb_matrix <- xgb.DMatrix(data = df %>% \n                        select(-!!y) %>% \n                        as.matrix %>% \n                        as(\"dgCMatrix\"),\n                      label = df %>%\n                        pull(!!y))\n  return(xgb_matrix)\n}\n\ndtrain <- xgb_matrix_convert(train, medv)\ndtest <- test %>% xgb_matrix_convert(medv)\n\n\n# Xây mô hình\nxgb_reg <- xgboost::xgb.train(\n                  data=dtrain,\n                  watchlist = list(train = dtrain, test = dtest),\n                   max_depth=3, \n                   eta = 0.2, \n                   nthread=3, \n                   nrounds=40, \n                   lambda=0,\n                   objective=\"reg:linear\")\n\n[14:38:20] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[1] train-rmse:19.309819    test-rmse:18.937997 \n[2] train-rmse:15.606620    test-rmse:15.234527 \n[3] train-rmse:12.638811    test-rmse:12.341616 \n[4] train-rmse:10.270188    test-rmse:10.178778 \n[5] train-rmse:8.403936 test-rmse:8.555597 \n[6] train-rmse:6.920763 test-rmse:7.133534 \n[7] train-rmse:5.768784 test-rmse:6.140970 \n[8] train-rmse:4.855680 test-rmse:5.344052 \n[9] train-rmse:4.150053 test-rmse:4.837382 \n[10]    train-rmse:3.609667 test-rmse:4.362998 \n[11]    train-rmse:3.200611 test-rmse:4.098615 \n[12]    train-rmse:2.894428 test-rmse:3.873524 \n[13]    train-rmse:2.671378 test-rmse:3.725358 \n[14]    train-rmse:2.497225 test-rmse:3.605675 \n[15]    train-rmse:2.366818 test-rmse:3.525180 \n[16]    train-rmse:2.237165 test-rmse:3.467244 \n[17]    train-rmse:2.160299 test-rmse:3.405528 \n[18]    train-rmse:2.068731 test-rmse:3.340547 \n[19]    train-rmse:2.009169 test-rmse:3.306505 \n[20]    train-rmse:1.959114 test-rmse:3.269817 \n[21]    train-rmse:1.896273 test-rmse:3.262797 \n[22]    train-rmse:1.859122 test-rmse:3.247868 \n[23]    train-rmse:1.819388 test-rmse:3.225070 \n[24]    train-rmse:1.784970 test-rmse:3.206565 \n[25]    train-rmse:1.750531 test-rmse:3.187188 \n[26]    train-rmse:1.728462 test-rmse:3.183269 \n[27]    train-rmse:1.686153 test-rmse:3.169784 \n[28]    train-rmse:1.676003 test-rmse:3.165701 \n[29]    train-rmse:1.659468 test-rmse:3.157377 \n[30]    train-rmse:1.620732 test-rmse:3.150728 \n[31]    train-rmse:1.588564 test-rmse:3.147539 \n[32]    train-rmse:1.561644 test-rmse:3.136729 \n[33]    train-rmse:1.548848 test-rmse:3.133808 \n[34]    train-rmse:1.533705 test-rmse:3.128936 \n[35]    train-rmse:1.527019 test-rmse:3.127589 \n[36]    train-rmse:1.507101 test-rmse:3.120439 \n[37]    train-rmse:1.475981 test-rmse:3.109688 \n[38]    train-rmse:1.455215 test-rmse:3.104411 \n[39]    train-rmse:1.434109 test-rmse:3.102678 \n[40]    train-rmse:1.411514 test-rmse:3.099896 \n\nxgb_reg\n\n##### xgb.Booster\nraw: 48.8 Kb \ncall:\n  xgboost::xgb.train(data = dtrain, nrounds = 40, watchlist = list(train = dtrain, \n    test = dtest), max_depth = 3, eta = 0.2, nthread = 3, lambda = 0, \n    objective = \"reg:linear\")\nparams (as set within xgb.train):\n  max_depth = \"3\", eta = \"0.2\", nthread = \"3\", lambda = \"0\", objective = \"reg:linear\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.print.evaluation(period = print_every_n)\n  cb.evaluation.log()\n# of features: 13 \nniter: 40\nnfeatures : 13 \nevaluation_log:\n    iter train_rmse test_rmse\n       1  19.309819 18.937997\n       2  15.606620 15.234527\n---                          \n      39   1.434109  3.102678\n      40   1.411514  3.099896\n\npred <- predict(xgb_reg, dtest)\n# Error RMSE\nmean((pred - test$medv)^2) %>% sqrt %>% round(2)\n\n[1] 3.1"
  },
  {
    "objectID": "p03-08-xgboost.html#tuning-xgboost",
    "href": "p03-08-xgboost.html#tuning-xgboost",
    "title": "24  Extreme Gradient Boosting",
    "section": "24.4 Tuning xgboost",
    "text": "24.4 Tuning xgboost\nKhi mới xây dựng mô hình xgboost, ta có thể sử dụng các tham số sau:\n\neta: Quyết định learning_rate, thử với 0.01\nnrounds: 100 nếu dữ liệu lớn, 1000 nếu dữ liệu nhỏ\nmax_depth: 3\nsubsample: 0.8\ncolsample_bytree: 1\ngamma: 1\n\nCác bước tuning như sau:\n\nTuning nrounds để xác định số cây quyết định\nTuning max_depth, bắt đầu với 3 và tăng dần lên, dừng lại nếu mô hình không tốt hơn trên tập test\nlearning_rate: Thử giá trị trong khoảng 0.01 đến 0.1\nsubsample: Thử các giá trị trong khoảng [0.8, 1]\ncolsample_bytree: Thử giá trị trong khoảng [0.3, 0.8] để tránh 1 biến ảnh hưởng quá nhiều đến chất lượng mô hình\ngamma: Thử giá trị [0,1,5]\n\nTa sẽ tuning với tập dữ liệu BostonHousing\n\ndf <- BostonHousing %>% \n  mutate(chas = as.numeric(chas)) # Fixing factor\nindex <- sample(1:nrow(BostonHousing), 0.75*nrow(BostonHousing))\ntrain <- df[index, ]\ntest <- df[-index, ]\n\n# Tạo hàm convert\nxgb_matrix_convert <- function(df, y){\n  y <- enquo(y)\n  xgb_matrix <- xgb.DMatrix(data = df %>% \n                        select(-!!y) %>% \n                        as.matrix %>% \n                        as(\"dgCMatrix\"),\n                      label = df %>%\n                        pull(!!y))\n  return(xgb_matrix)\n}\n\ndtrain <- xgb_matrix_convert(train, medv)\ndtest <- test %>% xgb_matrix_convert(medv)\n\n\n# Xây mô hình\nxgb_reg <- xgboost::xgb.train(\n                  data=dtrain,\n                  watchlist = list(train = dtrain, test = dtest),\n                   max_depth=3, \n                   eta = 0.01, \n                   nrounds=1000, \n                   objective=\"reg:linear\", \n                  verbose = F,\n                  eval = \"rmse\")\n\n[14:38:21] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:21] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\nxgb_reg$evaluation_log %>% \n  head\n\n   iter train_rmse test_rmse\n1:    1   23.75130  23.32698\n2:    2   23.52634  23.10579\n3:    3   23.30340  22.88668\n4:    4   23.08271  22.66982\n5:    5   22.86451  22.45537\n6:    6   22.64825  22.24294\n\nlibrary(reshape2)\nxgb_reg$evaluation_log %>% \n  melt(id.vars = \"iter\") %>% \n  ggplot(aes(iter, value)) +\n  geom_line(aes(col = variable)) +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\n\n\n\nNhư vậy, số lượng cây (nrounds) sẽ rơi vào khoảng 300\nBước tiếp theo, ta sẽ tuning với từng tham số.\n\n# Tune max_depth\nxgb_eval <- function(max_depth = 3,\n                     eta = 0.01,\n                     nrounds = 300,\n                     subsample = 0.8,\n                     colsample_bytree = 0.8,\n                     gamma = 0) {\n  xgb_model <- xgb.train(\n    data = dtrain,\n    watchlist = list(train = dtrain, test = dtest),\n    max_depth = max_depth,\n    eta = eta,\n    nrounds = nrounds,\n    subsample = subsample,\n    colsample_bytree = colsample_bytree,\n    gamma = gamma,\n    objective = \"reg:linear\",\n    verbose = F,\n    eval = \"rmse\"\n  )\n  return(xgb_model$evaluation_log %>% tail(1))\n}\n\nxgb_eval(nrounds = 10)\n\n[14:38:22] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:22] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n\n   iter train_rmse test_rmse\n1:   10   21.81913  21.45133\n\nmax_depth <- c(3:10)\nmap_df(max_depth, \n       as_mapper(~xgb_eval(max_depth = .x))) %>% \n  mutate(max_depth = max_depth) %>% \n  select(-1)\n\n[14:38:22] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:22] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:22] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:22] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:22] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:22] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:22] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:22] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:23] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:23] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:23] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:23] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:23] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:23] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:24] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:24] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n\n   train_rmse test_rmse max_depth\n1:   2.802225  4.111569         3\n2:   2.497150  4.000477         4\n3:   2.285728  3.856733         5\n4:   2.145971  3.777291         6\n5:   2.091390  3.825581         7\n6:   2.068180  3.888360         8\n7:   2.004094  3.819973         9\n8:   2.005576  3.857061        10\n\n\n\nNhư vậy, max_depth tối ưu trong trường hợp này là 8\n\n\n# Tune eta\neta <- seq(0.01, 0.2, by = 0.02)\neta_tune <- map_df(eta, \n       as_mapper(~xgb_eval(eta = .x, \n                           max_depth = 8))) %>% \n  mutate(eta = eta) %>% \n  select(-1)\n\n[14:38:24] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:24] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:24] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:24] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:24] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:24] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:25] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:25] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:25] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:25] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:25] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:25] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:26] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:26] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:26] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:26] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:26] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:26] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:26] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:26] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\neta_tune\n\n      train_rmse test_rmse  eta\n 1: 2.0460189144  3.808014 0.01\n 2: 0.2471150349  3.245818 0.03\n 3: 0.0554097094  3.540259 0.05\n 4: 0.0154485226  3.381197 0.07\n 5: 0.0033025644  3.554486 0.09\n 6: 0.0008227960  3.404650 0.11\n 7: 0.0006471857  3.327971 0.13\n 8: 0.0006024772  3.344477 0.15\n 9: 0.0006011283  3.488997 0.17\n10: 0.0005846677  3.435446 0.19\n\n\n\nNhư vậy, eta tối ưu trong trường hợp này là 0.05\n\n\n# Tune subsample\nsubsample <- seq(0.5, 0.8, by = 0.05)\nsubsample_tune <- map_df(subsample, \n       as_mapper(~xgb_eval(eta = 0.05, \n                           max_depth = 8,\n                           subsample = .x))) %>% \n  mutate(subsample = subsample) %>% \n  select(-1)\n\n[14:38:27] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:27] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:27] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:27] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:27] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:27] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:28] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:28] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:28] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:28] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:28] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:28] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:28] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:28] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\nsubsample_tune\n\n   train_rmse test_rmse subsample\n1: 0.26278404  3.588669      0.50\n2: 0.18770359  3.800652      0.55\n3: 0.15636033  3.517310      0.60\n4: 0.11577811  3.456728      0.65\n5: 0.08067518  3.276463      0.70\n6: 0.08803140  3.476305      0.75\n7: 0.05803862  3.545669      0.80\n\n\nNhư vậy, subsample tối ưu trong trường hợp này là 0.55\n\n#Tune colsample_bytree\n\ncolsample_bytree <- seq(0.7, 1, by = 0.05)\ncolsample_bytree_tune <- map_df(colsample_bytree, \n       as_mapper(~xgb_eval(eta = 0.05, \n                           max_depth = 8,\n                           subsample = 0.55,\n                           colsample_bytree = .x))) %>% \n  mutate(colsample_bytree = colsample_bytree) %>% \n  select(-1)\n\n[14:38:29] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:29] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:29] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:29] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:29] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:29] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:30] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:30] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:30] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:30] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:30] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:30] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:30] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:30] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\ncolsample_bytree_tune\n\n   train_rmse test_rmse colsample_bytree\n1:  0.1574834  3.507313             0.70\n2:  0.1897105  3.611232             0.75\n3:  0.1825235  3.382111             0.80\n4:  0.1724446  3.432489             0.85\n5:  0.1727087  3.643824             0.90\n6:  0.2043766  3.531467             0.95\n7:  0.1738617  3.383100             1.00\n\n\n\nNhư vậy, colsample_bytree tối ưu là 0.8\n\n\n#Tune colsample_bytree\n\ngamma<- seq(0, 10, by = 1)\ngamma_tune <- map_df(gamma, \n       as_mapper(~xgb_eval(eta = 0.05, \n                           max_depth = 8,\n                           subsample = 0.55,\n                           colsample_bytree = 0.8,\n                           gamma = .x))) %>% \n  mutate(gamma = gamma) %>% \n  select(-1)\n\n[14:38:31] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:31] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:31] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:31] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:31] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:31] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:32] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:32] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:32] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:32] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:32] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:32] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:33] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:33] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:33] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:33] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:33] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:33] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:33] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:33] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\n[14:38:34] WARNING: src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n[14:38:34] WARNING: src/learner.cc:767: \nParameters: { \"eval\" } are not used.\n\ngamma_tune\n\n    train_rmse test_rmse gamma\n 1:  0.1859322  3.562346     0\n 2:  0.4218689  3.669980     1\n 3:  0.5615026  3.706800     2\n 4:  0.6593507  3.520479     3\n 5:  0.7416174  3.356820     4\n 6:  0.7990263  3.589631     5\n 7:  0.8801037  3.756084     6\n 8:  0.9363509  3.595034     7\n 9:  0.9876672  3.735096     8\n10:  1.0422657  3.572406     9\n11:  1.1036199  3.533579    10\n\n\nMô hình cuối cùng sẽ như sau.\n\nxgb_reg <- xgb.train(\n  data = dtrain,\n  watchlist = list(train = dtrain,\n                   test = dtest),\n  eta = 0.05,\n  nrounds = 300,\n  max_depth = 8,\n  subsample = 0.55,\n  colsample_bytree = 0.8,\n  gamma = 5,\n  verbose = F\n)\nxgb_reg\n\n##### xgb.Booster\nraw: 995.7 Kb \ncall:\n  xgb.train(data = dtrain, nrounds = 300, watchlist = list(train = dtrain, \n    test = dtest), verbose = F, eta = 0.05, max_depth = 8, subsample = 0.55, \n    colsample_bytree = 0.8, gamma = 5)\nparams (as set within xgb.train):\n  eta = \"0.05\", max_depth = \"8\", subsample = \"0.55\", colsample_bytree = \"0.8\", gamma = \"5\", validate_parameters = \"1\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 13 \nniter: 300\nnfeatures : 13 \nevaluation_log:\n    iter train_rmse test_rmse\n       1 22.8594592 22.467018\n       2 21.7925064 21.412278\n---                          \n     299  0.8118169  3.472679\n     300  0.8118075  3.472565\n\n# Sai số mô hình\npred <- predict(xgb_reg, dtest)\n# Error RMSE\nmean((pred - test$medv)^2) %>% sqrt\n\n[1] 3.472565"
  },
  {
    "objectID": "p03-08-xgboost.html#tài-liệu-tham-khảo",
    "href": "p03-08-xgboost.html#tài-liệu-tham-khảo",
    "title": "24  Extreme Gradient Boosting",
    "section": "24.5 Tài liệu tham khảo",
    "text": "24.5 Tài liệu tham khảo\n\nGiới thiệu về xgboost\nhttps://github.com/dmlc/xgboost/tree/master/demo\nParameters xgboost\nTuning xgboost"
  },
  {
    "objectID": "p03-10-knn.html#giới-thiệu",
    "href": "p03-10-knn.html#giới-thiệu",
    "title": "25  K nearest neighbors",
    "section": "25.1 Giới thiệu",
    "text": "25.1 Giới thiệu\nKNN (K-nearest neighbors) là mô hình học máy phi tham số, so sánh các quan sát trên tập test với các quan sát trên tập train gần nhất, từ đó đưa ra kết quả dự báo dựa trên k quan sát gần nhất.\nThuật toán sẽ diễn ra như sau:\n\nXác định giá trị tham số K (số láng giềng gần nhất)\nTính khoảng cách giữa quan sát (đối tượng) cần phân lớp (Query Point) với tất cả các đối tượng trong training data\nSắp xếp khoảng cách theo thứ tự tăng dần và xác định K láng giềng gần nhất với Query Point\nLấy tất cả các lớp của K láng giềng gần nhất đã xác định\nDựa vào phần lớn lớp của láng giềng gần nhất để xác định lớp cho Query Point\n\n\nTrong ví dụ trên, ta áp dụng knn với k = 3 tương ứng với việc so sánh quan sát cần dự báo với 3 quan sát khác gần nhất. Trong 3 quan sát này, có 2 quan sát thuộc Class B và 1 quan sát thuộc Class A. Do đó, quan sát cần dự báo thuộc nhóm B.\nCách tính khoảng cách: Khoảng cách giữa các điểm trên KNN có thể tính toán một cách đơn giản bằng cách tính khoảng cách Euclide.\n\\[d = \\sqrt{\\sum_{x_i\\neq x_0}(x_i - x_0)^2}\\]\nTrong đó:\n\n\\(x_0\\): Quan sát cần dự báo\n\\(x_i\\): Các quan sát từ tập train\n\nƯu điểm của KNN:\n\nĐơn giản trong quá trình tính toán, việc dự báo rất đơn giản và có thể giải thích được\nKhông cần đưa ra giả thuyết về phân phối của các class.\n\nNhược điểm của KNN:\n\nNhạy cảm với nhiễu.\nĐòi hỏi bộ nhớ và khối lương tính toán rất lớn khi tập dữ liệu train và test lớn. Thêm vào đó, thuật toán đòi hỏi phải lưu trữ toàn bộ tập train. Đây là nhược điểm rất lớn khi dữ liệu từ tập train lên đến hàng triệu quan sát.\n\nLưu ý khi thực hiện kNN: Khi xây dựng kNN trong thực tế, ta cần phải thực hiện các bước biến đổi dữ liệu sau:\n\nChuyển đổi các biến category thành biến số\nScale dữ liệu để tránh bị ảnh hưởng bời đơn vị khác nhau giữa các biến.\nknn có thể mở rộng để áp dụng cho bài toán regression\n\nỨng dụng: kNN có thể áp dụng rất tốt khi ta chưa có nhiều thông tin về quan sát/đối tượng cần dự báo. Ví dụ, khách hàng sau khi mới đăng ký, ta có rất ít thông tin về hành vi giao dịch mà chủ yếu chỉ có các thông tin về demographics, 1 số sản phẩm đơn giản. Lúc này kNN có thể kết hợp cùng kmeans với k=1 để dự báo nhóm đặc trưng hành vi của khách hàng mới."
  },
  {
    "objectID": "p03-10-knn.html#thực-hành",
    "href": "p03-10-knn.html#thực-hành",
    "title": "25  K nearest neighbors",
    "section": "25.2 Thực hành",
    "text": "25.2 Thực hành\n\n25.2.1 Mô hình cơ bản\n\nlibrary(dplyr)\nlibrary(ISLR)\ndata <- Default\ndata %>% head\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\n# Biến đổi dữ liệu\n\ndata <- data %>% \n  mutate(student = student %>% as.numeric %>% scale,\n         balance = balance %>% scale,\n         income = income %>% scale)\ndata %>% summary\n\n default         student.V1          balance.V1           income.V1      \n No :9667   Min.   :-0.6459039   Min.   :-1.726998   Min.   :-2.4552672  \n Yes: 333   1st Qu.:-0.6459039   1st Qu.:-0.731099   1st Qu.:-0.9130125  \n            Median :-0.6459039   Median :-0.024266   Median : 0.0776555  \n            Mean   : 0.0000000   Mean   : 0.000000   Mean   : 0.0000000  \n            3rd Qu.: 1.5480631   3rd Qu.: 0.684150   3rd Qu.: 0.7716147  \n            Max.   : 1.5480631   Max.   : 3.760371   Max.   : 3.0020495  \n\nnames(data) <- names(Default)\n\n# Chia train test\nset.seed(1234)\nindex <- sample(1:10000, size = 8000)\ntrain <- data[index, ]\ntest <- data[-index, ]\n\n# Xây mô hình\nlibrary(FNN)\nknn_model <- knn(train = train %>% select(2:4), \n                 test = test %>% select(2:4), \n                 cl = train$default, k = 2)\n\nresult <- table(knn_model, test$default)\nresult\n\n         \nknn_model   No  Yes\n      No  1924   52\n      Yes   11   13\n\n(result[1,2] + result[2,1])/sum(result)\n\n[1] 0.0315\n\n\n\n\n25.2.2 Tuning tham số k\n\nlibrary(purrr)\nlibrary(gghighlight)\n\nmodel_k <- function(k=2){\n  knn_model <- knn(train = train %>% select(2:4), \n                 test = test %>% select(2:4), \n                 cl = train$default, k = k)\n  result <- table(knn_model, test$default)\n  error <- (result[1,2] + result[2,1])/sum(result)\n  df <- data.frame(k = k, error = error)\n  return(df)\n}\n\nresult <- map_df(1:13, model_k)\nresult %>% \n  ggplot(aes(k, error)) +\n  geom_line() +\n  geom_point(size = 3) +\n  geom_point(data = result %>% \n               filter(error == min(result$error)),\n             col = \"darkred\",\n             size = 5) +\n  theme_minimal() +\n  labs(title = \"Error on test data by k neighbors\")\n\n\n\n\nNhư vậy, với tập dữ liệu trên, số lượng k tối ưu nên là k=8."
  },
  {
    "objectID": "p03-10-knn.html#tài-liệu-tham-khảo",
    "href": "p03-10-knn.html#tài-liệu-tham-khảo",
    "title": "25  K nearest neighbors",
    "section": "25.3 Tài liệu tham khảo",
    "text": "25.3 Tài liệu tham khảo\n\nChapter 7 - kNN classifier - Data Mining for Business Analytics with R"
  },
  {
    "objectID": "p03-11-svm.html#giới-thiệu",
    "href": "p03-11-svm.html#giới-thiệu",
    "title": "26  Support Vector Machine",
    "section": "26.1 Giới thiệu",
    "text": "26.1 Giới thiệu\nSupport Vector Machine (SVM) là một thuật toán thuộc nhóm học máy có định hướng (Supervised Learning). Thuật toán này được sử dụng từ cuối những năm 1990s, đầu 2000 trong việc dự báo.\nSVM xây dựng một mặt phẳng giúp phân cách giữa giữa hai nhóm. Mặt phằng được sử dụng phân loại hai nhóm được gọi là hyperplane, các điểm gần nhất với mặt phẳng được gọi là support vector. Khoảng cách giữa hyperplane và các support vector được gọi là margin. Mặt phẳng tối ưu (optimum hyperplance) là mặt phẳng có khoảng cách đến các điểm support vector (margin) là lớn nhất.\n\nLưu ý: Với cùng 1 tập dữ liệu, có thể có 1 hoặc nhiều mặt phẳng (hyperplane) giúp phân loại các quan sát với kết quả tốt như nhau. Tuy nhiên, hyperplane tốt nhất là hyperplane có margin lớn hơn. Xem ví dụ dưới đây.\nVí dụ 1: SVM chưa tối ưu - margin chưa đạt khoảng cách tối đa \nVí dụ 2: SVM tối ưu - margin đạt khoảng cách tối đa"
  },
  {
    "objectID": "p03-11-svm.html#non-linear-svm",
    "href": "p03-11-svm.html#non-linear-svm",
    "title": "26  Support Vector Machine",
    "section": "26.2 Non-linear SVM",
    "text": "26.2 Non-linear SVM\nTrong hai ví dụ trên, việc phân loại có thể giải quyết đơn giản bằng việc xây dựng hyperplane tuyến tính. Tuy nhiên, trong thực tế, không phải lúc nào cũng có thể xây dựng một mặt phẳng đơn giản để phân loại dữ liệu. Thay vào đó, ta phải thực hiện phép biến đổi dữ liệu để thay đổi dữ liệu gốc thành kiểu dữ liệu có thể xây dựng hyperplane đơn giản hơn. Phép biến đổi dữ liệu được gọi là kernel\nVí dụ 3: SVM trong mô hình phi tuyến"
  },
  {
    "objectID": "p03-11-svm.html#tìm-hiểu-kỹ-hơn-về-maximal-margin-classifier",
    "href": "p03-11-svm.html#tìm-hiểu-kỹ-hơn-về-maximal-margin-classifier",
    "title": "26  Support Vector Machine",
    "section": "26.3 Tìm hiểu kỹ hơn về Maximal Margin Classifier",
    "text": "26.3 Tìm hiểu kỹ hơn về Maximal Margin Classifier\nTrong không gian p chiều, hyperlane là một không gian con có p-1 chiều. VD: Trong không gian 2 chiều, hyperlane là 1 đường thẳng. Trong không gian ba chiều, hyperlane là 1 mặt phẳng\nTrong không gian 2 chiều, hyperlane được định nghĩa như sau:\n\\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 = 0\\]\nTrong không gian p chiều, hyperlane được định nghĩa như sau:\n\\[\\beta_0 + \\beta_1X_1 +...+ \\beta_pX_p = 0\\]\nMỗi mặt phẳng phân loại như vậy được gọi là “separating hyperplanes”.\nGiả sử biến cần phân loại y có giá trị {-1,1}, ta có:\n\\(\\sum_{j=1}^p\\beta_iX_{ij} > 0\\) nếu \\(y_i = 1\\) và \\(\\sum_{j=1}^p\\beta_iX_{ij} < 0\\) nếu \\(y_i = -1\\)\nĐể phân loại các quan sát, có thể tồn tại rất nhiều hyperplane. Do đó, người ta chọn 1 hyperplane có vị trí nằm “xa” nhất giữa 2 miền cần phân loại. Khoảng cách gần nhất giữa 2 miền cần phân loại gọi là margin. Mặt phẳng vuông góc với đường margin này gọi là maximal margin classifier\nKhi dữ liệu không chỉ còn đơn giản là margin classifier mà phải sử dụng phép biến đổi dữ liệu với kernel, mô hình được gọi là support vector machine. Trong thực tế, khi xây dựng support vector machine, ta không thể tìm được mặt phẳng phân loại hoàn hảo mà sẽ phải chấp nhận sai số nhất định, mặt phẳng có sai số khi phân loại được gọi là soft margin."
  },
  {
    "objectID": "p03-11-svm.html#ưu-nhược-điểm-của-svm",
    "href": "p03-11-svm.html#ưu-nhược-điểm-của-svm",
    "title": "26  Support Vector Machine",
    "section": "26.4 Ưu nhược điểm của SVM",
    "text": "26.4 Ưu nhược điểm của SVM\nƯu điểm:\n\nMô hình hiệu quả với nhiều biến (không gian nhiều chiều). Do đó, SVM hoạt động đặc biệt hiệu quả khi phân tích sentiment, phân loại tài liệu document, phân tích gene\nLưu trữ liệu quả: Mô hình SVM chỉ cần lưu trữ một tập con của dữ liệu huấn luyện khi dự báo trên tập mới\n\nNhược điểm:\n\nNhạy cảm với phép biến đổi dữ liệu kernel\nThời gian tính toán phức tạp với khi có số lượng quan sát lớn\nKhông có điểm xác suất khi dự báo: Với mỗi kết quả dự báo, chỉ có kết quả phân loại theo nhóm"
  },
  {
    "objectID": "p03-11-svm.html#thực-hành-với-r",
    "href": "p03-11-svm.html#thực-hành-với-r",
    "title": "26  Support Vector Machine",
    "section": "26.5 Thực hành với R",
    "text": "26.5 Thực hành với R\nCác tham số cần tối ưu:\n\n\\(C\\): Tham số quy định độ soft của mặt phẳng SVM, C càng lớn, các quan sát phân loại sai bị đánh trọng số càng lớn\n\nCác loại kernel thường gặp:\n\nlinear: Dữ liệu mới được tạo thành từ tổ hợp tuyến tính\npolynomial: Dữ liệu được biến đổi bằng đa thức bậc cao từ dữ liệu gốc\n\n\n\n\n\n\n\nRadial Basis Function: Phép biến đổi dữ liệu với hàm \\(\\phi(x, center) = exp(-\\gamma ||x-center||^2)\\)\n\n\ndf <- data.frame(x = c(0,0.5, 1, 1.5, 2, 2.5, 3,3.5, 4),\n                 y = rep(1,9),\n                 class = c(F, F, F, F, T, F, F, F, F))\n\ncenter <- df$x %>% mean\ngamma_1 <- 0.01\ngamma_2 <- 5\ndf <- df %>% \n  mutate(y_1 = exp(-gamma_1 * (x-center)^2),\n         y_2 = exp(-gamma_2 * (x-center)^2))\n\ndf %>% \n  gather(\"variable\", \"y\", -x, -class) %>% \n  mutate(variable = case_when(\n    variable == \"y_1\" ~ \"gamma = 0.01\",\n    variable == \"y_2\" ~ \"gamma = 5\",\n    TRUE ~ \"Not transformed\"\n  )) %>% \n  ggplot(aes(x, y)) +\n  geom_line(col = \"gray\") +\n  geom_point(aes(col = class), size = 4) +\n  facet_wrap(~variable, scale = \"free\") +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  labs(title = \"RBF transformation\")\n\n\n\n\nVí dụ với R:\n\nlibrary(ISLR)\nlibrary(e1071)\n\nDefault %>% head\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\nindex <- sample(1:nrow(Default), nrow(Default) * 0.75)\ntrain <- Default[index, ]\ntest  <- Default[-index, ]\n\nsvm_model <- svm(default ~ ., data = train, kernel = \"polynomial\", cost = 100)\nsummary(svm_model)\n\n\nCall:\nsvm(formula = default ~ ., data = train, kernel = \"polynomial\", cost = 100)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  polynomial \n       cost:  100 \n     degree:  3 \n     coef.0:  0 \n\nNumber of Support Vectors:  462\n\n ( 241 221 )\n\n\nNumber of Classes:  2 \n\nLevels: \n No Yes\n\npredict(svm_model, test) %>% table(test$default)\n\n     \n.       No  Yes\n  No  2411   63\n  Yes    6   20"
  },
  {
    "objectID": "p03-11-svm.html#tài-liệu-tham-khảo",
    "href": "p03-11-svm.html#tài-liệu-tham-khảo",
    "title": "26  Support Vector Machine",
    "section": "26.6 Tài liệu tham khảo",
    "text": "26.6 Tài liệu tham khảo\n\n[https://www.edureka.co/blog/support-vector-machine-in-r/]\n[https://www.datacamp.com/community/tutorials/support-vector-machines-r]\n[https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/]\n[https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496]\nSupport Vector Machine - chater 9 - Introduction to Statistical Learning with R"
  },
  {
    "objectID": "p03-15-nonlinear.html#giới-thiệu",
    "href": "p03-15-nonlinear.html#giới-thiệu",
    "title": "27  Mô hình phi tuyến tính",
    "section": "27.1 Giới thiệu",
    "text": "27.1 Giới thiệu\nỞ các phần trước, ta đã đề cập đến các kỹ thuật phân tích có tính ứng dụng thực tiễn cao như linear regression, logistic regression hay tree based methods. Ở chương này, ta sẽ đề cập nhiều hơn đến các mô hình mở rộng của regression và sử dụng các mô hình phi tuyến, bao gồm các nhóm sau:\n\nPolynomial regression: Hàm bậc cao X, \\(X^2\\), \\(X^3\\)\nStep functions: Chia biến thành k miền khác nhau, xây dựng mô hình đơn giản ứng với k miền đó.\nRegression splines: Chia biến y thành k miền khác nhau, với mỗi miền sẽ xây dựng một polynomial regression sao cho biến y liên tục trên toàn miền\nSmoothing splines: Tương tự như regression splines, nhưng thêm điều kiện tối ưu hóa tổng bình phương sai số trên toàn miền.\nLocal regression: Tương tự như splines nhưng cho phép các miền của biến y có vùng chồng lấn.\n\nCác phương pháp trên cho phép xây dựng mô hình với 1 biến độc lập (1 predictor). Với mô hình đa biến, ta có thể sử dụng mô hình GAM Generalized additive models"
  },
  {
    "objectID": "p03-15-nonlinear.html#polynomial-regression",
    "href": "p03-15-nonlinear.html#polynomial-regression",
    "title": "27  Mô hình phi tuyến tính",
    "section": "27.2 Polynomial regression",
    "text": "27.2 Polynomial regression\nPhương trình của polynomial regression\n\\[y_i = \\beta_0 + \\beta_1X_i + \\beta_2X_i^2+...+\\beta_dX_i^d +\\epsilon\\]\nLưu ý:\n\nPhương trình trên có thể ước lượng đơn giản với OLS\nKhông nên sử dụng với \\(d \\geq 4\\)\n\n\nlibrary(tidyverse)\nlibrary(ISLR)\ndata(Wage) \nWage %>% \n    select(age, wage) %>% \n    head\n\n       age      wage\n231655  18  75.04315\n86582   24  70.47602\n161300  45 130.98218\n155159  43 154.68529\n11443   50  75.04315\n376662  54 127.11574\n\nWage %>% \n    ggplot(aes(age, wage)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ poly(x, 4),\n                size = 2,\n                col = \"darkred\") +\n    theme_minimal() +\n    labs(title = \"Polynomial function with 4 degree\")\n\n\n\n\n\n# Xây dựng mô hình\nfit <- lm(wage ~ poly(age,4), data = Wage)\nfit %>% summary\n\n\nCall:\nlm(formula = wage ~ poly(age, 4), data = Wage)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-98.707 -24.626  -4.993  15.217 203.693 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    111.7036     0.7287 153.283  < 2e-16 ***\npoly(age, 4)1  447.0679    39.9148  11.201  < 2e-16 ***\npoly(age, 4)2 -478.3158    39.9148 -11.983  < 2e-16 ***\npoly(age, 4)3  125.5217    39.9148   3.145  0.00168 ** \npoly(age, 4)4  -77.9112    39.9148  -1.952  0.05104 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.91 on 2995 degrees of freedom\nMultiple R-squared:  0.08626,   Adjusted R-squared:  0.08504 \nF-statistic: 70.69 on 4 and 2995 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "p03-15-nonlinear.html#step-functions",
    "href": "p03-15-nonlinear.html#step-functions",
    "title": "27  Mô hình phi tuyến tính",
    "section": "27.3 Step functions",
    "text": "27.3 Step functions\nStep function còn được gọi là piece wise constant regression. Thuật toán này chia biến y thành các k khu vực (bins), tìm hằng số không đổi \\(C\\) cho mỗi bin và thay đổi các biến liên tục thành các biến category có sắp xếp như sau.\n\n\n\n\n\n\nLưu ý: Các hàm polynomial và step function (còn gọi là piecewise constant function) là dạng hàm đặc biệt của basis function như sau.\nGọi \\(b_i(.)\\) là hàm đã biết trước của \\(X\\), ta có:\n\\[y_i = \\beta_0 + \\beta_1b_1(x_i) + ... +\n\\beta_kb_k(x_i) + \\epsilon\\]\n\nĐối với polynomial: \\(b_j(x_i) = x_i^j\\)\nĐối với step function: \\(b_j(x_i)= I(c_j \\leq x_i \\prec_{j+1})\\)"
  },
  {
    "objectID": "p03-15-nonlinear.html#regression-splines",
    "href": "p03-15-nonlinear.html#regression-splines",
    "title": "27  Mô hình phi tuyến tính",
    "section": "27.4 Regression splines",
    "text": "27.4 Regression splines\nHàm piecewise polynomial chia X thành nhiều nhóm, và xây dựng mô hình hàm bậc cao cho từng nhóm. Điểm mà các hệ số tương quan của X thay đổi giữa các khu vực gọi là knots\n\nĐể các hàm splines được liên tục, ta giải hệ phương trình để tất cả các điểm trên đồ thị, đạo hàm bậc 1 và 2 đều liên tục.\n\nLưu ý:\n\nNatural splines là phương trình của cubic splines nhưng thêm điều kiện là phương trình phải tuyến tính (linear) tại các điểm biên (boundary constraints)\n\n\n\nĐể lựa chọn số lượng knots K, ta sử dụng Cross-Validation. MSE nhỏ nhất sẽ được sử dụng\n\nSo sánh với polynomial: Regression splines có những ưu điểm sau so với polynomial như sau:\n\nCho phép thêm các knots tại các khu vực mà hệ số tương quan thay đổi nhanh\nGiữ bậc của các biến trong mô hình thấp hơn\n\n\nlibrary(splines)\n\nfit <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)\n\nfit %>% summary\n\n\nCall:\nlm(formula = wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-98.832 -24.537  -5.049  15.209 203.207 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                       60.494      9.460   6.394 1.86e-10 ***\nbs(age, knots = c(25, 40, 60))1    3.980     12.538   0.317 0.750899    \nbs(age, knots = c(25, 40, 60))2   44.631      9.626   4.636 3.70e-06 ***\nbs(age, knots = c(25, 40, 60))3   62.839     10.755   5.843 5.69e-09 ***\nbs(age, knots = c(25, 40, 60))4   55.991     10.706   5.230 1.81e-07 ***\nbs(age, knots = c(25, 40, 60))5   50.688     14.402   3.520 0.000439 ***\nbs(age, knots = c(25, 40, 60))6   16.606     19.126   0.868 0.385338    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.92 on 2993 degrees of freedom\nMultiple R-squared:  0.08642,   Adjusted R-squared:  0.08459 \nF-statistic: 47.19 on 6 and 2993 DF,  p-value: < 2.2e-16\n\npred_data <- Wage %>% \n    select(age) %>% \n    distinct %>% \n    mutate(pred = predict(fit, .))\n\npred_data %>% head\n\n  age      pred\n1  18  60.49371\n2  24  82.84196\n3  45 119.39567\n4  43 118.91764\n5  50 119.41254\n6  54 118.52586\n\nWage %>% \n    ggplot(aes(age, wage)) +\n    geom_point(alpha = 0.2) +\n    geom_line(data = pred_data,\n              aes(age, pred), \n              col = \"darkred\",\n              size = 2) +\n    theme_minimal() +\n    labs(title = \"Splines\")"
  },
  {
    "objectID": "p03-15-nonlinear.html#smoothing-splines",
    "href": "p03-15-nonlinear.html#smoothing-splines",
    "title": "27  Mô hình phi tuyến tính",
    "section": "27.5 Smoothing splines",
    "text": "27.5 Smoothing splines\nKhi xây dựng mô hình, ta muốn xây dựng một hàm f(x) sao cho RSS là nhỏ nhất. Tuy nhiên, ta có thể biến RSS có giá trị bằng 0 bằng cách chọn f(x) sao cho f(x) bao quanh tất cả các giá trị của y.\nDo đó, ta muốn xây dựng đường f(x) sao cho RSS nhỏ nhất đồng thời smooth.\n\\[\\sum_{i=1}^n(y_i-g(x_i))^2+\\lambda\\int g^{''}(t)^2dt\\]\nTrong đó:\n\n\\(\\lambda\\) được gọi là tuning parameter\n\\(g\\) được gọi là smoothing spline\n\nĐối với phương trình trên:\n\n\\(\\sum_{i=1}^n(y_i-g(x_i))^2\\) được gọi là loss function, mục tiêu để phương trình lý thuyết và thực tế gần nhau nhất\n\\(\\lambda\\int g^{''}(t)^2dt\\) được gọi là penalty của hàm lý thuyết.\n\n\\(g^{'}\\) đo lường độ dốc của các hệ số trong phương trình tại điểm t,\n\\(g^{''}\\) đo lường độ thay đổi của các hệ số trong phương trình tại điểm t. Tích phân của \\(g^{''}\\) đo lường tổng các sự thay đổi của các hệ số trên toàn miền dữ liệu tính toán.\n\nNếu hệ số \\(\\beta\\) trên toàn miền không đổi, phần penalty này sẽ có giá trị bằng 0\n\\(\\lambda\\) càng lớn, phương trình sẽ càng trơn (smooth). Nếu \\(\\lambda = 0\\), phương trình sẽ là các đường nối các điểm \\(y_i\\). Nếu \\(\\lambda \\rightarrow \\infty\\), phương trình sẽ là 1 đường thẳng và trở về OLS\n\n\n\nLưu ý: Hàm \\(g(x)\\) thỏa mãn các điều kiện trên là đường natural cubic splines với các knots là tất cả các điểm riêng biệt \\(x_i\\)"
  },
  {
    "objectID": "p03-15-nonlinear.html#local-regression",
    "href": "p03-15-nonlinear.html#local-regression",
    "title": "27  Mô hình phi tuyến tính",
    "section": "27.6 Local regression",
    "text": "27.6 Local regression\nLocal regression (loess) là phương pháp xây một đường trơn (smoother) bằng cách xây dựng mô hình OLS đơn giản có trọng số. Dữ liệu được sử dụng xây dựng mô hình là các quan sát lân cận điểm xây dựng mô hình.\nVí dụ gần nhất với local regression là mô hình moving average trong chuỗi thời gian. Trong đó, kết quả của đường làm trơn \\(y\\) là giá trị trung bình của \\(k\\) quan sát trước và sau thời điểm \\(t_0\\).\n\nWage %>% \n    ggplot(aes(age, wage)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(col = \"darkred\",\n                method = \"loess\",\n                size = 2) +\n    theme_minimal() +\n    labs(title = \"Example of loess model\")\n\n\n\n\nCác bước xây dựng mô hình loess như sau.\n\nXác định tham số \\(s\\) cua mô hình. \\(s\\) có giá trị trong khoảng 0-1. Ví dụ \\(s=0.3\\), mô hình sẽ lấy 30% quan sát gần nhất với điểm \\(x_0\\) để xây dựng mô hình. Tham số \\(s\\) được gọi là span parameter. Số lượng quan sát để xây dựng local regression tại mỗi điểm là \\(k\\)\nXác định trọng số cho mỗi điểm trong dữ liệu xây mô hình như sau\n\n\nXác định khoảng cách đã chuẩn hóa từ điểm cần ước lượng \\(d_i=\\frac{x_i-x_0}{|max(x_k) - x_0|}\\)\nXác định trọng số \\(w_i = (1-d_i^3)^3\\)\n\n\nXây dựng mô hình hồi quy có trọng số bằng cách tối ưu hóa hàm loss sau.\n\n\\[\\sum_{i=1}^nw_i*(y_i - \\beta_0 - \\beta_1x_i)^2\\]\n\nGiá trị dự báo tại điểm \\(x_0\\) là \\(\\beta_0 + \\beta_1 x_0\\)\n\nVí dụ: Ta có \\(k = 7\\), ta cần đưa ra giá trị ước lượng của mô hình tại quan sát số 3 với \\(x = 2.5773252\\).\n\n\n\n\n\n\n\n\n\n\n\nno\nx\ny\ndistance\nscale_distance\nweight\n\n\n\n\n1\n0.5578196\n18.63654\n2.0195055\n 0.7982068\n0.1186858\n\n\n2\n2.0217271\n103.49646\n0.5555981\n 0.2195994\n0.9685654\n\n\n3\n2.5773252\n150.35391\n0.0000000\n 0.0000000\n1.0000000\n\n\n4\n3.4140288\n190.51031\n0.8367037\n 0.3307060\n0.8953727\n\n\n5\n4.3014084\n208.70115\n1.7240833\n 0.6814416\n0.3194020\n\n\n6\n4.7448394\n213.71135\n2.1675143\n 0.8567071\n0.0511567\n\n\n7\n5.1073781\n228.49353\n2.5300530\n 1.0000000\n0.0000000\n\n\n\nTa có thể tính toán như sau:\n\n\\(d_2 = |x_2 - x_3| = |2.0217271 - 2.5773252| = 0.5555981\\)\n\\(scale\\_d2 = \\frac{d_2}{|x_7-x_3|}=\\frac{0.5555981}{5.1073781-2.5773252}=0.2195994\\)\n\\(weight_2 = (1-d_2^3)^3 = 0.9685654\\)\n\nMô hình loess được hỗ trợ sẵn trên ggplot2 có thể tùy chỉnh các tham số span như sau.\n\nlibrary(patchwork)\np1 <- ggplot(mpg, aes(displ, hwy)) +\n    geom_point(alpha = 0.4) +\n    geom_smooth(span = 0.3) +\n    theme_minimal() +\n    labs(title = \"Span = 0.3\")\np2 <- ggplot(mpg, aes(displ, hwy)) +\n    geom_point(alpha = 0.4) +\n    geom_smooth(span = 0.7) +\n    theme_minimal() +\n    labs(title = \"Span = 0.7\")\np1 + p2"
  },
  {
    "objectID": "p03-15-nonlinear.html#generalized-additive-models-gam",
    "href": "p03-15-nonlinear.html#generalized-additive-models-gam",
    "title": "27  Mô hình phi tuyến tính",
    "section": "27.7 Generalized Additive Models (GAM)",
    "text": "27.7 Generalized Additive Models (GAM)\nMô hình GAM là mô hình mở rộng của các mô hình phía trên vaf dựa trên 2 giả định:\n\nBiến cần dự báo \\(y\\) có thể có mối quan hệ tuyến tính hoặc phi tuyến tính với các biến độc lập\nTa có thể cùng lúc xây dựng mô hình giữa biến phụ thuộc \\(y\\) với từng biến độc lập \\(x_i\\) và có thể dự báo \\(y\\) bằng cách cộng các mô hình vừa xây dựng.\n\nÝ tưởng của mô hình GAM có thể được minh họa qua biểu đồ sau.\n\nMô hình GAM được sử dụng với đa biến như sau.\n\\[y_i = \\beta_0 + f_1(x_{i1}) + ... + f_px_{ip} + \\epsilon_i\\]\nƯu điểm:\n\nDễ giải thích\nPhản ánh được mối quan hệ cả phi tuyến và phi tuyến giữa các biến\nHỗ trợ regularization\n\nNhược điểm: Giả định của GAM là các yếu tố của từng biến là có thể cộng được. Tuy nhiên, điều này không phải lúc nào cũng đúng trong thực tế. Do đó, với mô hình nhiều biến, hiệu ứng tương tác giữa các biến sẽ không được thể hiện rõ trong GAM\nLưu ý: GAM cũng có thể được dùng cho logistic như sau:\n\\[log(\\frac{p(X)}{1-p(X)})=\\beta_0 + f_1(x_{i1}) + ... + f_px_{ip}\\]\n\nlibrary(gam)\nlibrary(ISLR)\nlibrary(dplyr)\ngam_model <- gam(wage ~ s(year , 4) + s(age , 5) + education, \n              data = Wage)\ngam_model %>% summary\n\n\nCall: gam(formula = wage ~ s(year, 4) + s(age, 5) + education, data = Wage)\nDeviance Residuals:\n    Min      1Q  Median      3Q     Max \n-119.43  -19.70   -3.33   14.17  213.48 \n\n(Dispersion Parameter for gaussian family taken to be 1235.69)\n\n    Null Deviance: 5222086 on 2999 degrees of freedom\nResidual Deviance: 3689770 on 2986 degrees of freedom\nAIC: 29887.75 \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n             Df  Sum Sq Mean Sq F value    Pr(>F)    \ns(year, 4)    1   27162   27162  21.981 2.877e-06 ***\ns(age, 5)     1  195338  195338 158.081 < 2.2e-16 ***\neducation     4 1069726  267432 216.423 < 2.2e-16 ***\nResiduals  2986 3689770    1236                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n            Npar Df Npar F  Pr(F)    \n(Intercept)                          \ns(year, 4)        3  1.086 0.3537    \ns(age, 5)         4 32.380 <2e-16 ***\neducation                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "p03-15-nonlinear.html#tài-liệu-tham-khảo",
    "href": "p03-15-nonlinear.html#tài-liệu-tham-khảo",
    "title": "27  Mô hình phi tuyến tính",
    "section": "27.8 Tài liệu tham khảo",
    "text": "27.8 Tài liệu tham khảo\n\nIntroduction to Statistical Learning - chapter 7\nhttps://www.itl.nist.gov/div898/handbook/pmd/section1/dep/dep144.htm"
  },
  {
    "objectID": "p03-17-regularization.html#giới-thiệu",
    "href": "p03-17-regularization.html#giới-thiệu",
    "title": "28  Lựa chọn biến & regularization",
    "section": "28.1 Giới thiệu",
    "text": "28.1 Giới thiệu\nKhi xây dựng mô hình dự báo, ta sẽ gặp phải các vấn đề sau.\n\nOver fitting: Hiện tượng mô hình có chất lượng tốt trên tập train nhưng lại xây dựng không tốt trên tập test.\nĐộ chính xác kém khi số lượng quan sát không đủ: Khi \\(n \\gg p\\), nghĩa là số lượng quan sát lớn hơn rất nhiều so với số biến, chất lượng mô hình có thể được đảm bảo. Tuy nhiên, nếu n không lớn hơn nhiều so với p, mô hình sẽ gặp phải vấn đề về chất lượng mô hình.\nKhả năng giải thích ý nghĩa mô hình: Thông thường, khi đưa càng nhiều biến - độ chính xác của mô hình càng tăng lên. Tuy nhiên, nhiều biến trong đó không ảnh hưởng lớn đến mô hình. Việc giảm bớt biến trong mô hình có thể giúp việc giải thích mô hình trở nên đơn giản hơn.\n\nĐể khắc phục các nhược điểm trên, khi xây dựng mô hình, ta có hai kỹ thuật lớn sau.\n\nSubset selection: Xác định số lượng biến ảnh hưởng nhiều nhất đến biến cần dự báo và xây dựng mô hình dựa trên các biến này.\nShrinkage: Còn được gọi là regularization có tác dụng giảm variance của mô hình thông qua việc thay đổi loss function, tăng giá trị của hàm này khi mô hình trở nên phức tạp. Nhóm kỹ thuật này là nhóm được sử dụng đặc biệt nhiều trong tất cả các thuật toán của machine learning\n\nTrong chương này, ta sẽ tìm hiểu các nhóm kỹ thuật này thông qua bài toán hồi quy tuyến tính."
  },
  {
    "objectID": "p03-17-regularization.html#subset-selection",
    "href": "p03-17-regularization.html#subset-selection",
    "title": "28  Lựa chọn biến & regularization",
    "section": "28.2 Subset Selection",
    "text": "28.2 Subset Selection\n\n28.2.1 Best subset selection\nThuật toán này cho phép lựa chọn mô hình có số biến phụ thuộc tối ưu bằng việc xây dựng rất nhiều mô hình và chọn ra từ đó mô hình có RSS thấp nhất.\nCác bước thực hiện:\n\nGọi \\(M_0\\) là mô hình ban đầu (null model). Mô hình này không có biến độc lập, dự báo giá trị Y bằng giá trị trung bình của tập train\nVới k=1..p:\n\n\nXây dựng \\((\\stackrel{p}{k})\\) mô hình với đúng k biến độc lập\nLựa chọn từ mô hình gọi là \\(M_k\\) với RSS có giá trị nhỏ nhất\n\n\nChọn các mô hình \\(M_0,...,M_k\\) bằng cross validation bằng các tiêu chí AIC, BIC hoặc \\(\\overline{R}^2\\)\n\nLưu ý:\n\nSubset selection lựa chọn mô hình tốt nhất từ \\(2^p\\) mô hình. Trong bước 2, ta đã giảm việc lựa chọn cross validation từ \\(2^p\\) xuông p+1 mô hình\nPhải xây dựng rất nhiều mô hình, điều này có thể khiến quá trình phân tích tốn nhiều thời gian\n\n\n\n28.2.2 Stepwise selection\n\n28.2.2.1 Forward stepwise selection\nCác bước thực hiện:\n\nXây dựng null model không chưa biến độc lập\nThêm ĐÚNG 1 biến độc lập, lựa chọn mô hình có RSS nhỏ nhất, gọi là mô hình \\(M_1\\)\nLặp lại bước 2 cho đến khi tất cả các biến đều được đưa vào mô hình, mô hình có tất cả các biến là \\(M_p\\)\nLựa chọn mô hình tốt nhất của \\(M_0,...,M_p\\) bằng cross validation\n\nVí dụ: Mô hình có 3 biến \\(X_1\\), \\(X_2\\), \\(X3\\), biến phụ thuộc là Y, ta thực hiện như sau:\n\nGọi null model \\(M_0\\) không chưa biến độc lập\nLựa chọn \\(M_1\\) là mô hình CHỈ chưa MỘT biến độc lập (cách thực hiện: Xây dưng 3 mô hình với lần lượt từng biến, chọn mô hình có RSS thấp nhât), giả sử là mô hình chỉ chưa biến \\(X_1\\)\nLặp lại bước 2, xây dụng 2 mô hình bằng cách thêm lần lượt \\(X_2\\), \\(X_3\\) vào mô hình 2\nThực hiện cross validation để cọn mô hình tốt nhất\n\n*Ghi chú:\n\nMô hình này giảm đáng kể số lượng mô hình. Tổng số mô hình được xây dựng là \\(1+\\frac{p(p+1)}{2}\\)\nPhương pháp này không đảm bảo lựa chọn được mô hình tốt nhất từ \\(2^p\\) mô hình. VD: Mô hình có 3 biến \\(X_1\\),…,\\(X_3\\), ta có:\n\nMô hình 1 biến tốt nhất là mô hình với \\(X_1\\)\nMô hình 2 biến tốt nhất là mô hình với \\(X_2\\) và \\(X_3\\)\nPhương pháp stepwise selection, mô hình 2 biến tốt nhất là \\(X_1\\) và \\(X_2\\)\n\n\n\n\n28.2.2.2 Backward stepwise selection\nPhương pháp này ngược lại với forward stepwise, xây dựng mô hình full, sau đó giảm biến lần lượt\nPhương pháp chọn mô hình tốt nhất:\n\nPhương pháp gián tiếp: Ước lượng sai số của test error bằng cách điều chỉnh các tiêu chí dựa vào biến. VD: AIC, BIC, Adjusted-R-squared\nPhương pháp trực tiếp: Ước lượng trực tiếp dựa trên cross validation"
  },
  {
    "objectID": "p03-17-regularization.html#shrinkage-methods",
    "href": "p03-17-regularization.html#shrinkage-methods",
    "title": "28  Lựa chọn biến & regularization",
    "section": "28.3 Shrinkage methods",
    "text": "28.3 Shrinkage methods\nVới các phương pháp sử dụng subset selection, ta phải thực hiện nhiều bước tính toán để có thể tìm được tập \\(p\\) biến dự báo tốt nhất cho mô hình. Trong thực tế, để khắc phục nhược điểm trên, ta sử dụng kỹ thuật regularization để cân đối giữa số lượng tham số (complexity level) của mô hình với độ chính xác trên tập test. Qua đó giảm thiểu hiện tượng overfiting. Hai kỹ thuật được dùng nhiều nhất là ridge regression và lasso regression.\n\n28.3.1 Ridge regression\nĐối với mô hình hồi quy thông thường, hàm mục tiêu là tối thiểu hóa RSS:\n\\[RSS = \\sum^n_{i=1}(y_i - \\beta_0 - \\sum^p_{i=1}\\beta_jx_{ij})^2\\]\nRidge regression tương tự như OLS, tuy nhiên hàm mục tiêu là tối thiểu hóa hàm sau\n\\[RSS + \\lambda\\sum_{j=1}^p\\beta_j^2\\]\nGhi chú:\n\n\\(\\lambda\\sum\\beta_j^2\\) được gọi là shrinkage penalty. Giá trị này sẽ nhỏ khi \\(\\beta_j\\) tiến gần đến 0\n\\(\\lambda \\geq 0\\) được gọi là tuning parameter\nVới \\(\\lambda = 0\\), ridge regression trở thành OLS\nViệc lựa chọn giá trị của \\(\\lambda\\) rất quan trọng với ridge regression\nRidge regression còn được gọi là L2 regularization\n\n\n\n28.3.2 Lasso regression\nRidge regression có nhược điểm là không đưa các giá trị ước lượng của biến về 0 mà chỉ tiệm cận 0. Lasso khắc phục nhược điểm đó bằng việc thay đổi hàm mục tiêu như sau:\n\\[RSS + \\alpha(\\sum_{j=1}^p|\\beta_j|)\\]\nLưu ý: Lassso regression còn được gọi là L1 regularization\nCách tiếp cận khác của Ridge và Lasso\nMô hình Ridge và Lasso có thể được viết lại như sau:\nMinimize \\(RSS\\), subject to:\n\n\\(\\sum_{j=1}^p|\\beta_j| \\leq s\\), với Lasso regression\n\\(\\sum_{j=1}^p\\beta_j^2 \\leq s\\), với Ridge regression\n\nRidge regression chỉ cho phép các giá trị của tham số tiệm cận 0 như hình dưới đây.\n\nLưu ý:\n\nLasso cho phép mô hình có tính giải thích tốt hơn do số lượng predictor ít hơn. Tuy nhiên, với 1 số trường hợp, khi mối quan hệ giữa các biến (\\(\\beta\\)) gần bằng 0, mô hình Ridge sẽ tốt hơn\nRegularization kết hợp cả ridge và lasso được gọi là elastic net\nVì ridge regression chỉ đưa các tham số của mô hình tiệm cận 0, mô hình này chỉ giúp hỗ trợ chúng ta giải quyết vấn đề bias-variance. Trong khi đó, với lasso, các tham số có thể có giá trị bằng 0, điều này giúp ta xử lý cả vấn đề chọn biến trong mô hình.\nVới mô hình cây quyết định, thay vì hệ số \\(\\beta\\) của mô hình hồi quy, các tham số khác của cây quyết định như độ sâu của cây (depth), số lượng lần rẽ nhánh (nodes) được đưa vào thực hiện regularization."
  },
  {
    "objectID": "p03-17-regularization.html#thực-hành-với-r",
    "href": "p03-17-regularization.html#thực-hành-với-r",
    "title": "28  Lựa chọn biến & regularization",
    "section": "28.4 Thực hành với R",
    "text": "28.4 Thực hành với R\n\n28.4.1 Best subset selection\nVới best subset selection, ta có thể sử dung package leaps\n\nlibrary(ISLR)\nlibrary(dplyr)\nlibrary(ggplot2)\n#Xử lý số liệu\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\n#Loại bỏ giá trị bị thiếu\ndata <- na.omit(Hitters)\ndata %>% dim\n\n[1] 263  20\n\ndata %>% head\n\n                  AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun\n-Alan Ashby         315   81     7   24  38    39    14   3449   835     69\n-Alvin Davis        479  130    18   66  72    76     3   1624   457     63\n-Andre Dawson       496  141    20   65  78    37    11   5628  1575    225\n-Andres Galarraga   321   87    10   39  42    30     2    396   101     12\n-Alfredo Griffin    594  169     4   74  51    35    11   4408  1133     19\n-Al Newman          185   37     1   23   8    21     2    214    42      1\n                  CRuns CRBI CWalks League Division PutOuts Assists Errors\n-Alan Ashby         321  414    375      N        W     632      43     10\n-Alvin Davis        224  266    263      A        W     880      82     14\n-Andre Dawson       828  838    354      N        E     200      11      3\n-Andres Galarraga    48   46     33      N        E     805      40      4\n-Alfredo Griffin    501  336    194      A        W     282     421     25\n-Al Newman           30    9     24      N        E      76     127      7\n                  Salary NewLeague\n-Alan Ashby        475.0         N\n-Alvin Davis       480.0         A\n-Andre Dawson      500.0         N\n-Andres Galarraga   91.5         N\n-Alfredo Griffin   750.0         A\n-Al Newman          70.0         A\n\nlibrary(leaps)\n#Xây dựng mô hình\nregfit.full <- regsubsets(Salary ~ ., data, nvmax = 19)\nreg.summary <- regfit.full %>% summary\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\nreg.summary$adjr2 %>% which.max\n\n[1] 11\n\nreg.summary2 <- data.frame(nr = seq(1:19),\n                           adjr2 = reg.summary$adjr2,\n                           bic = reg.summary$bic)\n\n\nggplot(reg.summary2, aes(nr, y = adjr2)) + \n  geom_line() +\n  geom_point(size = 2) + \n  geom_point(aes(x = which.max(adjr2), \n                 y = max(adjr2)), \n             col = \"darkred\", \n             size =\n               5) +\n  geom_text(aes(x = which.max(adjr2), y = 1.05 * max(adjr2)),\n            label = reg.summary2$adjr2 %>% max %>% round(3)) +\n  theme_minimal() +\n  xlab(\"Number of variables\") +\n  ylab(\"Adjusted R-squared\") +\n  ggtitle(\"Best model selection\")\n\n\n\ncoef(regfit.full, 11)\n\n (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns \n 135.7512195   -2.1277482    6.9236994    5.6202755   -0.1389914    1.4553310 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n   0.7852528   -0.8228559   43.1116152 -111.1460252    0.2894087    0.2688277 \n\n\n\n\n28.4.2 Stepwise methods\n\nregfit.fwd <-  regsubsets(Salary ~ .,\n                          data = data ,\n                          nvmax = 19,\n                          method = \"forward\")\nregfit.fwd %>% summary\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = data, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\ncoef(regfit.fwd, 2)\n\n(Intercept)        Hits        CRBI \n-47.9559022   3.3008446   0.6898994 \n\ncoef(regfit.full, 2)\n\n(Intercept)        Hits        CRBI \n-47.9559022   3.3008446   0.6898994 \n\n\nLưu ý: Với số lượng biến nhỏ, mô hình best selection và stepwise đưa ra kết quả tương tự nhau\n\n\n28.4.3 Ridge & Lasso\nVới mô hình ridge và lasso với OLS, ta có thể sử dụng package glmnet. Trong hàm glmnet, tham số alpha cho ta biết mô hình sử dụng là ridge (alpha=0) hay lasso (alpha=1).\n\nlibrary(tidyverse)\nlibrary(glmnet)\n\n# Load the data\n\ndata(\"Boston\", package = \"MASS\")\n# Split the data into training and test set\nset.seed(123)\nindex <- sample(1:nrow(Boston), size = 0.8*nrow(Boston))\ntrain.data  <- Boston[index, ]\ntest.data <- Boston[-index, ]\n\n\nVới glmnet, ta có thể sử dụng hàm model.matrix để biến đổi dữ liệu category thành dữ liệu dummy\n\n\n# Predictor variables\nx <- model.matrix(medv~., train.data)[,-1]\n# Outcome variable\ny <- train.data$medv\n\nVới mỗi giá trị của \\(\\lambda\\), ta sẽ có các giá trị khác nhau của mô hình. Với \\(\\lambda=100\\) và \\(\\lambda=0.5\\), ta có mô hình như sau\n\nset.seed(123) \nridge_model_1 <- glmnet(x, y, alpha = 0, lambda = 100)\nridge_model_2 <- glmnet(x, y, alpha = 0, lambda = 0.5)\nridge_model_1 %>% coef\n\n14 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept) 23.658646697\ncrim        -0.022909606\nzn           0.007833157\nindus       -0.034850021\nchas         0.490839837\nnox         -1.796274567\nrm           0.662251454\nage         -0.006384242\ndis          0.036766230\nrad         -0.020942370\ntax         -0.001460787\nptratio     -0.153092873\nblack        0.002104939\nlstat       -0.065132786\n\nridge_model_2 %>% coef\n\n14 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s0\n(Intercept)  32.584333754\ncrim         -0.087170080\nzn            0.036197609\nindus        -0.034893448\nchas          3.237474156\nnox         -13.495108502\nrm            3.690568561\nage           0.004447692\ndis          -1.193672539\nrad           0.186930946\ntax          -0.006036195\nptratio      -0.912414376\nblack         0.007693141\nlstat        -0.535862761\n\n# Loss funciton vơi hai mô hình\nL1 <- sum(predict(ridge_model_1, x) - y)^2 + 100*(coef(ridge_model_1)^2 %>% sum)\nL2<- sum(predict(ridge_model_2, x) - y)^2 + 0.5*(coef(ridge_model_2)^2 %>% sum)\nL1\n\n[1] 56366.9\n\nL2\n\n[1] 635.274\n\n\nNhư vậy, trong trường hợp trên, tham số \\(\\lambda=0.5\\) đem lại kết quả tốt hơn rõ rệt.\nĐể xác định được tham số \\lambda tối ưu, ta cần tuning mô hình. Với glmnet, ta có thể tune qua cv.glmnet\n\nset.seed(123) \ncv <- cv.glmnet(x, y, alpha = 0)\n# Display the best lambda value\ncv$lambda.min\n\n[1] 0.6836632\n\n# Fit the final model on the training data\nmodel_ridge <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)\n# Display regression coefficients\ncoef(model_ridge)\n\n14 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s0\n(Intercept)  30.869686585\ncrim         -0.083683008\nzn            0.033515937\nindus        -0.042160761\nchas          3.240620842\nnox         -12.291603129\nrm            3.733572682\nage           0.003009141\ndis          -1.116113825\nrad           0.163294977\ntax          -0.005207278\nptratio      -0.892743304\nblack         0.007716638\nlstat        -0.520412813\n\n\nTương tự, ta có thể xây được mô hình lasso như sau\n\nset.seed(123) \ncv <- cv.glmnet(x, y, alpha = 0)\n# Display the best lambda value\ncv$lambda.min\n\n[1] 0.6836632\n\n# Fit the final model on the training data\nmodel_lasso <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)\n# Display regression coefficients\ncoef(model_lasso)\n\n14 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 17.52149872\ncrim         .         \nzn           .         \nindus        .         \nchas         1.56396540\nnox          .         \nrm           3.81759565\nage          .         \ndis          .         \nrad          .         \ntax          .         \nptratio     -0.70523547\nblack        0.00184477\nlstat       -0.53752341\n\n\nSo sánh hai mô hình trên, ta thấy mô hình lasso cho phép các biến ít quan trọng có giá trị bằng 0. Trong khi đó, mô hình ridge chỉ cho phép các hệ số này tiệm cận đến 0.\n\n# Dự báo trên tập mới\nlibrary(caret)\n# Make predictions on the test data\nx.test <- model.matrix(medv ~., test.data)[,-1]\npredictions <- model_ridge %>% predict(x.test) %>% as.vector()\n# Model performance metrics\ndata.frame(\n  RMSE = RMSE(predictions, test.data$medv),\n  RSquare = R2(predictions, test.data$medv)\n)\n\n      RMSE   RSquare\n1 4.820402 0.7274865"
  },
  {
    "objectID": "p03-17-regularization.html#tài-liệu-tham-khảo",
    "href": "p03-17-regularization.html#tài-liệu-tham-khảo",
    "title": "28  Lựa chọn biến & regularization",
    "section": "28.5 Tài liệu tham khảo",
    "text": "28.5 Tài liệu tham khảo\n\nChapter 6 - Linear model selection & regularizaiton - Introduction to Statistical Learning\nhttp://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/"
  },
  {
    "objectID": "p03-20-feature-engineering.html#feature-engineering-cho-các-biến-nhóm",
    "href": "p03-20-feature-engineering.html#feature-engineering-cho-các-biến-nhóm",
    "title": "29  Feature Engineering",
    "section": "29.1 Feature engineering cho các biến nhóm",
    "text": "29.1 Feature engineering cho các biến nhóm\n\n29.1.1 Tạo dữ liệu giả (dummy data) cho biến không phân biệt thứ tự\nTrong phương pháp này, toàn bộ các dữ liệu gốc được chuyển sang dạng 0-1. Tuy nhiên, dữ liệu mới được tạo ra sẽ ít hơn dữ liệu gốc 1 trường hợp. Bởi lẽ khi biết giá trị của 6 biến, ta có thể biết được giá trị của biến cuối cùng.\n\n\n\n\n\n\n\n\n\n\n\n\nBiến gốc\nMon\nTues\nWed\nThurs\nFri\nSat\n\n\n\n\nSun\n0\n0\n0\n0\n0\n0\n\n\nMon\n1\n0\n0\n0\n0\n0\n\n\nTues\n0\n1\n0\n0\n0\n0\n\n\nWed\n0\n0\n1\n0\n0\n0\n\n\nThurs\n0\n0\n0\n1\n0\n0\n\n\nFri\n0\n0\n0\n0\n1\n0\n\n\nSat\n0\n0\n0\n0\n0\n1\n\n\n\nzero-variance predictor: là biến chỉ có một giá trị. Khi xây dựng mô hình, ta cần loại biến này.\n\n\n29.1.2 Dữ liệu có rất nhiều nhóm\nĐối với các biến có rất nhiều nhóm (ví dụ: 200 chi nhánh trong ngân hàng), ta có 2 cách tiếp cận.\n\nCách một, dựa vào kiến thức nghiệp vụ tự nhóm. Ví dụ, các chi nhánh ở Hà Nội sẽ đánh dấu là HN, ở Hồ Chí Minh là HCM, các chi nhánh còn lại là Others.\nCách hai, sử dụng hash function. Trong trường hợp này, các biến category sẽ được tạo thành một biến hoàn toàn mới có giá trị số. Xem ví dụ dưới đây.\n\n\n\n\n\n\n\n\n Giá trị\nHash\n\n\n\n\nbelvedere tiburon\n58275378\n\n\nberkeley\n1166288024\n\n\n\nLưu ý: Nhiều thí nghiệm đã được sử dụng để so sánh sự khác biệt giữa việc dùng factor và encoding 0-1 trong dữ liệu. Kết quả cho thấy không có nhiều sự khác biệt giữa hai cách."
  },
  {
    "objectID": "p03-20-feature-engineering.html#các-biến-liên-tục",
    "href": "p03-20-feature-engineering.html#các-biến-liên-tục",
    "title": "29  Feature Engineering",
    "section": "29.2 Các biến liên tục",
    "text": "29.2 Các biến liên tục\nĐối với các biến liên tục, khi xây dựng mô hình, ta sẽ gặp phải các vấn đề sau.\n\nCác biến có các đơn vị khác nhau. Ví dụ, tuổi có giá trị từ 15-75, thu nhập có giá trị từ 2 triệu VND đến 200 triệu VND\nCác biến bị lệch sang phải (skewness)\nCác biến có xuất hiện giá trị ngoại lai (outliers)\nCác biến có thể bị chặn trai hoặc chặn phải. Ví dụ, độ tuổi có giá trị không quá 80\n\nĐối với các biến số, có ba nhóm kỹ thuật lớn biến đổi dữ liệu.\n\nBiến đổi 1:1 - một biến được biến đổi thành một biến khác\nBiến đổi 1:n - một biến được biến đổi thành nhiều biến khác nhau\nBiến đổi n:n - n biến gốc được biến đổi cùng lúc thành n biến khác\n\n\n29.2.1 Biến đổi 1:1\nTrong biến đổi 1:1, có rất nhiều cách khác nhau.\n\nBiến đổi theo scale của dữ liệu: log, Box-Cox\n\n\\[x^{*} = \\left\\{ \\begin{array}{l l} \\frac{x^{\\lambda}-1}{\\lambda\\: \\tilde{x}^{\\lambda-1}}, & \\lambda \\neq 0 \\\\ \\tilde{x} \\: \\log x, & \\lambda = 0 \\\\ \\end{array} \\right.\\]"
  },
  {
    "objectID": "p03-30-credit-scoring.html#score-card",
    "href": "p03-30-credit-scoring.html#score-card",
    "title": "30  Credit Scoring",
    "section": "30.1 Score card",
    "text": "30.1 Score card\nKhi xây dựng scorecard, cần chia bucket để dễ dàng hơn trong việc tính điểm\nCác định nghĩa quan trọng:\n\nWOE (Weight of Evidence): Đo lường độ mạnh của việc phân loại good vs. bad giữa một nhóm (VD: age 27-29). Nếu WOE có giá trị âm càng lớn, càng thể hiện khả năng phân nhóm khoản vay xấu.\n\n\\[WOE = ln(\\frac{Distr\\:Good}{Distr\\: Bad})\\]\n\nIV (Informatio Value): Đo lường độ mạnh dự báo của biến dự báo với biến cần được dự báo (response). Các đặc tính như sau:\n\nIV < 0.02: Không có giá trị dự báo\n0.02 - 0.1: thấp\n0.1 - 0.3: trung bình\n0.3 - 0.5: cao\nTrên 0.5: Cần kiểm tra lại\n\n\n\\[IV = \\sum_{i = 1}^{n}(Distr\\:Good - Distr\\:Bad)*ln(\\frac{Distr\\:Good}{Distr\\:Bad})\\]\n\n\n\n\n\n\nCác quy tắc khi xây dựng:\n\nMỗi bucket có tối thiểu 5%\nKhi nhóm các biến, cần đảm báo quy tắc WOE sau khi nhóm là tuyến tính (đồng biến hoặc nghịch biến)\n\nLưu ý: Xem thêm file excel (data/woe-example.xlsx)để hiểu rõ hơn cách tính WOE và IV"
  },
  {
    "objectID": "p03-30-credit-scoring.html#các-bước-xây-dựng-score-card",
    "href": "p03-30-credit-scoring.html#các-bước-xây-dựng-score-card",
    "title": "30  Credit Scoring",
    "section": "30.2 Các bước xây dựng score card",
    "text": "30.2 Các bước xây dựng score card\n\nBổ sung các thông tin dữ liệu bị thiếu\nTính toán WOE và IV, loại các biến có IV quá thấp, kiểm tra lại các biến có IV quá cao\nTạo tập mới là tập df_woe, các biến đã được nhóm lại để tối ưu hóa WOE\nXây dựng mô hình logistic đơn giản trên tập df_woe\nScore dữ liệu mới, việc xây dựng score card cần thực hiện hai việc:\n\nXác định odds tại mốc điểm scorecard nhất định\nXác định điểm score card giúp tăng gấp đôi odds"
  },
  {
    "objectID": "p03-30-credit-scoring.html#các-tính-score",
    "href": "p03-30-credit-scoring.html#các-tính-score",
    "title": "30  Credit Scoring",
    "section": "30.3 Các tính score",
    "text": "30.3 Các tính score\n\nĐiểm mặc định (point 0) thường được dùng là 600. Mốc điểm này phản ánh một tỷ lệ odd cho trước (ví dụ: 30)\nĐiểm tăng gấp đôi odd (point double ood - pdo): Là số điểm mà cứ tăng thêm pdo, odd sẽ tăng gấp đôi - thường là 20. Xem minh họa dưới đây\n\n\n\n\n\n\n\n\n\nScore\nOdd\n\n\n\n\n600\n30\n\n\n601\n31\n\n\n…\n….\n\n\n620\n60"
  },
  {
    "objectID": "p03-30-credit-scoring.html#xây-dựng-score-card",
    "href": "p03-30-credit-scoring.html#xây-dựng-score-card",
    "title": "30  Credit Scoring",
    "section": "30.4 Xây dựng score card",
    "text": "30.4 Xây dựng score card\n\ndata <- read.csv(\"http://www.creditriskanalytics.net/uploads/1/9/5/1/19511601/hmeq.csv\")\nsave(data, file = \"data/hmeq.rda\")\n\n\nlibrary(tidyverse)\nlibrary(scorecard)\nload(\"data/hmeq.rda\")\nnames(data) <- names(data) %>% tolower\ndata %>% head\n\n  bad loan mortdue  value  reason    job  yoj derog delinq     clage ninq clno\n1   1 1100   25860  39025 HomeImp  Other 10.5     0      0  94.36667    1    9\n2   1 1300   70053  68400 HomeImp  Other  7.0     0      2 121.83333    0   14\n3   1 1500   13500  16700 HomeImp  Other  4.0     0      0 149.46667    1   10\n4   1 1500      NA     NA                  NA    NA     NA        NA   NA   NA\n5   0 1700   97800 112000 HomeImp Office  3.0     0      0  93.33333    0   14\n6   1 1700   30548  40320 HomeImp  Other  9.0     0      0 101.46600    1    8\n   debtinc\n1       NA\n2       NA\n3       NA\n4       NA\n5       NA\n6 37.11361\n\n\n\nKiểm tra dữ liệu missing\n\n\ncheck_na <- function(x) {\n  (x %>% is.na %>% sum)/length(x)\n}\ndata %>% map_df(check_na)\n\n# A tibble: 1 × 13\n    bad  loan mortdue  value reason   job    yoj derog delinq  clage   ninq\n  <dbl> <dbl>   <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl>\n1     0     0  0.0869 0.0188      0     0 0.0864 0.119 0.0973 0.0517 0.0856\n# ℹ 2 more variables: clno <dbl>, debtinc <dbl>\n\ndata %>% summary\n\n      bad              loan          mortdue           value       \n Min.   :0.0000   Min.   : 1100   Min.   :  2063   Min.   :  8000  \n 1st Qu.:0.0000   1st Qu.:11100   1st Qu.: 46276   1st Qu.: 66076  \n Median :0.0000   Median :16300   Median : 65019   Median : 89236  \n Mean   :0.1995   Mean   :18608   Mean   : 73761   Mean   :101776  \n 3rd Qu.:0.0000   3rd Qu.:23300   3rd Qu.: 91488   3rd Qu.:119824  \n Max.   :1.0000   Max.   :89900   Max.   :399550   Max.   :855909  \n                                  NA's   :518      NA's   :112     \n     reason          job            yoj             derog        \n        : 252          : 279   Min.   : 0.000   Min.   : 0.0000  \n DebtCon:3928   Mgr    : 767   1st Qu.: 3.000   1st Qu.: 0.0000  \n HomeImp:1780   Office : 948   Median : 7.000   Median : 0.0000  \n                Other  :2388   Mean   : 8.922   Mean   : 0.2546  \n                ProfExe:1276   3rd Qu.:13.000   3rd Qu.: 0.0000  \n                Sales  : 109   Max.   :41.000   Max.   :10.0000  \n                Self   : 193   NA's   :515      NA's   :708      \n     delinq            clage             ninq             clno     \n Min.   : 0.0000   Min.   :   0.0   Min.   : 0.000   Min.   : 0.0  \n 1st Qu.: 0.0000   1st Qu.: 115.1   1st Qu.: 0.000   1st Qu.:15.0  \n Median : 0.0000   Median : 173.5   Median : 1.000   Median :20.0  \n Mean   : 0.4494   Mean   : 179.8   Mean   : 1.186   Mean   :21.3  \n 3rd Qu.: 0.0000   3rd Qu.: 231.6   3rd Qu.: 2.000   3rd Qu.:26.0  \n Max.   :15.0000   Max.   :1168.2   Max.   :17.000   Max.   :71.0  \n NA's   :580       NA's   :308      NA's   :510      NA's   :222   \n    debtinc        \n Min.   :  0.5245  \n 1st Qu.: 29.1400  \n Median : 34.8183  \n Mean   : 33.7799  \n 3rd Qu.: 39.0031  \n Max.   :203.3121  \n NA's   :1267      \n\n\n\n30.4.1 Điền thêm missing value\n\n# Thay NA bằng mean\nreplace_by_mean <- function(x) {\n  x[is.na(x)] <- mean(x, na.rm = TRUE)\n  return(x)\n}\n\n# Thay NA bằng category\n\nreplace_na_categorical <- function(x) {\n  x %>% \n    table() %>% \n    as.data.frame() %>% \n    arrange(-Freq) ->> my_df\n  \n  n_obs <- sum(my_df$Freq)\n  pop <- my_df$. %>% as.character()\n  set.seed(129)\n  x[is.na(x)] <- sample(pop, sum(is.na(x)), \n                        replace = TRUE, \n                        prob = my_df$Freq)\n  return(x)\n}\n\n\ndf <- data %>% \n  mutate_if(is.factor, as.character) %>% \n  mutate(reason = case_when(reason == \"\" ~ NA_character_, \n                            TRUE ~ reason), \n         job = case_when(job == \"\" ~ NA_character_, \n                         TRUE ~ job)) %>%\n  mutate_if(is_character, as.factor) %>% \n  mutate_if(is.numeric, replace_by_mean) %>% \n  mutate_if(is.factor, replace_na_categorical)\n\n\nPhân chia train/test\n\n\ntrain <- df %>% \n  group_by(bad) %>% \n  sample_frac(0.5) %>% \n  ungroup() \n \ntest <- setdiff(df, train)  \n\n\nXây dựng WOE\n\n\nlibrary(scorecard)\nbins_var <- woebin(train, y = \"bad\", no_cores = 1, positive = \"bad|1\")\n\n✔ Binning on 2980 rows and 13 columns in 00:00:04\n\nbins_var$derog\n\n   variable                 bin count count_distr  neg pos   posprob        woe\n1:    derog [-Inf,0.2545696877)  2262   0.7590604 1882 380 0.1679929 -0.2094248\n2:    derog    [0.2545696877,1)   341   0.1144295  303  38 0.1114370 -0.6856524\n3:    derog            [1, Inf)   377   0.1265101  201 176 0.4668435  1.2576734\n       bin_iv  total_iv       breaks is_special_values\n1: 0.03121208 0.3411161 0.2545696877             FALSE\n2: 0.04320825 0.3411161            1             FALSE\n3: 0.26669579 0.3411161          Inf             FALSE\n\n# Lọc các biến có woe cao\nbins_var$loan %>% select(total_iv) %>% head(1)\n\n    total_iv\n1: 0.1586945\n\n# Kiểm tra IV cho tất cả các biến\nbins_var %>% map_df(function(df){df %>% select(total_iv) %>% head(1) %>% pull})\n\n# A tibble: 1 × 12\n   loan mortdue value  reason    job    yoj derog delinq clage  ninq   clno\n  <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl> <dbl>  <dbl>\n1 0.159  0.0555 0.174 0.00604 0.0833 0.0895 0.341  0.575 0.207 0.107 0.0665\n# ℹ 1 more variable: debtinc <dbl>\n\n# Biểu đồ WOE cho biến\nbins_var$loan \n\n   variable           bin count count_distr  neg pos   posprob         woe\n1:     loan   [-Inf,6000)   152  0.05100671   87  65 0.4276316  1.09897343\n2:     loan  [6000,13000)   868  0.29127517  683 185 0.2131336  0.08435525\n3:     loan [13000,15000)   249  0.08355705  212  37 0.1485944 -0.35517408\n4:     loan [15000,16000)   190  0.06375839  135  55 0.2894737  0.49255269\n5:     loan [16000,38000)  1339  0.44932886 1134 205 0.1530993 -0.32000222\n6:     loan  [38000, Inf)   182  0.06107383  135  47 0.2582418  0.33536711\n        bin_iv  total_iv breaks is_special_values\n1: 0.080186498 0.1586945   6000             FALSE\n2: 0.002125302 0.1586945  13000             FALSE\n3: 0.009434160 0.1586945  15000             FALSE\n4: 0.017738075 0.1586945  16000             FALSE\n5: 0.041649754 0.1586945  38000             FALSE\n6: 0.007560693 0.1586945    Inf             FALSE\n\np <- woebin_plot(bins_var$derog)\np$derog\n\n\n\n\nLưu ý: Trong package scorecard, WOE được tính đảo ngược lại, nghĩa là \\(WOE = ln(\\frac{Distr\\:Bad}{Distr\\:Good})\\). Do đó, với bucket nào có hệ số dương, nhóm đó có xu hướng laf\n\n# Tạo dataframe woe\ntrain_woe <- woebin_ply(train, bins_var)\n\n✔ Woe transformating on 2980 rows and 12 columns in 00:00:02\n\n# Logistic Regression:\nmy_logistic <- glm(bad ~ ., family = binomial, data = train_woe)\n\n# Kết quả\nmy_logistic %>% summary()\n\n\nCall:\nglm(formula = bad ~ ., family = binomial, data = train_woe)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3874  -0.4542  -0.2655  -0.1434   2.9964  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.39388    0.06267 -22.241  < 2e-16 ***\nloan_woe     0.55661    0.15662   3.554 0.000379 ***\nmortdue_woe  0.45576    0.26473   1.722 0.085139 .  \nvalue_woe    0.72577    0.14587   4.975 6.51e-07 ***\nreason_woe   1.06208    0.81259   1.307 0.191202    \njob_woe      0.93845    0.21525   4.360 1.30e-05 ***\nyoj_woe      1.05720    0.21329   4.957 7.17e-07 ***\nderog_woe    0.67661    0.09936   6.810 9.77e-12 ***\ndelinq_woe   0.95015    0.07827  12.140  < 2e-16 ***\nclage_woe    0.83732    0.13584   6.164 7.10e-10 ***\nninq_woe     0.36784    0.18398   1.999 0.045569 *  \nclno_woe     0.92563    0.22804   4.059 4.93e-05 ***\ndebtinc_woe  0.91605    0.04644  19.727  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2976.8  on 2979  degrees of freedom\nResidual deviance: 1831.7  on 2967  degrees of freedom\nAIC: 1857.7\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nXây dựng scorecard\n\n\n# Calculate scorecard scores for variables based on the results from woebin and glm: \nmy_card <- scorecard(bins_var, my_logistic, points0 = 600, odds0 = 1/19, pdo = 50)\nmy_card\n\n$basepoints\n     variable bin woe points\n1: basepoints  NA  NA    488\n\n$loan\n   variable           bin count count_distr  neg pos   posprob         woe\n1:     loan   [-Inf,6000)   152  0.05100671   87  65 0.4276316  1.09897343\n2:     loan  [6000,13000)   868  0.29127517  683 185 0.2131336  0.08435525\n3:     loan [13000,15000)   249  0.08355705  212  37 0.1485944 -0.35517408\n4:     loan [15000,16000)   190  0.06375839  135  55 0.2894737  0.49255269\n5:     loan [16000,38000)  1339  0.44932886 1134 205 0.1530993 -0.32000222\n6:     loan  [38000, Inf)   182  0.06107383  135  47 0.2582418  0.33536711\n        bin_iv  total_iv breaks is_special_values points\n1: 0.080186498 0.1586945   6000             FALSE    -44\n2: 0.002125302 0.1586945  13000             FALSE     -3\n3: 0.009434160 0.1586945  15000             FALSE     14\n4: 0.017738075 0.1586945  16000             FALSE    -20\n5: 0.041649754 0.1586945  38000             FALSE     13\n6: 0.007560693 0.1586945    Inf             FALSE    -13\n\n$mortdue\n   variable            bin count count_distr neg pos   posprob         woe\n1:  mortdue   [-Inf,30000)   323  0.10838926 233  90 0.2786378  0.43926550\n2:  mortdue  [30000,55000)   659  0.22114094 513 146 0.2215478  0.13382506\n3:  mortdue  [55000,75000)   950  0.31879195 765 185 0.1947368 -0.02902573\n4:  mortdue  [75000,85000)   260  0.08724832 224  36 0.1384615 -0.43763283\n5:  mortdue [85000,145000)   612  0.20536913 513  99 0.1617647 -0.25466171\n6:  mortdue  [145000, Inf)   176  0.05906040 138  38 0.2159091  0.10082676\n         bin_iv   total_iv breaks is_special_values points\n1: 0.0236597956 0.05553666  30000             FALSE    -14\n2: 0.0041200794 0.05553666  55000             FALSE     -4\n3: 0.0002662377 0.05553666  75000             FALSE      1\n4: 0.0145621938 0.05553666  85000             FALSE     14\n5: 0.0123097167 0.05553666 145000             FALSE      8\n6: 0.0006186406 0.05553666    Inf             FALSE     -3\n\n$value\n   variable             bin count count_distr  neg pos   posprob        woe\n1:    value    [-Inf,45000)   220  0.07382550  142  78 0.3545455  0.7913761\n2:    value   [45000,90000)  1269  0.42583893 1039 230 0.1812451 -0.1174404\n3:    value  [90000,100000)   285  0.09563758  247  38 0.1333333 -0.4813079\n4:    value [100000,105000)   204  0.06845638  124  80 0.3921569  0.9522394\n5:    value   [105000, Inf)  1002  0.33624161  834 168 0.1676647 -0.2117751\n        bin_iv  total_iv breaks is_special_values points\n1: 0.056820247 0.1744091  45000             FALSE    -41\n2: 0.005666669 0.1744091  90000             FALSE      6\n3: 0.019034511 0.1744091 100000             FALSE     25\n4: 0.078760013 0.1744091 105000             FALSE    -50\n5: 0.014127667 0.1744091    Inf             FALSE     11\n\n$reason\n   variable     bin count count_distr  neg pos   posprob         woe\n1:   reason DebtCon  2063   0.6922819 1669 394 0.1909840 -0.05313473\n2:   reason HomeImp   917   0.3077181  717 200 0.2181025  0.11373581\n        bin_iv   total_iv  breaks is_special_values points\n1: 0.001923339 0.00604028 DebtCon             FALSE      4\n2: 0.004116941 0.00604028 HomeImp             FALSE     -9\n\n$job\n   variable                    bin count count_distr neg pos   posprob\n1:      job                    Mgr   409   0.1372483 312  97 0.2371638\n2:      job                 Office   494   0.1657718 434  60 0.1214575\n3:      job                  Other  1278   0.4288591 978 300 0.2347418\n4:      job ProfExe%,%Sales%,%Self   799   0.2681208 662 137 0.1714643\n          woe      bin_iv   total_iv                 breaks is_special_values\n1:  0.2222021 0.007229762 0.08332294                    Mgr             FALSE\n2: -0.5882057 0.047576595 0.08332294                 Office             FALSE\n3:  0.2087671 0.019866166 0.08332294                  Other             FALSE\n4: -0.1847903 0.008650419 0.08332294 ProfExe%,%Sales%,%Self             FALSE\n   points\n1:    -15\n2:     40\n3:    -14\n4:     13\n\n$yoj\n   variable       bin count count_distr neg pos   posprob         woe\n1:      yoj  [-Inf,5)   975  0.32718121 748 227 0.2328205  0.19804132\n2:      yoj     [5,6)   173  0.05805369 122  51 0.2947977  0.51829887\n3:      yoj     [6,9)   693  0.23255034 591 102 0.1471861 -0.36634892\n4:      yoj    [9,21)   874  0.29328859 691 183 0.2093822  0.06184061\n5:      yoj [21, Inf)   265  0.08892617 234  31 0.1169811 -0.63083963\n        bin_iv   total_iv breaks is_special_values points\n1: 0.013597417 0.08951837      5             FALSE    -15\n2: 0.017998957 0.08951837      6             FALSE    -40\n3: 0.027834354 0.08951837      9             FALSE     28\n4: 0.001142491 0.08951837     21             FALSE     -5\n5: 0.028945152 0.08951837    Inf             FALSE     48\n\n$derog\n   variable                 bin count count_distr  neg pos   posprob        woe\n1:    derog [-Inf,0.2545696877)  2262   0.7590604 1882 380 0.1679929 -0.2094248\n2:    derog    [0.2545696877,1)   341   0.1144295  303  38 0.1114370 -0.6856524\n3:    derog            [1, Inf)   377   0.1265101  201 176 0.4668435  1.2576734\n       bin_iv  total_iv       breaks is_special_values points\n1: 0.03121208 0.3411161 0.2545696877             FALSE     10\n2: 0.04320825 0.3411161            1             FALSE     33\n3: 0.26669579 0.3411161          Inf             FALSE    -61\n\n$delinq\n   variable                 bin count count_distr  neg pos   posprob        woe\n1:   delinq [-Inf,0.4494423792)  2084  0.69932886 1800 284 0.1362764 -0.4560734\n2:   delinq    [0.4494423792,1)   289  0.09697987  258  31 0.1072664 -0.7284781\n3:   delinq            [1, Inf)   607  0.20369128  328 279 0.4596376  1.2286925\n       bin_iv total_iv       breaks is_special_values points\n1: 0.12600679 0.574966 0.4494423792             FALSE     31\n2: 0.04075268 0.574966            1             FALSE     50\n3: 0.40820653 0.574966          Inf             FALSE    -84\n\n$clage\n   variable        bin count count_distr neg pos   posprob         woe\n1:    clage  [-Inf,70)   158  0.05302013 101  57 0.3607595  0.81842503\n2:    clage   [70,150)  1037  0.34798658 770 267 0.2574735  0.33135243\n3:    clage  [150,230)  1048  0.35167785 853 195 0.1860687 -0.08526571\n4:    clage [230, Inf)   737  0.24731544 662  75 0.1017639 -0.78728316\n        bin_iv  total_iv breaks is_special_values points\n1: 0.043891591 0.2074202     70             FALSE    -49\n2: 0.042008565 0.2074202    150             FALSE    -20\n3: 0.002491401 0.2074202    230             FALSE      5\n4: 0.119028692 0.2074202    Inf             FALSE     48\n\n$ninq\n   variable      bin count count_distr  neg pos   posprob        woe\n1:     ninq [-Inf,2)  2229  0.74798658 1843 386 0.1731718 -0.1728183\n2:     ninq    [2,3)   378  0.12684564  295  83 0.2195767  0.1223595\n3:     ninq    [3,4)   182  0.06107383  131  51 0.2802198  0.4471226\n4:     ninq [4, Inf)   191  0.06409396  117  74 0.3874346  0.9323854\n        bin_iv total_iv breaks is_special_values points\n1: 0.021185937 0.107431      2             FALSE      5\n2: 0.001969102 0.107431      3             FALSE     -3\n3: 0.013840672 0.107431      4             FALSE    -12\n4: 0.070435269 0.107431    Inf             FALSE    -25\n\n$clno\n   variable       bin count count_distr  neg pos   posprob         woe\n1:     clno  [-Inf,9)   232  0.07785235  159  73 0.3146552  0.61204952\n2:     clno    [9,27)  2038  0.68389262 1679 359 0.1761531 -0.15213699\n3:     clno   [27,36)   455  0.15268456  363  92 0.2021978  0.01788003\n4:     clno [36, Inf)   255  0.08557047  185  70 0.2745098  0.41863370\n         bin_iv   total_iv breaks is_special_values points\n1: 3.443201e-02 0.06646488      9             FALSE    -41\n2: 1.510889e-02 0.06646488     27             FALSE     10\n3: 4.907506e-05 0.06646488     36             FALSE     -1\n4: 1.687491e-02 0.06646488    Inf             FALSE    -28\n\n$debtinc\n   variable       bin count count_distr  neg pos    posprob        woe\n1:  debtinc [-Inf,31)   803  0.26946309  756  47 0.05853051 -1.3873995\n2:  debtinc   [31,33)   175  0.05872483  156  19 0.10857143 -0.7149227\n3:  debtinc   [33,34)   703  0.23590604  316 387 0.55049787  1.5931768\n4:  debtinc   [34,42)  1128  0.37852349 1043  85 0.07535461 -1.1167109\n5:  debtinc [42, Inf)   171  0.05738255  115  56 0.32748538  0.6709138\n       bin_iv total_iv breaks is_special_values points\n1: 0.32981774  1.53994     31             FALSE     92\n2: 0.02387474  1.53994     33             FALSE     47\n3: 0.82697970  1.53994     34             FALSE   -105\n4: 0.32835280  1.53994     42             FALSE     74\n5: 0.03091455  1.53994    Inf             FALSE    -44\n\n\nLưu ý:\n\nDefault là 600 điểm tương ứng với odd là 19\npdo là 50\nBasepoint là điểm offset 485 điểm\n\n\n# So sánh điểm score tương ứng với prob trên train\nscore <-  scorecard_ply(train, my_card)$score\npred <- predict(my_logistic, train_woe, type = \"response\")\ndata.frame(score = score,\n           pred = pred) %>% \n  mutate(odds = (1-pred)/pred %>% round(2)) %>% \n  filter(score %in% c(600, 650, 700)) %>% \n  arrange(score)\n\n     score       pred     odds\n220    600 0.04893927 19.02121\n1153   600 0.05024819 18.99504\n1916   600 0.04984767 19.00305\n1989   600 0.04970749 19.00585\n2068   600 0.05033726 18.99325\n2807   600 0.05024819 18.99504\n2961   600 0.05024819 18.99504\n79     650 0.02543880 32.48537\n638    650 0.02555429 32.48152\n955    650 0.02555429 32.48152\n971    650 0.02534197 32.48860\n1398   650 0.02571808 32.47606\n1762   650 0.02571808 32.47606\n2078   650 0.02571808 32.47606\n2141   650 0.02568026 32.47732\n2263   650 0.02555429 32.48152\n540    700 0.01300547 98.69945\n636    700 0.01305757 98.69424\n802    700 0.01313392 98.68661\n810    700 0.01294695 98.70531\n1402   700 0.01294695 98.70531\n1435   700 0.01294459 98.70554\n1793   700 0.01294695 98.70531\n1854   700 0.01294695 98.70531\n1855   700 0.01305757 98.69424\n\n\nDự báo với tập mới:\n\ntest_woe <- woebin_ply(test, bins_var)\n\n✔ Woe transformating on 2980 rows and 12 columns in 00:00:02\n\ntest_pred <- predict(my_logistic, test_woe, type = \"response\")\n\n\nperf_eva(test_pred, test$bad,  \n         type = c(\"ks\", \"lift\", \"roc\", \"pr\"), \n         title = \"Test Data\")\n\n\n\n\n$binomial_metric\n$binomial_metric$`Test Data`\n          MSE      RMSE   LogLoss        R2        KS       AUC      Gini\n1: 0.09501332 0.3082423 0.3125839 0.4054181 0.6188503 0.8871089 0.7742177\n\n\n$pic\nTableGrob (1 x 2) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (1-1,2-2) arrange gtable[layout]\n\ntest_woe %>% head\n\n   bad loan_woe mortdue_woe  value_woe  reason_woe    job_woe     yoj_woe\n1:   1 1.098973  0.43926550  0.7913761  0.11373581  0.2087671  0.06184061\n2:   1 1.098973 -0.02902573 -0.1174404  0.11373581  0.2087671 -0.36634892\n3:   1 1.098973  0.43926550  0.7913761  0.11373581  0.2087671  0.19804132\n4:   1 1.098973 -0.02902573  0.9522394 -0.05313473  0.2087671 -0.36634892\n5:   0 1.098973 -0.25466171 -0.2117751  0.11373581 -0.5882057  0.19804132\n6:   1 1.098973  0.43926550  0.7913761  0.11373581  0.2087671  0.06184061\n    derog_woe delinq_woe   clage_woe   ninq_woe   clno_woe debtinc_woe\n1: -0.2094248 -0.4560734  0.33135243 -0.1728183 -0.1521370    1.593177\n2: -0.2094248  1.2286925  0.33135243 -0.1728183 -0.1521370    1.593177\n3: -0.2094248 -0.4560734  0.33135243 -0.1728183 -0.1521370    1.593177\n4: -0.6856524 -0.4560734 -0.08526571 -0.1728183 -0.1521370    1.593177\n5: -0.2094248 -0.4560734  0.33135243 -0.1728183 -0.1521370    1.593177\n6: -0.2094248 -0.4560734  0.33135243 -0.1728183  0.6120495   -1.116711\n\nscore_result <- scorecard_ply(test, my_card, only_total_score = F)\nscore_result %>% head\n\n   loan_points mortdue_points value_points reason_points job_points yoj_points\n1:         -44            -14          -41            -9        -14         -5\n2:         -44              1            6            -9        -14         28\n3:         -44            -14          -41            -9        -14        -15\n4:         -44              1          -50             4        -14         28\n5:         -44              8           11            -9         40        -15\n6:         -44            -14          -41            -9        -14         -5\n   derog_points delinq_points clage_points ninq_points clno_points\n1:           10            31          -20           5          10\n2:           10           -84          -20           5          10\n3:           10            31          -20           5          10\n4:           33            31            5           5          10\n5:           10            31          -20           5          10\n6:           10            31          -20           5         -41\n   debtinc_points score\n1:           -105   292\n2:           -105   272\n3:           -105   282\n4:           -105   392\n5:           -105   410\n6:             74   420\n\n\n\nresult <- data.frame(predict = test_pred, actual = test$bad)\nresult %>% head\n\n    predict actual\n1 0.7911585      1\n2 0.8329938      1\n3 0.8139546      1\n4 0.4835735      1\n5 0.4215718      0\n6 0.3909958      1\n\n# result %>% VPBank::model_performance(predict, actual)\n\n\n\n30.4.2 Đo lường độ ổn định mô hình\nĐộ ổn định\n\nindex <- read.table(textConnection(\n  c(\"Score_bands    Actual  Expected    Diff    log_Ac_Ex   Index\n< 251   5%  8%  -3% -0.47   0.014\n251–290 6%  9%  -3% -0.41   0.012\n291–320 6%  10% -4% -0.51   0.020\n321–350 8%  13% -5% -0.49   0.024\n351–380 10% 12% -2% -0.18   0.004\n381–410 12% 11% 1%  0.09    0.001\n411–440 14% 10% 4%  0.34    0.013\n441–470 14% 9%  5%  0.44    0.022\n471–520 13% 9%  4%  0.37    0.015\n520 <   9%  8%  1%  0.12    0.001\")\n), header = T, sep = \"\\t\")\nindex\n\n   Score_bands Actual Expected Diff log_Ac_Ex Index\n1        < 251     5%       8%  -3%     -0.47 0.014\n2      251–290     6%       9%  -3%     -0.41 0.012\n3      291–320     6%      10%  -4%     -0.51 0.020\n4      321–350     8%      13%  -5%     -0.49 0.024\n5      351–380    10%      12%  -2%     -0.18 0.004\n6      381–410    12%      11%   1%      0.09 0.001\n7      411–440    14%      10%   4%      0.34 0.013\n8      441–470    14%       9%   5%      0.44 0.022\n9      471–520    13%       9%   4%      0.37 0.015\n10       520 <     9%       8%   1%      0.12 0.001\n\n\n\\[log_Ac_Ex = log(\\frac{AC}{Ec})\\] \\[Diff = AC - EC\\] \\[Index = (AC - EC)*log(\\frac{AC}{EC})\\]\n\\[Population\\:Stability\\:Index\\:(PSI) ==  \\sum(Index) = 0.1269\\]\nQuy tắc:\n\nPSI < 0.1: Mô hình ổn định\n0.1 <= PSI <= 0.25: Kiểm tra lại mô hình\nPSI >= 0.25: Mô hình cần phải xây lại\n\n\ntrain_score <- scorecard_ply(train, my_card)\ntest_score <- scorecard_ply(test, my_card)\npsi <- perf_psi(\n  score = list(train = train_score, test = test_score),\n  label = list(train = train$bad, test = test$bad)\n)\n\npsi$psi  # psi dataframe\n\n   variable    dataset        psi\n1:    score train_test 0.01135422\n\np <- psi$pic\nlibrary(scales)\np$score\n\n\n\ndf <- train_score %>% \n  mutate(class = \"train\") %>% \n  mutate(actual = train$bad) %>% \n  bind_rows(test_score %>% \n              mutate(class = \"test\") %>% \n              mutate(actual = test$bad))\ndf %>% summary\n\n     score          class               actual      \n Min.   : 40.0   Length:5960        Min.   :0.0000  \n 1st Qu.:457.0   Class :character   1st Qu.:0.0000  \n Median :570.0   Mode  :character   Median :0.0000  \n Mean   :540.8                      Mean   :0.1995  \n 3rd Qu.:642.0                      3rd Qu.:0.0000  \n Max.   :815.0                      Max.   :1.0000  \n\ndf <- df %>% \n  mutate(bucket = cut(score, breaks = seq(150, 850, by = 50)))  %>% \n  mutate(bucket = as.character(bucket)) %>% \n  mutate(bucket = case_when(\n    score < 200 ~ \"(-50,200]\",\n    score > 750 ~ \"[750, 850)\",\n    TRUE ~ bucket\n  )) %>% \n  mutate(\n    class = as.factor(class),\n    actual = as.factor(actual),\n    bucket = as.factor(bucket)\n  )\n\ndf %>% \n  ggplot(aes(bucket)) +\n  geom_bar(aes(fill = class),\n           stat = \"count\",\n           position = \"dodge\") \n\n\n\np$score\n\n\n\ndf$bucket %>% levels\n\n [1] \"(-50,200]\"  \"(150,200]\"  \"(200,250]\"  \"(250,300]\"  \"(300,350]\" \n [6] \"(350,400]\"  \"(400,450]\"  \"(450,500]\"  \"(500,550]\"  \"(550,600]\" \n[11] \"(600,650]\"  \"(650,700]\"  \"(700,750]\"  \"[750, 850)\"\n\ndf %>% \n  filter(bucket == \"(300,350]\") %>% \n  group_by(class, actual) %>% \n  summarise(no = n()) %>% \n  ungroup %>% \n  group_by(class) %>% \n  mutate(perc = no/sum(no))\n\n# A tibble: 4 × 4\n# Groups:   class [2]\n  class actual    no  perc\n  <fct> <fct>  <int> <dbl>\n1 test  0         34 0.298\n2 test  1         80 0.702\n3 train 0         50 0.391\n4 train 1         78 0.609"
  },
  {
    "objectID": "p03-30-credit-scoring.html#tài-liệu-tham-khảo",
    "href": "p03-30-credit-scoring.html#tài-liệu-tham-khảo",
    "title": "30  Credit Scoring",
    "section": "30.5 Tài liệu tham khảo",
    "text": "30.5 Tài liệu tham khảo\n\nhttp://ucanalytics.com/blogs/information-value-and-weight-of-evidencebanking-case/\nIFRS 9 and CECL Credit Risk Modelling and ValidationA Practical Guide with Examples Worked in R and SAS"
  },
  {
    "objectID": "p03-40-interpretable-ml.html#giới-thiệu",
    "href": "p03-40-interpretable-ml.html#giới-thiệu",
    "title": "31  Giải thích mô hình học máy",
    "section": "31.1 Giới thiệu",
    "text": "31.1 Giới thiệu\nKhi xây dựng các mô hình học máy, ta sẽ phải đánh đổi giữa độ chính xác và độ phức tạp của mô hình. Mô hình có khả năng dự báo càng chính xác thì khả năng dự báo càng thấp với ngược lại. Ví dụ, các mô hình như OLS, logistics, decision tree có khả năng giải thích tốt nhưng khả năng dự báo lại hạn chế. Các mô hình có khả năng dự báo tốt nhất là các mô hình dạng black box như random forest, xgboost, neural network. Tuy nhiên, khi ứng dụng mô hình vào thực tế, đơn vị sử dụng (đơn vị kinh doanh) nhiều khi sẽ thiếu sự tin tưởng vào kết quả dự báo của mô hình do không biết cách thức mô hình dự báo trong từng trường hợp cụ thể.\nĐể khắc phục nhược điểm này, từ năm 2016 xuất hiện nhánh phân tích mới nghiên cứu việc giải thích các mô hình black box. Trong các hướng phân tích mới này, có 3 nhóm thuật toán chính là LIME, SHAP và DALEX."
  },
  {
    "objectID": "p03-40-interpretable-ml.html#khả-năng-giải-thích-toàn-bộ-và-cục-bộ",
    "href": "p03-40-interpretable-ml.html#khả-năng-giải-thích-toàn-bộ-và-cục-bộ",
    "title": "31  Giải thích mô hình học máy",
    "section": "31.2 Khả năng giải thích toàn bộ và cục bộ",
    "text": "31.2 Khả năng giải thích toàn bộ và cục bộ\nVới mô hình tuyến tính, hệ số beta có thể được sử dụng để dự báo với tất cả các giá trị của x. Ví dụ, với mô hình \\(y = 3x\\), \\(\\beta=3\\) được sử dụng để dự báo với cả giá trị \\(x=1\\) hoặc \\(x=100\\). Mô hình có các tham số cố định và được sử dụng với toàn bộ dữ liệu được gọi là mô hình có khả năng giải thích toàn bộ (global fidelity).\nNgược lại với mô hình trên, các mô hình như random forest hay svm, không gian dùng để phân loại không có khả năng giải thích toàn bộ cao. Tuy nhiên, với mỗi quan sát, luôn tồn tại một mô hình đơn giản có khả năng giải thích (OLS, logistics) tốt. Các mô hình này chỉ có thể dự báo tại điểm đó hoặc 1 số điểm lân cận đủ gần mà không thể dùng để dự báo tại các điểm khác. Các mô hình loại này được gọi là mô hình chỉ có khả năng giải thích cục bộ (local fidelity)."
  },
  {
    "objectID": "p03-40-interpretable-ml.html#mô-hình-thay-thế-cục-bộ",
    "href": "p03-40-interpretable-ml.html#mô-hình-thay-thế-cục-bộ",
    "title": "31  Giải thích mô hình học máy",
    "section": "31.3 Mô hình thay thế cục bộ",
    "text": "31.3 Mô hình thay thế cục bộ\nCác mô hình thay thế cục bộ (surrogate model) là các mô hình đơn giản, có khả năng giải thích được tạo ra bằng các dữ liệu giả lập. Mô hình LIME, SHAP là các mô hình dạng này. Mô hình được diễn ra như sau.\n\nXây dựng mô hình \\(g(x)\\) với \\(x\\), \\(y\\) là các biến đầu vào và đầu ra\nChọn 1 điểm cần giải thích trong mô hình là \\(x_i\\)\nTạo thêm các biến \\(x'\\) bằng việc điều chỉnh \\(x + \\Delta x\\).\nDự báo kết quả \\(y' = g(x') = g(x + \\Delta x)\\)\nXây dựng mô hình \\(f(x')\\) với biến đầu vào mới \\(x'\\) và đầu ra mới \\(y'\\)\n\n\nDữ liệu đầu vào có khả năng giải thích:\nKhi dự báo với dữ liệu mới, dữ liệu đầu vào sẽ phải thực hiện qua bước feature engineering để có thể làm việc dễ dàng hơn với surrogate model nhưng cũng phải đảm bảo yếu tố thân thiện với người dùng để dễ đánh giá chính xác tác động của từng biến. Ví dụ, dữ liệu ảnh trong mô hình sẽ được chuyển đổi thành các tensor 2d. Tuy nhiên, dữ liệu đó lại không thân thiện với người ra quyết định mà sẽ được chuyển đổi thành 1 mảng ảnh như ví dụ dưới đây."
  },
  {
    "objectID": "p03-40-interpretable-ml.html#lime",
    "href": "p03-40-interpretable-ml.html#lime",
    "title": "31  Giải thích mô hình học máy",
    "section": "31.4 LIME",
    "text": "31.4 LIME\nThuật toán LIME diễn ra theo các bước sau:\n\nVới quan sát cần phân tích, tạo ra thêm các dữ liệu giả lập bằng cách thay đổi dữ liệu đầu vào một lượng \\(\\Delta x\\)\nTính toán khoảng cách từ quan sát gốc đến các quan sát mới được tạo ra.\nÁp dụng mô hình đã xây dựng, dự báo với biến mới được tạo \\(x'\\)\nChọn ra m biến có mức độ quan trọng lớn nhất nhất đến mô hình\nXây dựng mô hình đơn giản (linear/logistics) với dữ liệu mới tạo với m biến đầu vào được chọn với trọng số bằng khoảng cách ở bước 2.\nSử dụng hệ số của mô hình mới tạo để giải thích hành vi của mô hình tại điểm cần phân tích\n\n\n31.4.1 Sử dụng Lime với Regression\n\nlibrary(tidyverse)\nlibrary(mlbench)\nlibrary(caret)\ndata(\"BostonHousing\")\nBostonHousing %>% head\n\n# Các biến mô hình\nfeatures <- setdiff(names(BostonHousing), \"medv\")\n\nindex <- sample(1:nrow(BostonHousing), 4)\n\n# Train\nx_train <- BostonHousing[-index, features]\ny_train <- BostonHousing[-index, \"medv\"]\ntrain <- x_train %>% \n  mutate(medv = y_train)\n\n# Test\nx_test <- BostonHousing[index, features]\ny_test <- BostonHousing[index, \"medv\"]\n\n# Build model\n\nmodel_rf <- train(x = x_train,\n                  y = y_train,\n                  method = \"rf\")\nlibrary(randomForest)\nmodel_rf <- train(x = x_train,\n                   y = y_train,\n                   method = \"rf\",\n                   tuneLength = 3,\n                   trControl = trainControl(method = \"cv\"))\n\nmodel_rf\n\n# Prediction\ndf_test <- data.frame(x_test,\n                      medv = y_test,\n                      predict = predict(model_rf, x_test))\ndf_test\n\n# Step 1: Tạo 'explainer' object\nlibrary(lime)\nexplainer <- lime(x = x_train, model = model_rf)\nexplainer\n\n# Step 2: Biến đổi `explainer` giải thích cho tập test\nexplaination <- explain(x = x_test,\n                       explainer = explainer,\n                       n_permutations = 100, # Có thể tăng lên 1000\n                       feature_select = \"auto\",\n                       n_features = 5)\n\nexplaination %>% head(5) %>% \n  mutate(new = feature_value * feature_weight) %>% \n  pull(new) %>% sum + 22.48\n\n# Step 3: Plot\n\nplot_features(explaination, ncol = 2) +\n    labs(title = \"Model explaination\")\n\n\n\n31.4.2 Sử dụng Lime với Regression H2O\n\nlibrary(h2o)\nh2o.init(max_mem_size = \"3g\")\nh_train <- BostonHousing[-index,] %>% as.h2o\nh_test <- BostonHousing[index,] %>% as.h2o\n\n# Build model\nmodel_automl <- h2o.automl(x = features,\n                           y = \"medv\",\n                           training_frame = h_train,\n                           nfolds = 5,\n                           max_runtime_secs = 60,\n                           max_models = 12,\n                           stopping_metric = \"RMSE\",\n                           seed = 1234)\n\n# Lựa chọn mô hình tốt nhất\nmodel_automl@leader\n\n# Step 1: Tạo explainer object\n\nexplainer <- lime(x = as.data.frame(h_train[, features]), \n                  model = model_automl@leader)\nexplainer\n\n# Step 2: Biến đổi `explainer` giải thích cho tập test\nexplaination <- explain(x = as.data.frame(h_test[, features]),\n                       explainer = explainer,\n                       n_permutations = 100,\n                       feature_select = \"lasso_path\",\n                       n_features = 5,\n                       labels = \"Yes\")\n\nexplaination %>% head\n\n# Step 3: Plot\n\nplot_features(explaination, ncol = 2) +\n  labs(title = \"Model explaination with H2O\")\n\n\n\n31.4.3 Sử dụng Lime với Classification H2O\n\n# create data sets\ndf <- modeldata::attrition %>% \n  mutate_if(is.ordered, factor, ordered = FALSE) %>%\n  mutate(Attrition = factor(Attrition, levels = c(\"Yes\", \"No\")))\n\nindex <- 1:4\ntrain_obs <- df[-index, ]\nlocal_obs <- df[index, ]\n\n# create h2o objects for modeling\ny <- \"Attrition\"\nx <- setdiff(names(train_obs), y)\ntrain_obs.h2o <- as.h2o(train_obs)\nlocal_obs.h2o <- as.h2o(local_obs)\n\n\n# Build mô hình\nh2o_model  <- h2o.gbm(x, y, \n                      training_frame = train_obs.h2o,\n                      nfolds = 5,\n                      seed = 1234)\n\n\nexplainer <- lime(x = as.data.frame(train_obs.h2o[, x]), \n                  model = h2o_model)\nexplainer\n\n# Step 2: Biến đổi `explainer` giải thích cho tập test\nexplaination <- explain(x = as.data.frame(local_obs.h2o[, x]),\n                       explainer = explainer,\n                       n_permutations = 1000,\n                       feature_select = \"lasso_path\",\n                       n_features = 5,\n                       labels = \"Yes\")\n\nplot_features(explaination)\nh2o.shutdown(prompt = F)"
  },
  {
    "objectID": "p03-40-interpretable-ml.html#tài-liệu-tham-khảo",
    "href": "p03-40-interpretable-ml.html#tài-liệu-tham-khảo",
    "title": "31  Giải thích mô hình học máy",
    "section": "31.5 Tài liệu tham khảo",
    "text": "31.5 Tài liệu tham khảo\n\nhttps://www.slideshare.net/JofaiChow/automatic-and-interpretable-machine-learning-with-h2o-and-lime\nhttps://www.slideshare.net/0xdata/hr-analytics-using-machine-learning-to-predict-employee-turnover\nhttps://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html\n[https://towardsdatascience.com/idea-behind-lime-and-shap-b603d35d34e"
  },
  {
    "objectID": "p03-50-chat-luong-mo-hinh.html#auc",
    "href": "p03-50-chat-luong-mo-hinh.html#auc",
    "title": "32  Chất lượng mô hình",
    "section": "32.1 AUC",
    "text": "32.1 AUC\nTrường hợp 1: Xác suất TRUE phân phối đều trong khoảng từ 0 đến 1\n\nprob <- seq(0.1,1, by = 0.1)\nclass <- c(0,1,0,0,0,0,1,1,0,1) %>% as.factor\ndf1 <- data.frame(prob = prob,\n                 class = class %>% as.factor)\ndf1\n\n   prob class\n1   0.1     0\n2   0.2     1\n3   0.3     0\n4   0.4     0\n5   0.5     0\n6   0.6     0\n7   0.7     1\n8   0.8     1\n9   0.9     0\n10  1.0     1\n\nroc_auc(df1, class, prob)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.292\n\n\nTrường hợp 2: Xác xuất TRUE phân phối trong khoảng từ 0.55 đến 1\n\nprob <- seq(0.55,1, by = 0.05)\nclass <- c(0,1,0,0,0,0,1,1,0,1) %>% as.factor\ndf2 <- data.frame(prob = prob,\n                 class = class %>% as.factor)\ndf2\n\n   prob class\n1  0.55     0\n2  0.60     1\n3  0.65     0\n4  0.70     0\n5  0.75     0\n6  0.80     0\n7  0.85     1\n8  0.90     1\n9  0.95     0\n10 1.00     1\n\nroc_auc(df2, class, prob)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.292\n\n\nTrong cả hai trường hợp trên, ta thấy AUC của cả 2 mô hình đều không đổi và hoàn toàn không phụ thuộc vào điểm xác suất dự báo. Như vậy, yếu tố ảnh hưởng đến AUC sẽ là thứ tự (ranking) của các điểm dự báo."
  },
  {
    "objectID": "p03-50-chat-luong-mo-hinh.html#confusion-matrix",
    "href": "p03-50-chat-luong-mo-hinh.html#confusion-matrix",
    "title": "32  Chất lượng mô hình",
    "section": "32.2 Confusion Matrix",
    "text": "32.2 Confusion Matrix\nCó thể dễ dàng nhận thấy rằng, confusion matrix cũng không bị ảnh hưởng nhiều bởi điểm score mà phụ thuộc hoàn toàn vào giá trị được ranking khi lựa chọn điểm cutoff.\nTrong ví dụ dưới đây, nếu lựa chọn điểm cut-off là top 30% điểm cao nhất, sẽ thấy là confusion matrix của cả 2 mô hình với prob1 và prob2 là giống hệt nhau.\n\ndf <- df1 %>% bind_cols(df2) \nnames(df) <- c(\"prob1\", \"class\", \"prob2\", \"class\")\ndf\n\n   prob1 class prob2 class\n1    0.1     0  0.55     0\n2    0.2     1  0.60     1\n3    0.3     0  0.65     0\n4    0.4     0  0.70     0\n5    0.5     0  0.75     0\n6    0.6     0  0.80     0\n7    0.7     1  0.85     1\n8    0.8     1  0.90     1\n9    0.9     0  0.95     0\n10   1.0     1  1.00     1"
  },
  {
    "objectID": "p03-50-chat-luong-mo-hinh.html#lựa-chọn-cut-off",
    "href": "p03-50-chat-luong-mo-hinh.html#lựa-chọn-cut-off",
    "title": "32  Chất lượng mô hình",
    "section": "32.3 Lựa chọn cut-off",
    "text": "32.3 Lựa chọn cut-off\nNhư vậy, khi nắm được rằng giá trị tuyệt đối của điểm score không ảnh hưởng nhiều đến chất lượng mô hình, ta có thể tập trung vào việc lựa chọn điểm cut-off để dự báo positive class.\nViệc lựa chọn cut-off tỏng thực tế hoàn toàn phụ thuộc vào khẩu vị rủi ro và năng lực triển khai của đơn vị. Khi giải thích cho đơn vị kinh doanh, ứng dụng quan trọng nhất là decile table.\n\n\n\nGiải thích:\n\nToàn bộ dữ liệu có 200 quan sát (tổng cột total), trong đó có 93 trường hợp positive, ứng với tỷ lệ positive toàn tập là 43.5%\nTrong top 10% đầu tiên, có 16/20 trường hợp dự báo đúng, ứng với tỷ lệ posittive lũy kế là 0.8. Việc sử dụng top 10% dự báo sẽ có tỷ lệ chuyển đổi 80% (cột `response rate``), cao hơn 1.72 lần so với sử dụng ngẫu nhiên khách hàng trong marketing (\\(liift = \\frac{0.8}{0.465} = 1.72\\))\nTrong top 10% tiếp theo (decile 2), có 17/20 trường hợp dự báo đúng, ứng với response_rate của nhóm là 0.85. Việc sử dụng top 20% khách hàng sẽ có tỷ lệ chuyển đổi lũy kế là 0.825 và lift là 1.77 (\\(1.77=\\frac{0.825}{0.465}\\))\n\nTương ứng như vậy, chỉ số lift sẽ giảm dần xuống đến 1.\nVới ví dụ trên, nếu như phía kinh doanh chỉ có khả năng khai thác 20K (10%) khách hàng điểm cao nhất, thì ta nên đề xuất điểm cut-off là top 10%. Tương tự, nếu triển khai digital markeking với số lượng đông hơn, có thể đẩy 30-50% dữ liệu toàn tập (ứng với 30%-50% khách hàng tiềm năng nhất).\n\nLưu ý: Mô hình trong ví dụ trên chưa phải là mô hình tốt nhất có thể có do lift không giảm dần mà lại tăng ở decile 2 và 3. Khi xây dựng mô hình tốt, lift sẽ giảm dần.\n\n\nLưu ý: Việc ứng dụng dự báo cho bán chéo, giữ chân khách hàng và chấm điểm tín dụng sẽ có các khác biệt cơ bản sau:\n\nVới bán chéo, rủi ro sẽ thấp hơn nên có thể lựa chọn điểm cut-off rộng hơn, quan trọng là dựa vào khả năng khai thác khách hàng từ đơn vị kinh doanh. Kết quả dự báo sẽ được nhìn thấy rõ ràng ngay sau khi bán.\nVới giữ chân khách hàng, kết quả sẽ không nhìn thấy rõ rệt ngay sau khi ứng dụng mô hình do mục tiêu kinh doanh và mô hình khác nhau. Với mô hình, mục tiêu là dự báo khách hàng churn. Tuy nhiên, nhóm khách hàng đó lại được sử dụng để chăm sóc và giữ chân. Do đó, nếu giữ chân tốt, nhóm này sẽ tiếp tục sử dụng sản phẩm, dịch vụ và sẽ đưa ra nhận định nghịch lý - “Kết quả dự báo sai”. Do đó, để đo lường kết quả chính xác, phải đo lường tỷ lệ thay đổi của khách hàng sau 3-6 tháng triển khai ứng dụng mô hình\nVới mô hình chấm điểm tín dụng, lựa chọn điểm cut-off cần phải được sử dụng khéo léo do chi phí ròng nếu dự báo sai cao hơn 2 ứng dụng trên rất nhiều (do không thu hồi được nợ).\n\nĐể đánh giá được chính xác hiệu quả do việc sử dụng mô hình dự báo đem lại, ta phải hết sức lưu ý và nắm rõ quy trình áp dụng kết quả dự báo vào các hoạt động ứng dụng trong kinh doanh."
  },
  {
    "objectID": "p03-69-caret.html#giới-thiệu",
    "href": "p03-69-caret.html#giới-thiệu",
    "title": "33  Xây dựng mô hình dự báo với caret",
    "section": "33.1 Giới thiệu",
    "text": "33.1 Giới thiệu\nCaret (viết tắt của Classification & Regression) là một package được sử dụng rộng rãi trong việc xây dựng mô hình dự báo, đặc biệt hữu dụng khi học và tìm hiểu các concept của machine learning. Tuy nhiên, khi xây dựng mô hình thực tế sẽ gặp vấn đề về tốc độ.Chương này chỉ thuần túy mang tính chất giới thiệu về caret trọng xây dựng mô hình supervised learning.\nTrong caret có 1 số hàm cơ bản sau:\n\ncreateDataPartition: Tạo train & test\ntrain: xây dựng mô hình, áp dụng cấu trúc gần như đồng nhất giữa các mô hình (đây là điểm mạnh rất lớn của caret)\n\nMô hình đơn giản:\n\npkg <- c(\"caret\", \"dplyr\", \"mlbench\")\nlapply(pkg, library, character.only = T)\n\n[[1]]\n [1] \"caret\"     \"lattice\"   \"ggplot2\"   \"stats\"     \"graphics\"  \"grDevices\"\n [7] \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[2]]\n [1] \"dplyr\"     \"caret\"     \"lattice\"   \"ggplot2\"   \"stats\"     \"graphics\" \n [7] \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[3]]\n [1] \"mlbench\"   \"dplyr\"     \"caret\"     \"lattice\"   \"ggplot2\"   \"stats\"    \n [7] \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n#Tạo tập train & test\ninTrain <- createDataPartition(y = iris$Species,\n                               p = 0.75,\n                               list = F)\ntrain <- iris[inTrain,]\ntest <- iris[-inTrain,]\n#Mô hình\nrf.model <- train(Species ~ ., \n                  data = train, \n                  method = \"rf\",\n                  preProc = c(\"center\", \"scale\")) #Preprocess: Center & scale\nrf.model %>% summary\n\n                Length Class      Mode     \ncall               4   -none-     call     \ntype               1   -none-     character\npredicted        114   factor     numeric  \nerr.rate        2000   -none-     numeric  \nconfusion         12   -none-     numeric  \nvotes            342   matrix     numeric  \noob.times        114   -none-     numeric  \nclasses            3   -none-     character\nimportance         4   -none-     numeric  \nimportanceSD       0   -none-     NULL     \nlocalImportance    0   -none-     NULL     \nproximity          0   -none-     NULL     \nntree              1   -none-     numeric  \nmtry               1   -none-     numeric  \nforest            14   -none-     list     \ny                114   factor     numeric  \ntest               0   -none-     NULL     \ninbag              0   -none-     NULL     \nxNames             4   -none-     character\nproblemType        1   -none-     character\ntuneValue          1   data.frame list     \nobsLevels          3   -none-     character\nparam              0   -none-     list     \n\nrf.model\n\nRandom Forest \n\n114 samples\n  4 predictor\n  3 classes: 'setosa', 'versicolor', 'virginica' \n\nPre-processing: centered (4), scaled (4) \nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 114, 114, 114, 114, 114, 114, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  2     0.9312877  0.8958854\n  3     0.9322401  0.8973176\n  4     0.9312501  0.8958986\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 3.\n\n\nMô hình phức tạp:\n\n#Train & test\ninTrain <- createDataPartition(y = iris$Species,\n                               p = 0.75,\n                               list = F)\ntrain <- iris[inTrain,]\ntest <- iris[-inTrain,]\n\n#Xây dựng mô hình\n\nctrl <- trainControl(method = \"repeatedcv\", #repeated cross validate\n                     repeats = 3,\n                     classProbs = TRUE)\n\nrf.model <- train(Species ~ ., \n                  data = train, \n                  method = \"rf\",\n                  trainControl = ctrl,\n                  metric = \"Accuracy\",\n                  preProc = c(\"center\", \"scale\")) #Preprocess: Center & scale\n\nfit.lda <- train(Species ~ ., \n                  data = train, \n                  method = \"lda\",\n                  trainControl = ctrl,\n                  metric = \"Accuracy\",\n                  preProc = c(\"center\", \"scale\")) #Preprocess: Center & scale\n\nfit.multi <- train(Species ~ ., \n                  data = train, \n                  method = \"multinom\",\n                  trainControl = ctrl,\n                  metric = \"Accuracy\",\n                  preProc = c(\"center\", \"scale\")) #Preprocess: Center & scale\n\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 12.870492\niter  20 value 2.338626\niter  30 value 1.323630\niter  40 value 1.097034\niter  50 value 1.061098\niter  60 value 0.968698\niter  70 value 0.780558\niter  80 value 0.743231\niter  90 value 0.714811\niter 100 value 0.648532\nfinal  value 0.648532 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 23.340947\niter  20 value 22.013883\nfinal  value 22.013882 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 12.887733\niter  20 value 3.066945\niter  30 value 2.738044\niter  40 value 2.699590\niter  50 value 2.669614\niter  60 value 2.605682\niter  70 value 2.536741\niter  80 value 2.521729\niter  90 value 2.514912\niter 100 value 2.507340\nfinal  value 2.507340 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 3.604727\niter  20 value 0.006042\niter  30 value 0.001386\niter  40 value 0.001241\niter  50 value 0.001081\niter  60 value 0.001041\niter  70 value 0.000940\niter  80 value 0.000916\niter  90 value 0.000783\niter 100 value 0.000758\nfinal  value 0.000758 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 15.637533\niter  20 value 15.422347\nfinal  value 15.422347 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 3.629616\niter  20 value 0.221486\niter  30 value 0.218274\niter  40 value 0.213588\niter  50 value 0.210141\niter  60 value 0.207407\niter  70 value 0.203576\niter  80 value 0.202970\niter  90 value 0.202544\niter 100 value 0.202266\nfinal  value 0.202266 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 17.258140\niter  20 value 0.178402\niter  30 value 0.048397\niter  40 value 0.030445\niter  50 value 0.029283\niter  60 value 0.028680\niter  70 value 0.027711\niter  80 value 0.023737\niter  90 value 0.022976\niter 100 value 0.021323\nfinal  value 0.021323 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 25.962774\niter  20 value 23.204647\nfinal  value 23.204646 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 17.271208\niter  20 value 1.395185\niter  30 value 1.355753\niter  40 value 1.328902\niter  50 value 1.250168\niter  60 value 1.232601\niter  70 value 1.219832\niter  80 value 1.204163\niter  90 value 1.189508\niter 100 value 1.181864\nfinal  value 1.181864 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 7.881791\niter  20 value 0.030782\niter  30 value 0.004904\niter  40 value 0.003517\niter  50 value 0.002985\niter  60 value 0.002582\niter  70 value 0.002065\niter  80 value 0.001973\niter  90 value 0.001581\niter 100 value 0.001525\nfinal  value 0.001525 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 17.158088\niter  20 value 16.834661\nfinal  value 16.834661 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 7.899644\niter  20 value 0.304449\niter  30 value 0.280782\niter  40 value 0.271076\niter  50 value 0.239067\niter  60 value 0.236487\niter  70 value 0.234608\niter  80 value 0.233436\niter  90 value 0.232392\niter 100 value 0.232102\nfinal  value 0.232102 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 10.350004\niter  20 value 0.550018\niter  30 value 0.068164\niter  40 value 0.057318\niter  50 value 0.050370\niter  60 value 0.047175\niter  70 value 0.045360\niter  80 value 0.044765\niter  90 value 0.043761\niter 100 value 0.037133\nfinal  value 0.037133 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 21.863544\niter  20 value 21.024867\nfinal  value 21.024866 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 10.370788\niter  20 value 3.017469\niter  30 value 2.858690\niter  40 value 2.684973\niter  50 value 2.660384\niter  60 value 2.549081\niter  70 value 2.298492\niter  80 value 2.271649\niter  90 value 2.264550\niter 100 value 2.257432\nfinal  value 2.257432 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 12.451649\niter  20 value 0.426214\niter  30 value 0.060841\niter  40 value 0.049073\niter  50 value 0.043534\niter  60 value 0.013751\niter  70 value 0.011760\niter  80 value 0.005324\niter  90 value 0.004450\niter 100 value 0.004082\nfinal  value 0.004082 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 21.597401\niter  20 value 21.081301\niter  20 value 21.081301\niter  20 value 21.081301\nfinal  value 21.081301 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 12.467655\niter  20 value 1.767533\niter  30 value 1.520033\niter  40 value 1.353104\niter  50 value 1.322680\niter  60 value 1.264441\niter  70 value 1.163761\niter  80 value 1.141218\niter  90 value 1.135683\niter 100 value 1.125464\nfinal  value 1.125464 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 5.748461\niter  20 value 0.018653\niter  30 value 0.003774\niter  40 value 0.003573\niter  50 value 0.003287\niter  60 value 0.002938\niter  70 value 0.002707\niter  80 value 0.002475\niter  90 value 0.002392\niter 100 value 0.002299\nfinal  value 0.002299 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 16.905969\niter  20 value 16.264098\nfinal  value 16.264097 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 5.772538\niter  20 value 0.441191\niter  30 value 0.416525\niter  40 value 0.369219\niter  50 value 0.353481\niter  60 value 0.338778\niter  70 value 0.314855\niter  80 value 0.312827\niter  90 value 0.305259\niter 100 value 0.304370\nfinal  value 0.304370 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 12.083667\niter  20 value 0.709325\niter  30 value 0.193158\niter  40 value 0.049755\niter  50 value 0.046877\niter  60 value 0.046231\niter  70 value 0.042665\niter  80 value 0.040795\niter  90 value 0.038557\niter 100 value 0.037147\nfinal  value 0.037147 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 22.570796\niter  20 value 21.176056\nfinal  value 21.176053 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 12.101374\niter  20 value 2.187878\niter  30 value 1.906767\niter  40 value 1.799603\niter  50 value 1.663926\niter  60 value 1.638679\niter  70 value 1.632450\niter  80 value 1.607854\niter  90 value 1.580245\niter 100 value 1.573173\nfinal  value 1.573173 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 6.266221\niter  20 value 0.029629\niter  30 value 0.003076\niter  40 value 0.002432\niter  50 value 0.002333\niter  60 value 0.002255\niter  70 value 0.001819\niter  80 value 0.001748\niter  90 value 0.001595\niter 100 value 0.001527\nfinal  value 0.001527 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 18.906576\niter  20 value 18.619923\niter  20 value 18.619923\niter  20 value 18.619923\nfinal  value 18.619923 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 6.293547\niter  20 value 0.753955\niter  30 value 0.708876\niter  40 value 0.688209\niter  50 value 0.677400\niter  60 value 0.663973\niter  70 value 0.644015\niter  80 value 0.637446\niter  90 value 0.632904\niter 100 value 0.628558\nfinal  value 0.628558 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 6.815399\niter  20 value 0.065218\niter  30 value 0.003462\niter  40 value 0.001605\niter  50 value 0.001222\niter  60 value 0.000767\niter  70 value 0.000675\niter  80 value 0.000563\niter  90 value 0.000351\niter 100 value 0.000335\nfinal  value 0.000335 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 17.967925\niter  20 value 17.302749\nfinal  value 17.302748 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 6.835214\niter  20 value 0.465290\niter  30 value 0.396067\niter  40 value 0.368929\niter  50 value 0.342049\niter  60 value 0.327051\niter  70 value 0.317642\niter  80 value 0.313597\niter  90 value 0.308628\niter 100 value 0.305261\nfinal  value 0.305261 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 13.812640\niter  20 value 2.519106\niter  30 value 0.754907\niter  40 value 0.702345\niter  50 value 0.670114\niter  60 value 0.590841\niter  70 value 0.556782\niter  80 value 0.474681\niter  90 value 0.447064\niter 100 value 0.402154\nfinal  value 0.402154 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 22.928049\niter  20 value 21.254957\nfinal  value 21.254956 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 13.827925\niter  20 value 3.010676\niter  30 value 2.494686\niter  40 value 2.396554\niter  50 value 2.374500\niter  60 value 2.216745\niter  70 value 2.153221\niter  80 value 2.131694\niter  90 value 2.128736\niter 100 value 2.116713\nfinal  value 2.116713 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 9.934062\niter  20 value 0.716452\niter  30 value 0.117555\niter  40 value 0.077896\niter  50 value 0.076278\niter  60 value 0.065879\niter  70 value 0.063716\niter  80 value 0.060950\niter  90 value 0.054006\niter 100 value 0.047359\nfinal  value 0.047359 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 20.871466\niter  20 value 19.794757\nfinal  value 19.794756 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 9.952449\niter  20 value 2.554688\niter  30 value 2.329405\niter  40 value 2.220181\niter  50 value 2.181940\niter  60 value 2.123576\niter  70 value 2.090293\niter  80 value 2.010931\niter  90 value 1.965959\niter 100 value 1.950273\nfinal  value 1.950273 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 14.866026\niter  20 value 2.706321\niter  30 value 0.098175\niter  40 value 0.039463\niter  50 value 0.037619\niter  60 value 0.028718\niter  70 value 0.028496\niter  80 value 0.028301\niter  90 value 0.026124\niter 100 value 0.023774\nfinal  value 0.023774 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 23.716686\niter  20 value 22.635991\nfinal  value 22.635990 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 14.881689\niter  20 value 4.605412\niter  30 value 3.722310\niter  40 value 3.523272\niter  50 value 3.472794\niter  60 value 3.366111\niter  70 value 3.135874\niter  80 value 3.078178\niter  90 value 3.033175\niter 100 value 2.997116\nfinal  value 2.997116 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 0.888296\niter  20 value 0.254381\niter  30 value 0.073382\niter  40 value 0.032616\niter  50 value 0.026843\niter  60 value 0.013506\niter  70 value 0.005884\niter  80 value 0.002881\niter  90 value 0.000462\niter 100 value 0.000361\nfinal  value 0.000361 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 17.169689\niter  20 value 15.981376\nfinal  value 15.981373 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 0.941437\niter  20 value 0.383941\niter  30 value 0.250701\niter  40 value 0.226631\niter  50 value 0.224138\niter  60 value 0.220559\niter  70 value 0.219060\niter  80 value 0.218831\niter  90 value 0.218687\niter 100 value 0.218583\nfinal  value 0.218583 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 9.856005\niter  20 value 0.337165\niter  30 value 0.032644\niter  40 value 0.002407\niter  50 value 0.001851\niter  60 value 0.001775\niter  70 value 0.001720\niter  80 value 0.001595\niter  90 value 0.000890\niter 100 value 0.000887\nfinal  value 0.000887 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 20.498754\niter  20 value 19.868805\nfinal  value 19.868804 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 9.877069\niter  20 value 1.996642\niter  30 value 1.459613\niter  40 value 1.380211\niter  50 value 1.345939\niter  60 value 1.296604\niter  70 value 1.242781\niter  80 value 1.224095\niter  90 value 1.191251\niter 100 value 1.163791\nfinal  value 1.163791 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 8.675682\niter  20 value 0.087433\niter  30 value 0.009068\niter  40 value 0.004880\niter  50 value 0.002997\niter  60 value 0.002849\niter  70 value 0.001861\niter  80 value 0.001798\niter  90 value 0.001719\niter 100 value 0.001709\nfinal  value 0.001709 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 21.222212\niter  20 value 20.720853\nfinal  value 20.720852 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 8.699511\niter  20 value 0.827466\niter  30 value 0.743569\niter  40 value 0.718053\niter  50 value 0.703254\niter  60 value 0.692095\niter  70 value 0.670948\niter  80 value 0.662357\niter  90 value 0.656328\niter 100 value 0.651924\nfinal  value 0.651924 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 7.149480\niter  20 value 0.162756\niter  30 value 0.038567\niter  40 value 0.022878\niter  50 value 0.018343\niter  60 value 0.017202\niter  70 value 0.016995\niter  80 value 0.011280\niter  90 value 0.010530\niter 100 value 0.010420\nfinal  value 0.010420 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 19.475498\niter  20 value 18.948965\nfinal  value 18.948965 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 7.175724\niter  20 value 1.556138\niter  30 value 1.452426\niter  40 value 1.355209\niter  50 value 1.326871\niter  60 value 1.294233\niter  70 value 1.282574\niter  80 value 1.259766\niter  90 value 1.253975\niter 100 value 1.243254\nfinal  value 1.243254 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 16.096686\niter  20 value 3.495912\niter  30 value 0.513666\niter  40 value 0.321913\niter  50 value 0.210101\niter  60 value 0.194356\niter  70 value 0.181570\niter  80 value 0.178865\niter  90 value 0.175393\niter 100 value 0.169654\nfinal  value 0.169654 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 25.430206\niter  20 value 23.447792\nfinal  value 23.447789 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 16.111200\niter  20 value 5.234235\niter  30 value 3.858949\niter  40 value 3.751878\niter  50 value 3.616966\niter  60 value 3.567849\niter  70 value 3.486922\niter  80 value 3.432852\niter  90 value 3.379128\niter 100 value 3.350787\nfinal  value 3.350787 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 4.969726\niter  20 value 0.760433\niter  30 value 0.081587\niter  40 value 0.026428\niter  50 value 0.025371\niter  60 value 0.022701\niter  70 value 0.021564\niter  80 value 0.020260\niter  90 value 0.019964\niter 100 value 0.018905\nfinal  value 0.018905 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 22.525772\niter  20 value 20.596111\nfinal  value 20.596110 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 5.008877\niter  20 value 1.431964\niter  30 value 0.981565\niter  40 value 0.941841\niter  50 value 0.931639\niter  60 value 0.912228\niter  70 value 0.901136\niter  80 value 0.899462\niter  90 value 0.897425\niter 100 value 0.896602\nfinal  value 0.896602 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 7.031149\niter  20 value 1.153882\niter  30 value 0.153734\niter  40 value 0.005722\niter  50 value 0.001211\niter  60 value 0.001013\niter  70 value 0.001005\niter  80 value 0.000996\niter  90 value 0.000715\niter 100 value 0.000689\nfinal  value 0.000689 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 21.486286\nfinal  value 21.360259 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 7.082064\niter  20 value 3.510429\niter  30 value 2.740201\niter  40 value 2.514457\niter  50 value 2.487460\niter  60 value 2.422255\niter  70 value 2.182551\niter  80 value 2.152975\niter  90 value 2.150107\niter 100 value 2.143693\nfinal  value 2.143693 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 7.674774\niter  20 value 0.057651\niter  30 value 0.010449\niter  40 value 0.008008\niter  50 value 0.004361\niter  60 value 0.003575\niter  70 value 0.003165\niter  80 value 0.002962\niter  90 value 0.002580\niter 100 value 0.002544\nfinal  value 0.002544 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 18.293263\niter  20 value 17.743089\nfinal  value 17.743089 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 7.692709\niter  20 value 0.588640\niter  30 value 0.560578\niter  40 value 0.515518\niter  50 value 0.512586\niter  60 value 0.487534\niter  70 value 0.480767\niter  80 value 0.479056\niter  90 value 0.477861\niter 100 value 0.477305\nfinal  value 0.477305 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 11.692754\niter  20 value 0.068895\niter  30 value 0.005755\niter  40 value 0.004046\niter  50 value 0.003357\niter  60 value 0.002899\niter  70 value 0.002470\niter  80 value 0.002145\niter  90 value 0.001605\niter 100 value 0.001365\nfinal  value 0.001365 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 22.466004\niter  20 value 21.194740\nfinal  value 21.194739 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 11.710313\niter  20 value 0.909390\niter  30 value 0.812415\niter  40 value 0.769239\niter  50 value 0.745041\niter  60 value 0.733689\niter  70 value 0.718137\niter  80 value 0.704683\niter  90 value 0.700641\niter 100 value 0.694349\nfinal  value 0.694349 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 5.973036\niter  20 value 0.175432\niter  30 value 0.020917\niter  40 value 0.007351\niter  50 value 0.005768\niter  60 value 0.004694\niter  70 value 0.003313\niter  80 value 0.003070\niter  90 value 0.001591\niter 100 value 0.001273\nfinal  value 0.001273 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 17.614824\niter  20 value 17.411148\nfinal  value 17.411147 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 5.999440\niter  20 value 1.458002\niter  30 value 1.081677\niter  40 value 0.937887\niter  50 value 0.922704\niter  60 value 0.889228\niter  70 value 0.856565\niter  80 value 0.838700\niter  90 value 0.798971\niter 100 value 0.774415\nfinal  value 0.774415 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 6.310576\niter  20 value 0.037980\niter  30 value 0.004288\niter  40 value 0.003908\niter  50 value 0.002268\niter  60 value 0.002104\niter  70 value 0.001965\niter  80 value 0.001924\niter  90 value 0.001220\niter 100 value 0.001212\nfinal  value 0.001212 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 16.887744\niter  20 value 16.505403\nfinal  value 16.505402 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 6.329286\niter  20 value 0.493295\niter  30 value 0.461486\niter  40 value 0.427921\niter  50 value 0.415043\niter  60 value 0.404792\niter  70 value 0.401274\niter  80 value 0.396797\niter  90 value 0.391133\niter 100 value 0.388409\nfinal  value 0.388409 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 11.063074\niter  20 value 0.847954\niter  30 value 0.176815\niter  40 value 0.094656\niter  50 value 0.087125\niter  60 value 0.054481\niter  70 value 0.045824\niter  80 value 0.033008\niter  90 value 0.032312\niter 100 value 0.012473\nfinal  value 0.012473 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 20.082426\niter  20 value 19.240056\nfinal  value 19.240055 \nconverged\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 11.079507\niter  20 value 1.794007\niter  30 value 1.630510\niter  40 value 1.435388\niter  50 value 1.369630\niter  60 value 1.293850\niter  70 value 1.226270\niter  80 value 1.222939\niter  90 value 1.209966\niter 100 value 1.206558\nfinal  value 1.206558 \nstopped after 100 iterations\n# weights:  18 (10 variable)\ninitial  value 125.241801 \niter  10 value 20.926329\niter  20 value 20.390172\niter  20 value 20.390171\niter  20 value 20.390171\nfinal  value 20.390171 \nconverged\n\n#So sánh 2 mô hình\nboosting_results <- resamples(list(rf=rf.model, LDA=fit.lda, MULTI = fit.multi))\nsummary(boosting_results)\n\n\nCall:\nsummary.resamples(object = boosting_results)\n\nModels: rf, LDA, MULTI \nNumber of resamples: 25 \n\nAccuracy \n           Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nrf    0.9047619 0.9487179 0.9729730 0.9596034 0.9761905    1    0\nLDA   0.9361702 0.9729730 0.9761905 0.9771513 1.0000000    1    0\nMULTI 0.9130435 0.9512195 0.9574468 0.9657657 0.9787234    1    0\n\nKappa \n           Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nrf    0.8541667 0.9216867 0.9587514 0.9385796 0.9638865    1    0\nLDA   0.9038199 0.9572954 0.9640103 0.9649215 1.0000000    1    0\nMULTI 0.8672439 0.9258590 0.9360544 0.9478504 0.9672131    1    0\n\ndotplot(boosting_results)\n\n\n\n#Test RF\npredict_result <- predict(rf.model, test)\nconfusionMatrix(predict_result, test$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         12          0         0\n  versicolor      0         11         1\n  virginica       0          1        11\n\nOverall Statistics\n                                          \n               Accuracy : 0.9444          \n                 95% CI : (0.8134, 0.9932)\n    No Information Rate : 0.3333          \n    P-Value [Acc > NIR] : 1.728e-14       \n                                          \n                  Kappa : 0.9167          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9167           0.9167\nSpecificity                 1.0000            0.9583           0.9583\nPos Pred Value              1.0000            0.9167           0.9167\nNeg Pred Value              1.0000            0.9583           0.9583\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3056           0.3056\nDetection Prevalence        0.3333            0.3333           0.3333\nBalanced Accuracy           1.0000            0.9375           0.9375\n\n#Test LDA\npredict_result <- predict(fit.lda, test)\nconfusionMatrix(predict_result, test$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         12          0         0\n  versicolor      0         11         0\n  virginica       0          1        12\n\nOverall Statistics\n                                          \n               Accuracy : 0.9722          \n                 95% CI : (0.8547, 0.9993)\n    No Information Rate : 0.3333          \n    P-Value [Acc > NIR] : 4.864e-16       \n                                          \n                  Kappa : 0.9583          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9167           1.0000\nSpecificity                 1.0000            1.0000           0.9583\nPos Pred Value              1.0000            1.0000           0.9231\nNeg Pred Value              1.0000            0.9600           1.0000\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3056           0.3333\nDetection Prevalence        0.3333            0.3056           0.3611\nBalanced Accuracy           1.0000            0.9583           0.9792"
  },
  {
    "objectID": "p03-69-caret.html#tuning-parameter",
    "href": "p03-69-caret.html#tuning-parameter",
    "title": "33  Xây dựng mô hình dự báo với caret",
    "section": "33.2 Tuning parameter",
    "text": "33.2 Tuning parameter\nKhi tối ưu hóa các tham số, ta nên dùng cross validation hoặc boostrap (quy tắc 632)\n\nset.seed(998)\nlibrary(mlbench)\ndata(Sonar)\ninTraining <- createDataPartition(Sonar$Class, \n                               p = 0.75, \n                               list = F)\ntraining <- Sonar[inTraining,]\ntesting <- Sonar[-inTraining,]\n\nfitControl <- trainControl(method = \"repeatedcv\",\n                           number = 2, \n                           repeats = 2,\n                           classProbs = T,\n                           summaryFunction = twoClassSummary)\nxgb.model <- train(Class ~ ., \n                   data = training,\n                   method = \"xgbTree\",\n                   trControl = fitControl,\n                   verbose = F,\n                   metric = \"ROC\")\n\n[15:25:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:25:59] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:01] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:02] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:03] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:05] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:06] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:07] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:08] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:09] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:10] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:10] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:10] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:10] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:10] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:10] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:10] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:10] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:12] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:13] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:14] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:15] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:15] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:15] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:15] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:15] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:15] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:15] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:15] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[15:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n#Kết quả mô hình\nxgb.model \n\neXtreme Gradient Boosting \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (2 fold, repeated 2 times) \nSummary of sample sizes: 78, 79, 79, 78 \nResampling results across tuning parameters:\n\n  eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens     \n  0.3  1          0.6               0.50        50      0.8598509  0.8214286\n  0.3  1          0.6               0.50       100      0.8743654  0.8214286\n  0.3  1          0.6               0.50       150      0.8759474  0.8214286\n  0.3  1          0.6               0.75        50      0.8631131  0.8154762\n  0.3  1          0.6               0.75       100      0.8740482  0.8154762\n  0.3  1          0.6               0.75       150      0.8722472  0.7976190\n  0.3  1          0.6               1.00        50      0.8731633  0.8630952\n  0.3  1          0.6               1.00       100      0.8849162  0.8630952\n  0.3  1          0.6               1.00       150      0.8823913  0.8690476\n  0.3  1          0.8               0.50        50      0.8576523  0.8035714\n  0.3  1          0.8               0.50       100      0.8631355  0.8035714\n  0.3  1          0.8               0.50       150      0.8619825  0.8035714\n  0.3  1          0.8               0.75        50      0.8579428  0.8392857\n  0.3  1          0.8               0.75       100      0.8695347  0.8333333\n  0.3  1          0.8               0.75       150      0.8680645  0.8214286\n  0.3  1          0.8               1.00        50      0.8724886  0.8630952\n  0.3  1          0.8               1.00       100      0.8825433  0.8690476\n  0.3  1          0.8               1.00       150      0.8819936  0.8750000\n  0.3  2          0.6               0.50        50      0.8449566  0.7738095\n  0.3  2          0.6               0.50       100      0.8554671  0.7678571\n  0.3  2          0.6               0.50       150      0.8523970  0.7678571\n  0.3  2          0.6               0.75        50      0.8869182  0.8452381\n  0.3  2          0.6               0.75       100      0.8874053  0.8273810\n  0.3  2          0.6               0.75       150      0.8874008  0.8333333\n  0.3  2          0.6               1.00        50      0.8927856  0.8571429\n  0.3  2          0.6               1.00       100      0.8959763  0.8630952\n  0.3  2          0.6               1.00       150      0.8959763  0.8630952\n  0.3  2          0.8               0.50        50      0.8756882  0.7976190\n  0.3  2          0.8               0.50       100      0.8744101  0.8392857\n  0.3  2          0.8               0.50       150      0.8758714  0.8214286\n  0.3  2          0.8               0.75        50      0.8908194  0.8571429\n  0.3  2          0.8               0.75       100      0.8927499  0.8452381\n  0.3  2          0.8               0.75       150      0.8918025  0.8392857\n  0.3  2          0.8               1.00        50      0.8833387  0.8511905\n  0.3  2          0.8               1.00       100      0.8861093  0.8511905\n  0.3  2          0.8               1.00       150      0.8861093  0.8511905\n  0.3  3          0.6               0.50        50      0.8675819  0.8333333\n  0.3  3          0.6               0.50       100      0.8667194  0.8273810\n  0.3  3          0.6               0.50       150      0.8668758  0.8095238\n  0.3  3          0.6               0.75        50      0.8688018  0.8452381\n  0.3  3          0.6               0.75       100      0.8740258  0.8511905\n  0.3  3          0.6               0.75       150      0.8743342  0.8392857\n  0.3  3          0.6               1.00        50      0.8959585  0.8630952\n  0.3  3          0.6               1.00       100      0.8947608  0.8511905\n  0.3  3          0.6               1.00       150      0.8947608  0.8511905\n  0.3  3          0.8               0.50        50      0.8622283  0.8035714\n  0.3  3          0.8               0.50       100      0.8628897  0.7976190\n  0.3  3          0.8               0.50       150      0.8658167  0.7976190\n  0.3  3          0.8               0.75        50      0.8858189  0.8630952\n  0.3  3          0.8               0.75       100      0.8885850  0.8511905\n  0.3  3          0.8               0.75       150      0.8887682  0.8630952\n  0.3  3          0.8               1.00        50      0.8929733  0.8452381\n  0.3  3          0.8               1.00       100      0.8928259  0.8333333\n  0.3  3          0.8               1.00       150      0.8928259  0.8333333\n  0.4  1          0.6               0.50        50      0.8663798  0.8214286\n  0.4  1          0.6               0.50       100      0.8669786  0.8214286\n  0.4  1          0.6               0.50       150      0.8605480  0.8273810\n  0.4  1          0.6               0.75        50      0.8558603  0.8154762\n  0.4  1          0.6               0.75       100      0.8731410  0.8214286\n  0.4  1          0.6               0.75       150      0.8723187  0.8214286\n  0.4  1          0.6               1.00        50      0.8717065  0.8630952\n  0.4  1          0.6               1.00       100      0.8761753  0.8630952\n  0.4  1          0.6               1.00       150      0.8769707  0.8452381\n  0.4  1          0.8               0.50        50      0.8348527  0.8095238\n  0.4  1          0.8               0.50       100      0.8451353  0.8154762\n  0.4  1          0.8               0.50       150      0.8439734  0.8035714\n  0.4  1          0.8               0.75        50      0.8628763  0.8154762\n  0.4  1          0.8               0.75       100      0.8740616  0.8214286\n  0.4  1          0.8               0.75       150      0.8747408  0.8273810\n  0.4  1          0.8               1.00        50      0.8744101  0.8630952\n  0.4  1          0.8               1.00       100      0.8742582  0.8630952\n  0.4  1          0.8               1.00       150      0.8752190  0.8630952\n  0.4  2          0.6               0.50        50      0.8662815  0.8035714\n  0.4  2          0.6               0.50       100      0.8584745  0.7738095\n  0.4  2          0.6               0.50       150      0.8574691  0.7738095\n  0.4  2          0.6               0.75        50      0.8765283  0.8154762\n  0.4  2          0.6               0.75       100      0.8791604  0.8333333\n  0.4  2          0.6               0.75       150      0.8773684  0.8452381\n  0.4  2          0.6               1.00        50      0.9020449  0.8928571\n  0.4  2          0.6               1.00       100      0.9010528  0.8928571\n  0.4  2          0.6               1.00       150      0.9010528  0.8928571\n  0.4  2          0.8               0.50        50      0.8545242  0.8154762\n  0.4  2          0.8               0.50       100      0.8607581  0.8214286\n  0.4  2          0.8               0.50       150      0.8593906  0.8273810\n  0.4  2          0.8               0.75        50      0.8854077  0.8452381\n  0.4  2          0.8               0.75       100      0.8930716  0.8392857\n  0.4  2          0.8               0.75       150      0.8937062  0.8452381\n  0.4  2          0.8               1.00        50      0.8949172  0.8690476\n  0.4  2          0.8               1.00       100      0.8957261  0.8630952\n  0.4  2          0.8               1.00       150      0.8957261  0.8630952\n  0.4  3          0.6               0.50        50      0.8574244  0.8630952\n  0.4  3          0.6               0.50       100      0.8613211  0.8452381\n  0.4  3          0.6               0.50       150      0.8628003  0.8571429\n  0.4  3          0.6               0.75        50      0.8745710  0.8273810\n  0.4  3          0.6               0.75       100      0.8743967  0.8154762\n  0.4  3          0.6               0.75       150      0.8734181  0.8095238\n  0.4  3          0.6               1.00        50      0.8747632  0.8452381\n  0.4  3          0.6               1.00       100      0.8750670  0.8511905\n  0.4  3          0.6               1.00       150      0.8750670  0.8511905\n  0.4  3          0.8               0.50        50      0.8458727  0.7976190\n  0.4  3          0.8               0.50       100      0.8509492  0.8095238\n  0.4  3          0.8               0.50       150      0.8506051  0.7916667\n  0.4  3          0.8               0.75        50      0.8631444  0.8392857\n  0.4  3          0.8               0.75       100      0.8657273  0.8392857\n  0.4  3          0.8               0.75       150      0.8665138  0.8392857\n  0.4  3          0.8               1.00        50      0.8790353  0.8452381\n  0.4  3          0.8               1.00       100      0.8793436  0.8392857\n  0.4  3          0.8               1.00       150      0.8793436  0.8392857\n  Spec     \n  0.7047673\n  0.7325450\n  0.7460586\n  0.7524399\n  0.7595721\n  0.7528153\n  0.7188438\n  0.7530030\n  0.7462462\n  0.7188438\n  0.7533784\n  0.7464339\n  0.7118994\n  0.7464339\n  0.7531907\n  0.7118994\n  0.7393018\n  0.7595721\n  0.7595721\n  0.7665165\n  0.7871622\n  0.7805931\n  0.7738363\n  0.7805931\n  0.7599474\n  0.7805931\n  0.7805931\n  0.7670796\n  0.7597598\n  0.7802177\n  0.7389264\n  0.7460586\n  0.7530030\n  0.7530030\n  0.7533784\n  0.7533784\n  0.7257883\n  0.7190315\n  0.7327327\n  0.7466216\n  0.7533784\n  0.7533784\n  0.7325450\n  0.7049550\n  0.7049550\n  0.7875375\n  0.7736486\n  0.7736486\n  0.7805931\n  0.7736486\n  0.7736486\n  0.7804054\n  0.7734610\n  0.7734610\n  0.7393018\n  0.7526276\n  0.7528153\n  0.7261637\n  0.7256006\n  0.7188438\n  0.7120871\n  0.7327327\n  0.7327327\n  0.6779279\n  0.7057057\n  0.7195946\n  0.7396772\n  0.7464339\n  0.7601351\n  0.7464339\n  0.7464339\n  0.7599474\n  0.7462462\n  0.7391141\n  0.7323574\n  0.7939189\n  0.7802177\n  0.7734610\n  0.7595721\n  0.7800300\n  0.7800300\n  0.7329204\n  0.7261637\n  0.7466216\n  0.7661411\n  0.7661411\n  0.7728979\n  0.7665165\n  0.7595721\n  0.7595721\n  0.7319820\n  0.7389264\n  0.7389264\n  0.7460586\n  0.7460586\n  0.7528153\n  0.7601351\n  0.7533784\n  0.7533784\n  0.7535661\n  0.7398649\n  0.7464339\n  0.7257883\n  0.7256006\n  0.7257883\n  0.7591967\n  0.7591967\n  0.7591967\n\nTuning parameter 'gamma' was held constant at a value of 0\nTuning\n parameter 'min_child_weight' was held constant at a value of 1\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 50, max_depth = 2, eta\n = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample\n = 1.\n\n#Confusion Matrix tập testing\n\nconfusionMatrix(predict(xgb.model, testing), testing$Class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  M  R\n         M 21  6\n         R  6 18\n                                          \n               Accuracy : 0.7647          \n                 95% CI : (0.6251, 0.8721)\n    No Information Rate : 0.5294          \n    P-Value [Acc > NIR] : 0.0004667       \n                                          \n                  Kappa : 0.5278          \n                                          \n Mcnemar's Test P-Value : 1.0000000       \n                                          \n            Sensitivity : 0.7778          \n            Specificity : 0.7500          \n         Pos Pred Value : 0.7778          \n         Neg Pred Value : 0.7500          \n             Prevalence : 0.5294          \n         Detection Rate : 0.4118          \n   Detection Prevalence : 0.5294          \n      Balanced Accuracy : 0.7639          \n                                          \n       'Positive' Class : M               \n                                          \n\n#Lấy kết quả dạng xác suất\npredict(xgb.model, head(testing), type = \"prob\")\n\n             M          R\n1 0.4210385978 0.57896140\n2 0.9614750147 0.03852499\n3 0.0007100953 0.99928990\n4 0.5138569474 0.48614305\n5 0.7951146364 0.20488536\n6 0.9669867158 0.03301328"
  },
  {
    "objectID": "p03-69-caret.html#tài-liệu-tham-khảo",
    "href": "p03-69-caret.html#tài-liệu-tham-khảo",
    "title": "33  Xây dựng mô hình dự báo với caret",
    "section": "33.3 Tài liệu tham khảo",
    "text": "33.3 Tài liệu tham khảo\n\nCaret packages\nEnsemble methods"
  },
  {
    "objectID": "p03-70-tidymodels.html#giới-thiệu",
    "href": "p03-70-tidymodels.html#giới-thiệu",
    "title": "34  Tidymodels",
    "section": "34.1 Giới thiệu",
    "text": "34.1 Giới thiệu\nCác chương trước đã đề cập đến các kỹ thuật cũng như các phương pháp khác nhau trong việc xây dựng mô hình. Tuy nhiên, các phương pháp này trong R đang phải sử dụng trên nhiều packages khác nhau và khá rời rạc. Trong những năm gần đây, R đang phát triển hệ sinh thái của tidymodels cho phép xây dựng machine learning pipe line theo phong cách của tidyverse. Hệ sinh thái của tidymodels bao gồm các package sau.\n\nPre-processing: rsample - hỗ trợ lấy sampling, recipes - hỗ trợ chuẩn hóa feature engineering\nBuild model: parsnip - chuẩn hóa các engine xây mô hình trên R\nĐánh giá chất lượng mô hình: yarsdstick"
  },
  {
    "objectID": "p03-70-tidymodels.html#luồng-làm-việc-cơ-bản",
    "href": "p03-70-tidymodels.html#luồng-làm-việc-cơ-bản",
    "title": "34  Tidymodels",
    "section": "34.2 Luồng làm việc cơ bản",
    "text": "34.2 Luồng làm việc cơ bản\n\nlibrary(dplyr)\nlibrary(parsnip)\nlibrary(tidymodels)\n\n\nset.seed(4831)\nsplit <- initial_split(mtcars, props = 9/10)\ncar_train <- training(split)\ncar_test  <- testing(split)\n\n\nXây dựng data processing:\n\nTạo các bước feature engineering với recipe\nÁp dụng vào tập train với juice\nÁp dụng vào tập test với bake\n\n\n\ncar_rec <- recipe(mpg ~ ., data = car_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors()) %>%\n  prep(training = car_train, retain = TRUE)\n\ncar_rec\n\n# The processed versions are:\ntrain_data <- juice(car_rec)\ntest_data  <- bake(car_rec, car_test)\n\n\nXác định mô hình:\n\n\ncar_model <- linear_reg()\ncar_model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\ncar_model \n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nlm_car_model <- car_model %>%\n  set_engine(\"lm\")\n\nlm_car_model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nlm_fit <- lm_car_model %>%\n  fit(mpg ~ ., data = car_train)\n\n\nDự báo trên tập mới\n\n\npredict(lm_fit, test_data)\n\n# A tibble: 8 × 1\n  .pred\n  <dbl>\n1 20.5 \n2 11.8 \n3 10.1 \n4  8.24\n5 23.1 \n6 13.8 \n7 20.3 \n8 14.9 \n\n\nMô hình xgboost\n\nxgb_model <- boost_tree(mode = \"regression\",\n                        trees = 30) \n\nxgb_fit <- xgb_model %>% \n  set_engine(\"xgboost\") %>% \n  fit(mpg ~ ., data = car_train)"
  },
  {
    "objectID": "p03-70-tidymodels.html#tài-liệu-tham-khảo",
    "href": "p03-70-tidymodels.html#tài-liệu-tham-khảo",
    "title": "34  Tidymodels",
    "section": "34.3 Tài liệu tham khảo",
    "text": "34.3 Tài liệu tham khảo\n\n[https://tidymodels.github.io/recipes/]\n[https://tidymodels.github.io/rsamples/]\n[https://tidymodels.github.io/parsnip/]\n[https://tidymodels.github.io/yardstick/]\n[https://www.tidyverse.org/articles/2018/11/parsnip-0-0-1/]\n[https://tidymodels.github.io/parsnip/reference/boost_tree.html]"
  },
  {
    "objectID": "p04-01-unsupervised-learning.html",
    "href": "p04-01-unsupervised-learning.html",
    "title": "35  Unsupervised learning",
    "section": "",
    "text": "Khác với bài toán phân loại(classification), khi ta có 1 tập dữ liệu với giá trị phân loại của từng quan sát \\((x^{(i)}, y^{(i)})\\) và mục tiêu là xây dựng hàm dự báo giúp phân loại \\(y^{(i)}\\), bài toán unsupersived learning không có trước hàm mục tiêu. Trong bài toán này, từ tập dữ liệu cho trước, ta phải tìm ra cấu trúc dữ liệu tồn tại trong dữ liệu.\nỨng dụng của unsupervised learning như sau:\n\nPhân nhóm dữ liệu (clustering): Phân chia dữ liệu thành các nhóm sao cho độ tương đồng giữa các nhóm là giống nhau nhất. Ứng dụng của thuật toán này giúp cho việc phân nhóm khách hàng (customer segmentation), phân tích cấu trúc thị trường (market structure). Thuật toán ứng dụng trong nhóm này bao gồm hierachical clustering và kmeans clustering.\nTìm kiếm quan hệ giao dịch (market basket analysis), phân tích cấu trúc và mối liên hệ giữa các sản phẩm có quan hệ bổ trợ cho nhau với thuật toán association rules\nGiảm biến dữ liệu với Pricipal Component Analysis, giúp giảm cấu trúc dữ liệu nhiều chiều.\nPhân tích quan hệ ẩn trong dữ liệu Factor Analysis, tìm kiềm các cấu trúc ẩn trong dữ liệu, được ứng dụng đặc biệt nhiều trong phân tích bảng hỏi (questionaire)"
  },
  {
    "objectID": "p04-02-hierarchical-clustering.html#giới-thiệu",
    "href": "p04-02-hierarchical-clustering.html#giới-thiệu",
    "title": "36  Hierachical clustering",
    "section": "36.1 Giới thiệu",
    "text": "36.1 Giới thiệu\nPhân nhóm dữ liệu là kỹ thuật hỗ trợ chia dữ liệu thành các nhóm có sự tương đồng với nhau nhiều nhất. Bài toán phân nhóm có rất nhiều ứng dụng trong thực tế như sau:\n\nPhân nhóm khách hàng dựa trên dữ liệu nhân khẩu học demographic và hành vi giao dịch.\nPhân tích cấu trúc thị trường (market structure analysis), tìm kiếm các sản phẩn, dịch vụ, thị trường có tính chất tương tự nhau. Ví dụ, phân tích cấu trúc các loại xe hơi đang có trên thị trường để xem độ tương đồng giữa các dòng xe trên thị trường\n\nVề cơ bản, bài toán phân nhóm dữ liệu được chia thành 2 loại:\n\nHierachical Clustering: Chưa xác định số nhóm cần phân tách, coi mỗi quan sát là 1 nhóm và tính toán khoảng cách giữa các quan sát và nhóm dần từ dưới lên cho đến khi tất cả quan sát thuộc cùng 1 nhóm. Ta có thể coi phương pháp này là bottom-up.\nK-mean Clustering: Xác định rõ số lượng nhóm cần phân tách và phân nhóm từng quan sát vào các nhóm nhất định. Ta có thể coi phương pháp này là top-down.\n\nVề mặt nguyên lý, yếu tố quan trọng nhất để có thể phân nhóm dữ liệu là phải tính được khoảng cách giữa các quan sát. Thông thường, khoảng cách giữa hai điểm được tính bằng khoảng cách Euclide hoặc Cosine. Khoảng cách giữa hai điểm trong không gian hai chiều có thể được tính như sau.\n\nKhoảng cách Euclide: \\(d(A, B) = \\sqrt{(x_a - x_b)^2 + (y_a-y_b)^2}\\)\nKhoảng cách Cosine: \\(d(A,B) = \\frac{x_a*x_b + x_a*y_b}{\\sqrt{x_a^2 + y_a^2}* \\sqrt{x_b^2+y_b^2}}\\)\n\nKhác với so sánh khoảng cách giữa các điểm, khoảng cách giữa các nhóm (cluster) có thể được tính như sau.\n\nMinimum distance: Khoảng cách của hai điểm gần nhất giữa 2 nhóm - cluster dạng này được gọi là single linkage clustering\nMaximum distance: Khoảng cách của hai điểm xa nhất giữa 2 nhóm - cluster dạng này được gọi là complete linkage clustering\nAverage distance: Khoảng cách trung bình của tất cả các điểm của 2 nhóm - cluster dạng này được gọi là average linkage clustering\nCentroid distance: Khoảng cách giữa tâm (centroid) của 2 nhóm. Centroid của mỗi nhóm được tính bằng giá trị trung bình của tất cả các biến trong nhóm. Cluster dạng này được gọi là Centroid linkage clustering\n\n\nNgoài ra, ta còn thường sử dụng ward's method. Đây là phương pháp tính toán lượng thông tin bị mất đi (loss information) khi phân nhóm. Chỉ số này được tính bằng ESS (sum of square) giữa mối điểm trong nhóm với giá trị trung bình của nhóm.\nVí dụ, các điểm (1,2,3,0) có giá trị trung bình là 1.5, lượng thông tin bị mất đi được tính như sau.\n\\[ESS = (1-1.5)^2 + (2-1.5)^2 + (3-1.5^2) + (0-1.5)^2 = 5\\]"
  },
  {
    "objectID": "p04-02-hierarchical-clustering.html#hierarchical-cluster",
    "href": "p04-02-hierarchical-clustering.html#hierarchical-cluster",
    "title": "36  Hierachical clustering",
    "section": "36.2 Hierarchical cluster",
    "text": "36.2 Hierarchical cluster\nThuật toán hierachical cluster diễn ra như sau:\n\nXác định mỗi quan sát là 1 cluster\nTính khoảng cách giữa các quan sát\nNhóm 2 cluster có khoảng cách gần nhất thành 1 cluster mới\nLặp lại bước 2,3 cho đến khi tất cả các quan sát hợp lại thành 1 cluster\n\nKhi phân tích, có 2 câu hỏi lớn:\n\nCác đối tượng phân nhóm với nhau như thế nào? Ta có thể sử dụng gói cluster::hclust. Biểu đồ mô tả sự phân nhóm được gọi là dendogram\nNên phân thành bao nhiêu nhóm (cluster)? Sử dụng NbClust, tuy nhiên, với mỗi tiêu chí, các kết quả lại có thể đưa ra khác nhau.\n\nTrong ví dụ này, ta sẽ phân nhóm các loại thực phẩm theo các thông số năng lượng như sau.\n\nlibrary(dplyr)\ndata(nutrient, package = \"flexclust\")\nrow.names(nutrient) <- row.names(nutrient) %>% tolower\nnutrient %>% head\n\n                energy protein fat calcium iron\nbeef braised       340      20  28       9  2.6\nhamburger          245      21  17       9  2.7\nbeef roast         420      15  39       7  2.0\nbeef steak         375      19  32       9  2.6\nbeef canned        180      22  10      17  3.7\nchicken broiled    115      20   3       8  1.4\n\n\n\n#Scale dữ liệu\nnutrient.scaled <- scale(nutrient)\n#Tính khoảng cách\nd <- dist(nutrient.scaled)\n#Tính cluster\nfit.average <- hclust(d, method = \"average\") \n\n#Vizualize\nplot(fit.average,\n     hang = -1,\n     #Điều chỉnh vị trí của text\n     cex = 0.8,\n     #Kích cỡ của text\n     main = \"Average Linkage Clustering\")\n\n\n\n\nNhư vậy, với mô hình trên, ta thấy beef braised tương đồng nhất với smoked ham, nhóm này gần nhất với nhóm chứa pork roast và pork skimmered\n\n#Xác định số lượng cluster\nlibrary(NbClust)\nnc <- NbClust(\n        nutrient.scaled,\n        # Data\n        distance = \"euclidean\",\n        # Cách tính khoảng cách giữa các điểm\n        min.nc = 2,\n        # Số lượng cluster nhỏ nhất\n        max.nc = 15,\n        # Số lượng cluster lớn nhất\n        method = \"average\"\n) # Cách tính khoảng cách giữa cluster\n\n\n\n\n*** : The Hubert index is a graphical method of determining the number of clusters.\n                In the plot of Hubert index, we seek a significant knee that corresponds to a \n                significant increase of the value of the measure i.e the significant peak in Hubert\n                index second differences plot. \n \n\n\n\n\n\n*** : The D index is a graphical method of determining the number of clusters. \n                In the plot of D index, we seek a significant knee (the significant peak in Dindex\n                second differences plot) that corresponds to a significant increase of the value of\n                the measure. \n \n******************************************************************* \n* Among all indices:                                                \n* 4 proposed 2 as the best number of clusters \n* 4 proposed 3 as the best number of clusters \n* 2 proposed 4 as the best number of clusters \n* 4 proposed 5 as the best number of clusters \n* 1 proposed 9 as the best number of clusters \n* 1 proposed 10 as the best number of clusters \n* 2 proposed 13 as the best number of clusters \n* 1 proposed 14 as the best number of clusters \n* 4 proposed 15 as the best number of clusters \n\n                   ***** Conclusion *****                            \n \n* According to the majority rule, the best number of clusters is  2 \n \n \n******************************************************************* \n\nresult <- nc$Best.nc %>% t %>% as.data.frame()\nresult\n\n           Number_clusters Value_Index\nKL                       3      7.1625\nCH                      15     43.7316\nHartigan                 4     21.5587\nCCC                     13      8.0689\nScott                    9     62.4209\nMarriot                  3  14912.6123\nTrCovW                   3    359.6716\nTraceW                   5     29.0322\nFriedman                 5    501.0101\nRubin                   13     -2.6324\nCindex                  10      0.4126\nDB                      15      0.3709\nSilhouette              15      0.6446\nDuda                     2      0.7197\nPseudoT2                 2      9.3494\nBeale                    2      1.1705\nRatkowsky                5      0.3850\nBall                     3     28.9871\nPtBiserial               4      0.6921\nFrey                     1          NA\nMcClain                  2      0.0449\nDunn                    14      0.7494\nHubert                   0      0.0000\nSDindex                  5      0.8518\nDindex                   0      0.0000\nSDbw                    15      0.0189\n\nresult %>%\n        group_by(Number_clusters) %>%\n        summarise(no = n()) %>% \n        ungroup %>% \n        arrange(desc(no))\n\n# A tibble: 11 × 2\n   Number_clusters    no\n             <dbl> <int>\n 1               2     4\n 2               3     4\n 3               5     4\n 4              15     4\n 5               0     2\n 6               4     2\n 7              13     2\n 8               1     1\n 9               9     1\n10              10     1\n11              14     1\n\n\nSố lượng cluster được tính toán dựa trên 26 các tiêu chí khác nhau. Với kết quả trên, ta có thể lựa chọn 2, 3 hoặc 5 cluster.\n\nplot(fit.average,\n     hang = -1,\n     cex = .8,\n     main = \"Average Linkage Clustering - 5 Cluster\")\n\nrect.hclust(fit.average, k = 5)\n\n\n\n\nLưu ý: Phân nhóm theo chiều dọc chỉ nên áp dụng với số lượng quan sát không quá lớn."
  },
  {
    "objectID": "p04-02-hierarchical-clustering.html#đánh-giá-chất-lượng-phân-nhóm",
    "href": "p04-02-hierarchical-clustering.html#đánh-giá-chất-lượng-phân-nhóm",
    "title": "36  Hierachical clustering",
    "section": "36.3 Đánh giá chất lượng phân nhóm",
    "text": "36.3 Đánh giá chất lượng phân nhóm\nKhác với các bài toán dự báo, bài toán phân nhóm cần ưu tiên yếu tố giải thích để có thể trình bày và ứng dụng trong hoạt động doanh nghiệp. Do đó, khi phân nhóm dữ liệu, để đánh giá chất lượng phân nhóm, ta cần quan tâm đến các yếu tố sau:\n\nKhả năng giải thích của mỗi nhóm\nĐộ ổn định của mỗi nhóm\nĐộ phân loại (khoảng cách) giữa các nhóm\nSố lượng nhóm"
  },
  {
    "objectID": "p04-02-hierarchical-clustering.html#tài-liệu-tham-khảo",
    "href": "p04-02-hierarchical-clustering.html#tài-liệu-tham-khảo",
    "title": "36  Hierachical clustering",
    "section": "36.4 Tài liệu tham khảo",
    "text": "36.4 Tài liệu tham khảo\n\nChapter 15 - Cluster Analysis - Data Mining for Business Analytics with R\nChapter 16 - Cluster Analysis - R in Action"
  },
  {
    "objectID": "p04-03-kmeans.html#giới-thiệu",
    "href": "p04-03-kmeans.html#giới-thiệu",
    "title": "37  k-means",
    "section": "37.1 Giới thiệu",
    "text": "37.1 Giới thiệu\nTrong chương này, chúng ta sẽ cùng tìm hiểu về thuật toán clustering K-means - một thuật toán thuộc về nhóm unsupervised learning (“học không giám sát”).\nClustering K-means có thể ứng dụng trong các lĩnh vực như ngân hàng hay thương mại điện tử vào việc phân nhóm khách hàng dựa trên những đặc điểm về hành vi, thói quen chi tiêu và demographics của họ. Điều đó sẽ giúp cho ngân hàng hay các công ty thương mại điện tử có thể hiểu rõ được hành vi khách hàng của mình, để sau đó có thể offer những sản phẩm, dịch vụ phù hợp với nhu cầu của từng khách hàng.\nThuật toán K-means sẽ nhóm các đối tượng (quan sát) có những đặc điểm và hành vi tương đồng vào K nhóm bất kỳ. Số lượng nhóm K sẽ phụ thuộc vào mục đích cũng như bối cảnh mà chúng ta đặt ra (thông thường nên từ 2 đến 10 nhóm)."
  },
  {
    "objectID": "p04-03-kmeans.html#thuật-toán",
    "href": "p04-03-kmeans.html#thuật-toán",
    "title": "37  k-means",
    "section": "37.2 Thuật toán",
    "text": "37.2 Thuật toán\nThuật toán của K-means diễn ra như sau:\n\nBước 1: Chọn ngẫu nhiên K tâm/centroids\nBước 2: Tính khoảng cách từ các đối tượng/quan sát đến K tâm\nBước 3: Dựa vào giá trị khoảng cách ở bước 2, chúng ta nhóm các quan sát vào K nhóm: Các quan sát gần với tâm nào hơn thì sẽ thuộc về nhóm đó.\nBước 4: Xác định các tâm mới của K nhóm (tâm mới của mỗi nhóm có tọa độ bằng trung bình tọa độ của các quan sát trong nhóm đó).\nBước 5: Lặp lại bước 2 đến bước 4 cho đến khi không có sự thay đổi kết quả phân nhóm các quan sát hoặc theo số lượng iterations mà chúng ta đặt ra cho thuật toán chạy.\n\nCác bước của kmean có thể trực quan hóa như sau.\n\nXác định k tối ưu\nThuật toán k-means đòi hỏi ta phải xác định k trước khi xây dựng mô hình. Do đó, ngay khi bắt đầu, ta không thể biết chính xác số tối ưu của centroid mà phải qua vài bước thử để có thể xác định chính xác. Thông thường có hai cách xác định k tối ưu:\n\nElbow method: Với phương pháp này, ta xác định WSS (Within Cluster Sum of Square) với k khác nhau. Số lượng k tối ưu là điểm uốn trên đồ thị.\nSilhouette method: Tính toán chỉ số Sihouette với số k khác nhau, k tối ưu là điểm có chỉ số Silhouette cao nhất. Chỉ số này có giá trị trong khoảng (-1, 1) với giá trị càng cao càng tốt."
  },
  {
    "objectID": "p04-03-kmeans.html#ưu-nhược-điểm-của-k-means",
    "href": "p04-03-kmeans.html#ưu-nhược-điểm-của-k-means",
    "title": "37  k-means",
    "section": "37.3 Ưu nhược điểm của K-means",
    "text": "37.3 Ưu nhược điểm của K-means\nƯu điểm của thuật toán clustering K-means là đơn giản và dễ tính toán.\nTuy nhiên thuật toán này có một số nhược điểm sau:\n\nThứ nhất, do lấy ngẫu nhiên tâm/centroid nên khi dữ liệu lớn với rất nhiều các biến đầu vào thuật toán có thể phân loại các quan sát vào các nhóm khác nhau tại mỗi vòng (iteration).\nThứ hai, thuật toán sẽ gặp vấn đề nếu các biến đầu vào có các đơn vị khác nhau, ví dụ như: biến chi tiêu hoặc thu nhập của khách hàng đơn vị là triệu đồng, biến đánh dấu khách hàng có sử dụng thẻ tín dụng (dummy 1-0), hoặc biến tỷ lệ chi tiêu thẻ tín dụng trên hạn mức được cấp, v.v.\n\nVì vậy, để khắc phục vấn đề trên trước khi áp dụng thuật toán K-means, chúng ta cần phải chuẩn hóa dữ liệu đầu vào bằng cách scaling dữ liệu:\n\\[x_i = \\frac{x_i - min(x_i)}{max(x_i) - min(x_i)}\\]\n\nThứ ba, k-means không thể thực hiện tốt việc phân nhóm nếu tồn tại các nhóm con nằm lồng trong nhóm cha.\n\n\n\nThứ tư, khó giải thích cho đơn vị kinh doanh hiểu. Với trường hợp có 2 biến, có thể giải thích dễ dàng nhưng khi từ 4-5 biến trở lên dùng cho mô hình phân nhóm, gần như rất khó giải thích cho các bên nghiệp vụ."
  },
  {
    "objectID": "p04-03-kmeans.html#thực-hành-trên-r",
    "href": "p04-03-kmeans.html#thực-hành-trên-r",
    "title": "37  k-means",
    "section": "37.4 Thực hành trên R",
    "text": "37.4 Thực hành trên R\nSau đây chúng ta sẽ cùng tìm hiểu cách thực hiện K-means trên R.\n\nBước 1: Chuẩn hóa dữ liệu (Scaling):\n\n\nlibrary(tidyverse)\n# Scale dữ liệu\niris_scale <- scale(iris %>% select(-Species))\n\n\nBước 2: Xác định số lượng nhóm tối ưu\n\n\n# Xác định số lượng nhóm tối ưu\nlibrary(factoextra)\nfviz_nbclust(iris_scale, \n             kmeans, \n             method = \"silhouette\")\n\n\n\n\nKết quả cho thấy trường hợp này chúng ta nên chia làm 2 nhóm sẽ tối ưu. Trong thực tế, việc chia thành bao nhiêu nhóm sẽ phụ thuộc vào mục tiêu kinh doanh, thông thường số lượng nhóm nên từ 2 đến 10.\n\nBước 3: Thực hiện clustering K-means thành 2 nhóm\n\n\n# Thực hiện thuật toán K-means\nk_means_fit <- kmeans(iris_scale, \n                      centers = 2 # số lượng nhóm\n                      )\n\n# Nhóm của các quan sát\nk_means_fit$cluster\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\n# Tâm của mỗi nhóm\nk_means_fit$centers\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1   -1.0111914   0.8504137    -1.300630  -1.2507035\n2    0.5055957  -0.4252069     0.650315   0.6253518\n\n# Số lượng quan sát tại mỗi nhóm\nk_means_fit$size\n\n[1]  50 100\n\n\n\nBước 4: Trực quan hóa kết quả\n\n\n# Trực quan hóa kết quả K-means\nfviz_cluster(k_means_fit, \n             data = iris_scale,\n             geom = \"point\",\n             main = \"Iris Cluster\"\n             )+\n  theme_minimal()"
  },
  {
    "objectID": "p04-03-kmeans.html#lưu-ý",
    "href": "p04-03-kmeans.html#lưu-ý",
    "title": "37  k-means",
    "section": "37.5 Lưu ý",
    "text": "37.5 Lưu ý\nDự báo với nhóm mới\nSau khi phân nhóm các quan sát với k-means, trong nhiều trường hợp, ta có thể dự báo quan sát mới thuộc nhóm nào. Để thực hiện điều này, cách đơn giản nhất là tính toán khoảng cách giữa quan sát mới và với các centroid vừa tạo. Quan sát mới gần với centroid nào nhất sẽ được gán vào nhóm đó\nTrọng số khi dự báo quan sát mới\nTrong nhiều trường hợp, việc dự báo 1 quan sát thuộc nhóm nào sẽ chịu ảnh hưởng bởi 1 vài biến quan trọng nào đấy theo nhu cầu kinh doanh. Ví dụ, phân nhóm khách hàng nhưng trọng số quan trọng là hành vi giao dịch. Trong trường hợp này, ta cũng có thể tính toán khoảng cách giữa quan sát mới với các centroid mới tạo nhưng đánh trọng số cao hơn với 1 số biến nhất định.\nCác bước khi xây dựng profile\nKhi phân nhóm xong, việc tiếp theo là xây dựng profile của các nhóm. Tuy nhiên, khi xây dựng mô hình không thể đưa hết các biến vào mà cần chia làm 2 bước.\n\nBước 1: Phân nhóm dựa trên 1 số các biến cơ bản. VD: Dựa trên giá trị, tần xuất giao dịch của khách hàng thuộc các nhóm chi tiêu khác nhau. Tại bước này, cần đặt tên các nhóm dựa trên phân bổ và hành vi điển hình của khách hàng.\nBước 2: Map ngược lại các biến cần quan tâm sau khi đã xác định được chính xác khách hàng thuộc profile nào. VD: Tính trung bình nhóm tuổi, thu nhập của khách hàng với nhóm profile thiên về chi tiêu online"
  },
  {
    "objectID": "p04-03-kmeans.html#tài-liệu-tham-khảo",
    "href": "p04-03-kmeans.html#tài-liệu-tham-khảo",
    "title": "37  k-means",
    "section": "37.6 Tài liệu tham khảo",
    "text": "37.6 Tài liệu tham khảo\n\nTài liệu cần đọc: “Data Mining Techniques in CRM - inside customer segmentation”\nChapter 15 - Cluster Analysis - Data Mining for Business Analytics with R\nChapter 16 - Cluster Analysis - R in Action"
  },
  {
    "objectID": "p04-04-basket-analysis.html#các-khái-niệm-cơ-bản",
    "href": "p04-04-basket-analysis.html#các-khái-niệm-cơ-bản",
    "title": "38  Phân tích giỏ hàng",
    "section": "38.1 Các khái niệm cơ bản",
    "text": "38.1 Các khái niệm cơ bản\nTrong phân tích giỏ hàng, chỉ có 5 thuật ngữ đơn giản ta cần phải nhớ là transaction, rule, support, confidence và lift\n\nItem: Sản phẩm chứa trong giỏ hàng\nTransaction: Giao dịch là một hoặc một nhóm các sản phẩm được mua khi khách hàng thực hiện trong cùng một giao dịch\nRule: Rule là một quy tắc thể hiện mối quan hệ giữa các sản phẩm có trong cùng một giỏ hàng, có dạng nếu A, thì B.\n\n\\[\\left \\{{i_{1},i_{2},...}\\right \\} => {i_{k}}\\]\nVí dụ:\n\n{Bánh mỳ} => {Sữa}: Nếu khách hàng mua bánh mỳ thì khách hàng sẽ mua thêm sữa\n\n\nSupport：Tần suất (dưới dạng phần trăm) xuất hiện của các quy tắc trong tổng số các giao dịch.\n\n\nVí dụ: Một cửa hàng trong tháng 1 có 100 khách hàng, mỗi khách hàng thực hiện một giao dịch. Trong đó 50 khách hàng mua sản phẩm A, 75 khách hàng mua sản phẩm B, và 25 khách hàng mua cả sản phẩm A và B - Support(sản phẩm A) = 50% - Support(Sản phẩm A, sản phẩm B) = 25%\n\n\nConfidence: Cơ hội mua sản phẩm tiếp theo trong hành vi giao dịch của khác hàng.\n\n\\[Confidence (i_{m} => i_{n}) = \\frac{support(i_{m}\\frown i_{n})}{support (i_{m})}\\] > Ví dụ: Confidence (sản phẩm A , sản phẩm B) = 25/50 = 50% - Tức là nếu 1 người mua sản phẩm A, thì xác suất họ cũng mua sản phẩm B là 50%\n\nLift: Nếu khách hàng mua sản phẩm A thì khả năng họ mua sản phẩm B sẽ tăng lên bao nhiêu % \\[Lift(i_{m} => i_{n}) = \\frac{support(i_{m}\\frown i_{n})}{support(i_{m}) * support (i_{n})} = \\frac{Confidence (i_{m} => i_{n})}{support (i_{n})}\\]\n\nLift có thể cho 3 loại giá trị\n\nLift > 1: tức là những sản phẩm ở vế trái của rule sẽ làm tăng khả năng xảy ra của những sản phẩm ở vế phải của rule (2 sản phẩm bổ trợ). Ví dụ, mua bia sẽ mua thêm lạc.\nLift < 1: tức là những sản phẩm ở vế trái của rule sẽ làm giảm khả năng xảy ra của những sản phẩm ở vế phải của rule ( 2 sản phẩm thay thế được cho nhau). Ví dụ, mua bia sẽ không mua thêm cafe.\nLift = 1: Các sản phẩm ở vế trái và vế phải xuất hiện độc lập với nhau về mặt thống kê, ta không thể đưa ra kết luận về tương quan giữa các sản phẩm."
  },
  {
    "objectID": "p04-04-basket-analysis.html#cách-thực-hiện-mô-hình",
    "href": "p04-04-basket-analysis.html#cách-thực-hiện-mô-hình",
    "title": "38  Phân tích giỏ hàng",
    "section": "38.2 Cách thực hiện mô hình",
    "text": "38.2 Cách thực hiện mô hình\n\nlibrary(dplyr)\nlibrary(arules)\nlibrary(arulesViz)\ndata(\"Groceries\")\n\nDữ liệu Groceries chứa gần 10,000 giao dịch với hơn 160 sản phẩm khác nhau. Ta có thể xem chi tiết dưới đây.\n\n# Thống kê giao dịch\nsummary(Groceries)\n\ntransactions as itemMatrix in sparse format with\n 9835 rows (elements/itemsets/transactions) and\n 169 columns (items) and a density of 0.02609146 \n\nmost frequent items:\n      whole milk other vegetables       rolls/buns             soda \n            2513             1903             1809             1715 \n          yogurt          (Other) \n            1372            34055 \n\nelement (itemset/transaction) length distribution:\nsizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77   55   46 \n  17   18   19   20   21   22   23   24   26   27   28   29   32 \n  29   14   14    9   11    4    6    1    1    1    1    3    1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.409   6.000  32.000 \n\nincludes extended item information - examples:\n       labels  level2           level1\n1 frankfurter sausage meat and sausage\n2     sausage sausage meat and sausage\n3  liver loaf sausage meat and sausage\n\n\nLưu ý: Dữ liệu phục vụ phân tích giỏ hàng không phải là dữ liệu dạng dataframe thông thường mà được cấu trúc định dạng transaction.\n\n# Sử dụng dữ liệu groceries\nclass(Groceries)\n\n[1] \"transactions\"\nattr(,\"package\")\n[1] \"arules\"\n\nstr(Groceries)\n\nFormal class 'transactions' [package \"arules\"] with 3 slots\n  ..@ data       :Formal class 'ngCMatrix' [package \"Matrix\"] with 5 slots\n  .. .. ..@ i       : int [1:43367] 13 60 69 78 14 29 98 24 15 29 ...\n  .. .. ..@ p       : int [1:9836] 0 4 7 8 12 16 21 22 27 28 ...\n  .. .. ..@ Dim     : int [1:2] 169 9835\n  .. .. ..@ Dimnames:List of 2\n  .. .. .. ..$ : NULL\n  .. .. .. ..$ : NULL\n  .. .. ..@ factors : list()\n  ..@ itemInfo   :'data.frame': 169 obs. of  3 variables:\n  .. ..$ labels: chr [1:169] \"frankfurter\" \"sausage\" \"liver loaf\" \"ham\" ...\n  .. ..$ level2: Factor w/ 55 levels \"baby food\",\"bags\",..: 44 44 44 44 44 44 44 42 42 41 ...\n  .. ..$ level1: Factor w/ 10 levels \"canned food\",..: 6 6 6 6 6 6 6 6 6 6 ...\n  ..@ itemsetInfo:'data.frame': 0 obs. of  0 variables\n\n# Xem 5 giao dịch đầu tiên\nGroceries[1:5] %>% inspect\n\n    items                     \n[1] {citrus fruit,            \n     semi-finished bread,     \n     margarine,               \n     ready soups}             \n[2] {tropical fruit,          \n     yogurt,                  \n     coffee}                  \n[3] {whole milk}              \n[4] {pip fruit,               \n     yogurt,                  \n     cream cheese ,           \n     meat spreads}            \n[5] {other vegetables,        \n     whole milk,              \n     condensed milk,          \n     long life bakery product}\n\n\nTrong thực tế, khi triển khai phân tích, việc đầu tiên ta cần làm là biến đổi từ định dạng dataframe sang định dạng transactions. Ta có thể biến đổi định dạng của dataframe về transactions với hàm as.\n\ndf <- data.frame(\n  prod_1 = c(1,0, 1) %>% as.factor,\n  prod_2 = c(0,0, 1) %>% as.factor,\n  prod_3 = c(1, 0, 0) %>% as.factor\n)\ndf\n\n  prod_1 prod_2 prod_3\n1      1      0      1\n2      0      0      0\n3      1      1      0\n\ndf %>% select(-1) %>% as(\"transactions\") %>% inspect\n\n    items                transactionID\n[1] {prod_2=0, prod_3=1} 1            \n[2] {prod_2=0, prod_3=0} 2            \n[3] {prod_2=1, prod_3=0} 3            \n\n\nPhân tích khám phá nhanh các sản phẩm được mua nhiều nhất.\n\n# Vẽ barchart đơn giản về các item phổ biến nhất\nitemFrequencyPlot(Groceries, \n                  type = \"absolute\", \n                  topN = 20, decreasing = T)"
  },
  {
    "objectID": "p04-04-basket-analysis.html#ba-câu-hỏi-khi-phân-tích-giỏ-hàng",
    "href": "p04-04-basket-analysis.html#ba-câu-hỏi-khi-phân-tích-giỏ-hàng",
    "title": "38  Phân tích giỏ hàng",
    "section": "38.3 Ba câu hỏi khi phân tích giỏ hàng",
    "text": "38.3 Ba câu hỏi khi phân tích giỏ hàng\nKhi sử dụng kỹ thuật phân tích giỏ hàng, có 3 câu hỏi thường gặp về mặt kinh doanh cần phải trả lời là:\n\nCác sản phẩm nào hay được mua cùng nhau?\nNếu khách hàng mua sản phẩm A rồi thì sẽ hay mua tiếp sản phẩm nào?\nKhách hàng nếu mua sản phẩm B thì trước đấy hay mua sản phẩm nào?\n\n\n38.3.1 Các sản phẩm nào hay được mua cùng nhau\n\nrules <- apriori(Groceries, \n                 parameter = list(supp = 0.001, \n                            # support >= 0.1% \n                                  conf = 0.5))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.5    0.1    1 none FALSE            TRUE       5   0.001      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 9 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\nsorting and recoding items ... [157 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 5 6 done [0.01s].\nwriting ... [5668 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n                            # confidence >= 50%\n# Tổng hợp các rules\nrules %>% summary\n\nset of 5668 rules\n\nrule length distribution (lhs + rhs):sizes\n   2    3    4    5    6 \n  11 1461 3211  939   46 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    3.00    4.00    3.92    4.00    6.00 \n\nsummary of quality measures:\n    support           confidence        coverage             lift       \n Min.   :0.001017   Min.   :0.5000   Min.   :0.001017   Min.   : 1.957  \n 1st Qu.:0.001118   1st Qu.:0.5455   1st Qu.:0.001729   1st Qu.: 2.464  \n Median :0.001322   Median :0.6000   Median :0.002135   Median : 2.899  \n Mean   :0.001668   Mean   :0.6250   Mean   :0.002788   Mean   : 3.262  \n 3rd Qu.:0.001729   3rd Qu.:0.6842   3rd Qu.:0.002949   3rd Qu.: 3.691  \n Max.   :0.022267   Max.   :1.0000   Max.   :0.043416   Max.   :18.996  \n     count      \n Min.   : 10.0  \n 1st Qu.: 11.0  \n Median : 13.0  \n Mean   : 16.4  \n 3rd Qu.: 17.0  \n Max.   :219.0  \n\nmining info:\n      data ntransactions support confidence\n Groceries          9835   0.001        0.5\n                                                                  call\n apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.5))\n\n\nNhư vậy, khi khai phá dữ liệu của tập Groceries, ta có 5668 rules thỏa mãn hai điều kiện:\n\nTần suất xuất hiện đạt ít nhất 1%\nConfidence của rule đạt ít nhất 50%\n\n\n# Top 5 rules có lift cao nhất\nrules %>% \n  sort(by = \"lift\") %>% \n  head(5) %>% \n  as(\"data.frame\")\n\n                                                     rules     support\n53        {Instant food products,soda} => {hamburger meat} 0.001220132\n37                         {soda,popcorn} => {salty snack} 0.001220132\n444                       {flour,baking powder} => {sugar} 0.001016777\n327                {ham,processed cheese} => {white bread} 0.001931876\n55  {whole milk,Instant food products} => {hamburger meat} 0.001525165\n    confidence    coverage     lift count\n53   0.6315789 0.001931876 18.99565    12\n37   0.6315789 0.001931876 16.69779    12\n444  0.5555556 0.001830198 16.40807    10\n327  0.6333333 0.003050330 15.04549    19\n55   0.5000000 0.003050330 15.03823    15\n\n\nGiải thích: Với rule đầu tiên là {Instant food products, soda}, ta có thể diễn giải như sau:\n\nsupport = 0.00122 - Tần xuất xuất hiện rules trong tổng số các transaction là 1.22%\nconfidence = 0.63157 - Nếu khách hàng mua đồ ăn nhanh (instant food products), 63.16% khách hàng sẽ mua thêm soda\nlift = 18.99 - Mối quan hệ giữa hai sản phẩm đồ ăn nhanh và soda cao gần 19 lần so với thông thường (khi hai sản phẩm hoàn toàn độc lập với nhau)\n\nLưu ý: Khi phân tích, ta cần loại bỏ các rule thừa (redundant) khi phân tích dữ liệu. Một rule A được gọi là một rule thừa nếu tồn tại một rule con có confidence lớn hơn hoặc bằng rule A này. Rule B được gọi là rule con của rule A nếu có cùng RHS nhưng các sản phẩm trong rule B ít hơn rule A\nVí dụ: Với 2 rule\n\nRule A với {a,b,c} → {d}\nRule B với {a,b} → {d} Rule B với {a,b} → {d} được gọi là rule thừa nếu \\(conf(A) >= conf(B)\\)\n\nCách loại bỏ rule thừa trong R\n\nsubset.matrix <- is.subset(rules, rules)\nsubset.matrix[lower.tri(subset.matrix, diag = T)] <- NA\nredundant <- colSums(subset.matrix, na.rm = T) >= 1\nrules.pruned <- rules[!redundant]\nrules <- rules.pruned\nrules %>% summary\n\nNhư vậy, sau khi lọc bỏ các rule thừa, số lượng rule trong dữ liệu giảm đi gần 3 lần xuống còn 1904 rules khác nhau. Ta có thể vẽ biểu đồ cho nhóm 10 rules có lift cao nhất như sau.\n\nrules %>% \n  head(10) %>% \n  plot(method = \"graph\") \n\n\n\n38.3.2 Khách hàng mua sản phẩm A thì sẽ mua sản phẩm nào tiếp theo?\n\nrules <- apriori(data = Groceries, \n                  parameter = list(supp = 0.001, \n                                   conf = 0.15,\n                                   minlen = 2), \n                  appearance = list(default = \"rhs\",\n                                    lhs = \"whole milk\"), \n                  control = list(verbose = F)) \nrules %>% \n  sort(by = \"lift\") %>%\n  head %>% \n  as(\"data.frame\")\n\n                               rules    support confidence coverage      lift\n2  {whole milk} => {root vegetables} 0.04890696  0.1914047 0.255516 1.7560310\n1   {whole milk} => {tropical fruit} 0.04229792  0.1655392 0.255516 1.5775950\n4           {whole milk} => {yogurt} 0.05602440  0.2192598 0.255516 1.5717351\n6 {whole milk} => {other vegetables} 0.07483477  0.2928770 0.255516 1.5136341\n5       {whole milk} => {rolls/buns} 0.05663447  0.2216474 0.255516 1.2050318\n3             {whole milk} => {soda} 0.04006101  0.1567847 0.255516 0.8991124\n  count\n2   481\n1   416\n4   551\n6   736\n5   557\n3   394\n\n\nTham số lhs cho phép chúng ta lựa chọn điều kiện về sản phẩm được mua đầu tiên. Trong trường hợp này, ta thấy khách hàng mua sữa sẽ có xu hương mua thêm rau củ quả (root vegetables)\n\n\n38.3.3 Khách hàng mua sản phẩm gì thì sẽ mua tiếp sản phẩm A?\nTương tự với lhs, ta có thể sử dụng tham số rhs để tìm kiếm các khách hàng tiềm năng cho một sản phẩm đã xác định trước.\n\nrules <- apriori(data = Groceries,\n                 parameter = list(supp = 0.001, \n                                  conf = 0.08), \n                 appearance = list(default = \"lhs\",\n                                   rhs = \"whole milk\"), \n                 control = list(verbose = F))\nrules %>% \n  sort(by = \"lift\") %>%\n  head %>% \n  as(\"data.frame\") \n\n                                                          rules     support\n196                                {rice,sugar} => {whole milk} 0.001220132\n323              {canned fish,hygiene articles} => {whole milk} 0.001118454\n1643              {root vegetables,butter,rice} => {whole milk} 0.001016777\n1705 {root vegetables,whipped/sour cream,flour} => {whole milk} 0.001728521\n1716         {butter,soft cheese,domestic eggs} => {whole milk} 0.001016777\n1985        {pip fruit,butter,hygiene articles} => {whole milk} 0.001016777\n     confidence    coverage     lift count\n196           1 0.001220132 3.913649    12\n323           1 0.001118454 3.913649    11\n1643          1 0.001016777 3.913649    10\n1705          1 0.001728521 3.913649    17\n1716          1 0.001016777 3.913649    10\n1985          1 0.001016777 3.913649    10"
  },
  {
    "objectID": "p04-04-basket-analysis.html#ưu-nhược-điểm",
    "href": "p04-04-basket-analysis.html#ưu-nhược-điểm",
    "title": "38  Phân tích giỏ hàng",
    "section": "38.4 Ưu nhược điểm",
    "text": "38.4 Ưu nhược điểm\nNhư vậy, ta vừa thực hiện xong việc ứng dụng phân tích giỏ hàng. Đây là phương pháp cực kỳ hữu hiệu trong việc tìm kiếm các mối quan hệ ẩn, chưa được khám phá giữa các biến. Phương pháp này cũng có các ưu và nhược điểm như sau.\nƯu điểm:\n\nThực hiện nhanh\nTính giải thích cao, trưc quan hóa\nCó thể nhanh chóng tìm ra tập khách hàng tiềm năng theo điều kiện và không cần phải chia ra thành các tập train/test như các thuật toán machine learning khác.\n\nNhược điểm:\n\nChỉ dùng cho các biến factor, không dùng được cho các biến dạng số như thu nhập, độ tuổi,… Nếu muốn đưa các biến này vào cần phải biến đổi thành dạng factor\nKhi có quá nhiều nhóm, tốc độ tính toán có thể rất chậm và tràn bộ nhớ"
  },
  {
    "objectID": "p04-05-pca.html#giới-thiệu",
    "href": "p04-05-pca.html#giới-thiệu",
    "title": "39  Principal component analysis",
    "section": "39.1 Giới thiệu",
    "text": "39.1 Giới thiệu\nPricipal Component Analysis và Factor Analysis không quan tâm đến việc dự báo do không có biến cần dự báo trước. Các kỹ thuật này trả lời các câu hỏi sau\n\nLiệu tồn tại các nhóm giữa các đối tượng trong dữ liệu?\nLàm thế nào để giảm số lượng các biến?\nLiệu có tồn tại mối quan hệ ẩn giữa các biến?\n\nSo sánh giữa PCA và FA\nPCA & FA là 2 phương pháp có quan hệ mật thiết với nhau. Có thể coi FA là phương pháp ngược lại với PCA\n\nPCA là kỹ thuật giảm số lượng các biến có tương quan với nhau thành 1 số ít biến không tương quan và vẫn đảm bảo được càng nhiều thông tin (variance) của tập dữ liệu gốc càng tốt. VD: Giảm 30 biến thành 2 biến mới không tương quan với nhau, vẫn giữ được phần lớn lượng thông tin (ít nhất 80%) so với dữ liệu gốc. Mỗi biến mới được gọi là 1 “Principal Component”\nFA là kỹ thuật tìm những biến “ẩn” ảnh hưởng đến các dữ liệu gốc. VD: 30 biến từ tập dữ liệu gốc được tổ hợp từ 4 biến ẩn (latent factor)"
  },
  {
    "objectID": "p04-05-pca.html#lý-thuyết",
    "href": "p04-05-pca.html#lý-thuyết",
    "title": "39  Principal component analysis",
    "section": "39.2 Lý thuyết",
    "text": "39.2 Lý thuyết\nGọi \\(X_1,...,X_p\\) là các biến trong tập dữ liệu ban đầu. \\(PC_1\\) (The first principal component) là tổ hơp tuyến tính đã chuẩn hóa của X (normalized linear combination):\n\\[Z_1=\\phi_{11} X_1 + \\phi_{21}X_2+...+\\phi_{p1}X_p\\]\nChuẩn hóa (normalized) được định nghĩa là:\n\\[\\sum_{j=1}^p\\phi_{j1}^2=1\\]\n\\(\\phi_{11},...,\\phi_{p1}\\) được gọi là “loadings” của PC1\n\\(Z_2\\) được gọi là \\(PC_2\\) nếu \\(Z_2\\) thỏa mãn các điều kiện tương tự như \\(PC_1\\) và \\(Z_2\\) vuông góc với \\(Z_1\\), nghĩa là:\n\\[\\sum_{j=1}^p\\phi_{j1}\\phi_{j2}=0\\]\nThuật toán:\n\nChuẩn hóa ma trận phương sai, hiệp phương sai của tập dữ liệu gốc về dạng chuẩn N(0,1)\nTìm giá trị riêng và vectơ riêng (eigen value - eigen vector) của ma trận trên\nMỗi tổ hợp của giá trị riêng của vector riêng là một PC\n\nÝ nghĩa:\n\n\\(PC_1\\) là tổ hợp tuyến tính của các biến X mà trong đó, phương sai của cả tập dữ liệu thay đổi nhiều nhất\n\\(PC_2\\) là chiều thứ 2 của dữ liệu, vuông góc với dữ liệu 1, var biến đổi nhiều thứ nhì\n\nLựa chọn số PC:\nCó 3 cách lựa chọn sau:\n\nLựa chọn dưa trên lý thuyết đã biết\nLựa chọn PC có giá trị riêng lớn hơn 1\nLựa chọn số PC cho đến khi tổng var của tất cả các PC lớn hơn hoặc bằng 80% var của tập dữ liệu gốc"
  },
  {
    "objectID": "p04-05-pca.html#thực-hành",
    "href": "p04-05-pca.html#thực-hành",
    "title": "39  Principal component analysis",
    "section": "39.3 Thực hành",
    "text": "39.3 Thực hành\n\nlibrary(psych)\nlibrary(dplyr)\nlibrary(broom)\n\ndata <- iris %>% select(-Species)\ndescribe(data)\n\n             vars   n mean   sd median trimmed  mad min max range  skew\nSepal.Length    1 150 5.84 0.83   5.80    5.81 1.04 4.3 7.9   3.6  0.31\nSepal.Width     2 150 3.06 0.44   3.00    3.04 0.44 2.0 4.4   2.4  0.31\nPetal.Length    3 150 3.76 1.77   4.35    3.76 1.85 1.0 6.9   5.9 -0.27\nPetal.Width     4 150 1.20 0.76   1.30    1.18 1.04 0.1 2.5   2.4 -0.10\n             kurtosis   se\nSepal.Length    -0.61 0.07\nSepal.Width      0.14 0.04\nPetal.Length    -1.42 0.14\nPetal.Width     -1.36 0.06\n\npr.out <- prcomp(data, scale = T) #Scale data\npr.out\n\nStandard deviations (1, .., p=4):\n[1] 1.7083611 0.9560494 0.3830886 0.1439265\n\nRotation (n x k) = (4 x 4):\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\npr.out %>% summary\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\nTrong ví dụ trên, ta thấy PC2 đã cover được 95% độ biến động của dữ liệu. Do đó, ta có thể chọn 2 nhân tố chính để thực hiện PCA. Kết quả trực quan hóa sau khi thực hiện PCA như sau.\n\nlibrary(ggfortify)\nautoplot(pr.out, loadings = TRUE, \n         col = \"Species\",\n         loadings.colour = \"darkred\",\n         loadings.label = T,\n         data = iris) +\n  theme_minimal() +\n  theme(legend.position = \"top\") +\n  labs(title = \"PCA for iris dataset\")\n\n\n\n\nTrong ví dụ trên, ta thấy loài hoa setosa được phân thành nhóm riêng và yếu tố được tách nhánh nhiều nhất của nhóm này là Sepal. Trong khi đó, hai loại hoa còn lại khác nhau phần lớn ở thuộc tính Petal"
  },
  {
    "objectID": "p04-06-factor-analysis.html#so-sánh-giữa-efa-và-pca",
    "href": "p04-06-factor-analysis.html#so-sánh-giữa-efa-và-pca",
    "title": "40  Phân tích nhân tố ẩn (factor analysis)",
    "section": "40.1 So sánh giữa EFA và PCA",
    "text": "40.1 So sánh giữa EFA và PCA\n\n40.1.1 Các điểm giống nhau\n\nĐều là các kỹ thuật làm giảm dữ liệu, bằng cách sử dụng sự tương quan giữa các biến ban đầu để gộp thành các biến mới. Với k biến ban đầu, k biến mới sẽ được tạo ra. Tuy nhiên, ta sẽ chỉ giữ lại n biến mới với n < k sao cho lượng thông tin vẫn được bảo toàn nhất có thể (Eigen Value > 1 hoặc Cumulative Variance Percentage > 80 % như trong PCA)\n\n\n\n40.1.2 Các điểm khác nhau\n\n40.1.2.1 Thuật toán\n\nTrong PCA, các thành phần chính (principal components) là các tổ hợp tuyến tính của tất cả các biến ban đầu, sao cho PC1 là tổ hợp giữ được lượng thông tin nhiều nhất, PC2 được bổ sung vuông góc với PC1 nhằm thêm các thông tin còn thiếu trong PC1 và chứa lượng thông tin ít hơn PC1,…cứ như vậy, các PC được bổ sung sau sẽ vuông góc với các PC trước đó và chứa lượng thông tin giảm dần. Như vây các PC phải vuông góc với nhau, đồng nghĩa với việc các PC không được tương quan với nhau.\nTrong EFA, các nhân tố (factor) là tổ hợp tuyến tính của các biến ban đầu có tương quan với nhau, sao cho mỗi nhân tố giữ lại được lượng thông tin nhiều nhất của 1 nhóm các biến ban đầu. Khác với PCA, các factor có thể không vuông góc với nhau, đồng nghĩa với việc các factor có thể tương quan với nhau\n\n\n\n40.1.2.2 Mục đích sử dụng\n\nPCA được sử dụng để đơn giản hoá tập dữ liệu, sao cho với số biến mới tối thiểu vẫn giữ lại được tối đa lượng thông tin của tập dữ liệu ban đầu. Do đó, các biến mới (principal components) không cần được đặt tên\n\n\n\n\n\nEFA được sử dụng để tìm ra biến ẩn (latent variable, factor, concept) đằng sau các biến đo được có tương quan với nhau (correlated observed variables, group of items). Nói cách khác, nhóm biến ban đầu phản ánh các khía cạnh của 1 khái niệm chung. Do đó các biến ẩn/ khái niệm chung này phải có ý nghĩa và gọi tên được\n\n\n\n\n\n\n40.1.2.3 Ứng dụng trong kinh doanh\n\nPCA được ứng dụng để tìm ra số nhóm tối ưu cho các bài toán phân lớp khách hàng (K-means clustering), hoặc làm giảm số biến đầu vào cho các mô hình dự báo\nEFA thường được sử dụng trong trong lĩnh vực marketing và nghiên cứu thị trường, xây dựng các bộ câu hỏi cho survey để đánh giá nhận thức/ hành vi của khách hàng về sản phẩm/ dịch vụ như: mức độ hài lòng với sản phẩm, sự trung thành với nhãn hiệu, lý do khách hàng mua sản phẩm…Sau đó các biến mới cũng có thể được sử dụng làm biến đầu vào cho các mô hình"
  },
  {
    "objectID": "p04-06-factor-analysis.html#các-bước-thực-hiện",
    "href": "p04-06-factor-analysis.html#các-bước-thực-hiện",
    "title": "40  Phân tích nhân tố ẩn (factor analysis)",
    "section": "40.2 Các bước thực hiện",
    "text": "40.2 Các bước thực hiện\n\n40.2.1 Kiểm định xem tập dữ liệu có đủ điều kiện cho factor analysis không\n\nSố quan sát tối thiểu là 50\nKMO test (0->1) kiểm tra mức độ tương quan của các biến ban đầu. KMO result > 0.5 là chấp nhận được\n\n\n\n\n\n\n40.2.2 Chọn số factor\nCân nhắc các điểm sau:\n\nÝ nghĩa kinh doanh\nEigen value > 1\nScree plot tại điểm thay đổi độ dốc\n\n\n\n40.2.3 Thực hiện factor analysis\nTrong đó chọn 1 trong 2 phương pháp xoay vector sau:\n\nChọn Orthogonal (rotate = “varimax”) khi ta cho rằng các factors không có tương quan với nhau\nChọn Oblique (rotate = “oblimin”) khi ta cho rằng các factors có tương quan với nhau (chủ yếu chọn phương pháp này)\n\n\n\n\n=> Xem loadings (correlation value) xem có hiện tượng 1 biến ban đầu tương quan mạnh với nhiều hơn 1 factor không (high multi-loadings)\n\n\n40.2.4 Đánh giá độ tin cậy và độ chính xác của mô hình\nTa mong đợi các biến đo được phải tương quan với nhau theo từng nhóm, đồng nghĩa chúng đo cùng 1 khái niệm (độ tin cậy, reliability) và phải đo đúng khái niệm ta cần (độ chính xác, validity)\nĐộ tin cậy được đo bằng chỉ số Cronbach’s alpha. () >= 0.3 là pass. Ta có thể cân nhắc loại bỏ 1 vài biến ban đầu không có tương quan với các biến khác, tuy nhiên phải đánh đổi bằng thông tin bị mất đi\nĐộ chính xác được đo theo 2 tiêu chí:\n\nConvergent validity: Chỉ số AVE (Average Variance Extracted) của các factor phải > 0.5 đồng nghĩa với việc các items giải thích được ít nhất 50% độ biến thiên của factor đó\n\n\\[AVE=\\frac{\\sum Factor.loadings^2}{No.of.items}\\]\n\nDiscriminant validity: Chỉ số AVE của từng factor phải > Shared variance giữa các factor, đồng nghĩa với việc các factor phải tương quan mạnh hơn với các items đo chính factor đó chứ không tương quan nhiều với các items đo các factor khác\n\n\\[Shared.variance=Correlation.between.factors^2\\]\nVí dụ:\n\n\n\n\nReliability test: Với cả 2 factor, AVE đều > 0.5 => nhóm các item (x1,x2,x3) và (x4,x5,x6) đều giải thích được trên 50% độ biến thiên của 2 factor tương ứng => good\nValidity test: Xét 3 trường hợp của Shared Variance\n\nShared Variance = 0.49 (< 0.72 và < 0.66) => good\nShared Variance = 0.64 (< 0.72 và < 0.66) => pass\nShared Variance = 0.90 (> 0.72 và > 0.66) => fail\n\n\n\n\n40.2.5 Đặt tên cho các factor\nTùy theo kết quả loadings các nhóm biến để đặt tên cho biến ẩn (khái niệm) sao cho biến ẩn có ý nghĩa tổng hợp nhất\nLưu ý: Các bước 2 -> 4 có thể phải thực hiện nhiều lần để có kết quả ưng ý"
  },
  {
    "objectID": "p04-06-factor-analysis.html#thực-hành-trong-r",
    "href": "p04-06-factor-analysis.html#thực-hành-trong-r",
    "title": "40  Phân tích nhân tố ẩn (factor analysis)",
    "section": "40.3 Thực hành trong R",
    "text": "40.3 Thực hành trong R\nSử dụng tập dữ liệu là kết quả phản hồi survey của 90 KH cho 14 tiêu chí họ ưu tiên khi chọn mua ô tô (bao gồm giá, tính an toàn, tiết kiệm nhiên liệu, dịch vụ hậu mãi…). Survey được thiết kế dưới dạng thang đo Likert với 5 lựa chọn (5-point Likert scale) từ “Very low” (tương ứng với 1) đến “Very high” (tương ứng với 5)\n\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(GPArotation)\n\nload(\"data/efa-df.Rda\")\ndf <- efa.df\nrm(efa.df)\ndf %>% head\n\n  Price Safety Exterior_Looks Space_comfort Technology After_Sales_Service\n1     4      4              5             4          3                   4\n2     3      5              3             3          4                   4\n3     4      4              3             4          5                   5\n4     4      4              4             3          3                   4\n5     5      5              4             4          5                   4\n6     4      4              5             3          4                   5\n  Resale_Value Fuel_Type Fuel_Efficiency Color Maintenance Test_drive\n1            5         4               4     2           4          2\n2            3         4               3     4           3          2\n3            5         4               5     4           5          4\n4            5         5               4     4           4          2\n5            5         3               4     5           5          5\n6            3         4               3     2           3          2\n  Product_reviews Testimonials\n1               4            3\n2               2            2\n3               4            3\n4               5            3\n5               5            2\n6               2            3\n\n\n\nBước 1: Tập dữ liệu có 90 quan sát, đủ lớn cho phân tích. Kiểm tra KMO test = 0.61 chỉ ở mức dưới trung bình nhưng đủ cho phân tích\n\n\nKMO(df)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = df)\nOverall MSA =  0.61\nMSA for each item = \n              Price              Safety      Exterior_Looks       Space_comfort \n               0.72                0.47                0.55                0.61 \n         Technology After_Sales_Service        Resale_Value           Fuel_Type \n               0.65                0.62                0.63                0.68 \n    Fuel_Efficiency               Color         Maintenance          Test_drive \n               0.62                0.56                0.61                0.64 \n    Product_reviews        Testimonials \n               0.69                0.50 \n\n\n\nBước 2: Lựa chọn số factor bằng cách visualise eigen value trong scree plot\n\n\nparallel <- fa.parallel(df, fm = 'minres', fa = 'fa')\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  NA \n\n\n=> Như vậy ta có thể chọn từ 2 hoặc 3 factor được giữ lại\n\nBước 3: Thực hiện factor analysis với 3 factors\n\n\nthree.fa <- fa(df,nfactors = 3, rotate = \"oblimin\", fm=\"minres\")\nprint(three.fa$loadings, cutoff = 0.3) # chỉ hiển thị các giá trị loading > 0.3\n\n\nLoadings:\n                    MR1    MR2    MR3   \nPrice                0.444              \nSafety                      0.311       \nExterior_Looks                          \nSpace_comfort               0.832       \nTechnology                  0.342       \nAfter_Sales_Service         0.460       \nResale_Value         0.599              \nFuel_Type                   0.573       \nFuel_Efficiency      0.655              \nColor                0.464              \nMaintenance          0.668              \nTest_drive                         0.328\nProduct_reviews      0.424              \nTestimonials                       0.742\n\n                 MR1   MR2   MR3\nSS loadings    2.015 1.605 0.972\nProportion Var 0.144 0.115 0.069\nCumulative Var 0.144 0.259 0.328\n\n\nTừ kết quả loadings của 3 factors mới, ta có thể thấy 1 số biến có tương quan cùng lúc với cả 2 factor mới (double loading)\nThực hiện lại factor analysis với 4 factors => Ta thấy không còn hiện tượng double loading nữa. Như vậy ta sử dụng model với 4 factor\n\nfour.fa <- fa(df,nfactors = 4, rotate = \"oblimin\", fm=\"minres\")\nprint(four.fa$loadings, cutoff = 0.3) # chỉ hiện thị các giá trị loading > 0.3\n\n\nLoadings:\n                    MR1    MR2    MR4    MR3   \nPrice                0.544                     \nSafety              -0.331  0.358              \nExterior_Looks                    -0.548       \nSpace_comfort               0.782              \nTechnology                  0.358              \nAfter_Sales_Service         0.537              \nResale_Value         0.729                     \nFuel_Type                   0.575              \nFuel_Efficiency      0.434         0.308       \nColor                              0.731       \nMaintenance          0.562                     \nTest_drive                                0.365\nProduct_reviews      0.345                0.364\nTestimonials                              0.685\n\n                 MR1   MR2   MR4   MR3\nSS loadings    1.639 1.637 1.053 0.968\nProportion Var 0.117 0.117 0.075 0.069\nCumulative Var 0.117 0.234 0.309 0.378\n\n\n\nBước 4: Đánh giá độ tin cậy và chính xác của mô hình\n\n() = 0.61 cho thấy mô hình đáng tin cậy. Xem thêm bảng Reliability if an item is dropped ta thấy nếu loại bỏ các biến Safety, Exterior_Looks, Testimonials có thể cải thiện () tuy nhiên rất ít => Ta chọn giữ lại tất cả các biến này để không bị mất thông tin\n\nalpha(df,check.keys=TRUE)\n\n\nReliability analysis   \nCall: alpha(x = df, check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.61      0.62    0.71       0.1 1.6 0.059  3.7 0.34    0.091\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.48  0.61  0.72\nDuhachek  0.49  0.61  0.73\n\n Reliability if an item is dropped:\n                    raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r\nPrice                    0.59      0.59    0.69     0.101 1.5    0.063 0.027\nSafety-                  0.62      0.64    0.72     0.120 1.8    0.059 0.026\nExterior_Looks-          0.63      0.65    0.72     0.123 1.8    0.056 0.025\nSpace_comfort            0.60      0.61    0.68     0.106 1.5    0.061 0.023\nTechnology               0.60      0.61    0.70     0.107 1.6    0.060 0.028\nAfter_Sales_Service      0.58      0.59    0.68     0.099 1.4    0.064 0.026\nResale_Value             0.61      0.61    0.69     0.107 1.6    0.059 0.024\nFuel_Type                0.60      0.61    0.70     0.108 1.6    0.060 0.026\nFuel_Efficiency          0.53      0.54    0.64     0.084 1.2    0.072 0.025\nColor                    0.57      0.59    0.68     0.099 1.4    0.065 0.027\nMaintenance              0.55      0.56    0.65     0.090 1.3    0.069 0.026\nTest_drive               0.60      0.61    0.70     0.106 1.5    0.061 0.028\nProduct_reviews          0.56      0.57    0.67     0.094 1.3    0.067 0.028\nTestimonials             0.62      0.63    0.71     0.116 1.7    0.058 0.026\n                    med.r\nPrice               0.095\nSafety-             0.112\nExterior_Looks-     0.115\nSpace_comfort       0.087\nTechnology          0.104\nAfter_Sales_Service 0.087\nResale_Value        0.095\nFuel_Type           0.101\nFuel_Efficiency     0.079\nColor               0.094\nMaintenance         0.082\nTest_drive          0.095\nProduct_reviews     0.082\nTestimonials        0.107\n\n Item statistics \n                     n raw.r std.r r.cor  r.drop mean   sd\nPrice               90  0.41  0.45 0.382  0.3032  4.2 0.59\nSafety-             90  0.19  0.20 0.073  0.0556  1.8 0.64\nExterior_Looks-     90  0.16  0.16 0.022 -0.0069  2.2 0.82\nSpace_comfort       90  0.32  0.38 0.342  0.1872  4.0 0.65\nTechnology          90  0.35  0.37 0.269  0.1735  4.2 0.87\nAfter_Sales_Service 90  0.45  0.48 0.433  0.3265  4.4 0.65\nResale_Value        90  0.44  0.38 0.326  0.1971  3.7 1.22\nFuel_Type           90  0.30  0.36 0.280  0.1643  4.1 0.65\nFuel_Efficiency     90  0.69  0.69 0.710  0.5863  3.9 0.82\nColor               90  0.51  0.48 0.437  0.3428  3.7 0.92\nMaintenance         90  0.62  0.61 0.603  0.5001  4.0 0.77\nTest_drive          90  0.44  0.39 0.302  0.2283  3.4 1.11\nProduct_reviews     90  0.56  0.55 0.511  0.4073  4.1 0.89\nTestimonials        90  0.26  0.25 0.149  0.0839  3.9 0.87\n\nNon missing response frequency for each item\n                       1    2    3    4    5 miss\nPrice               0.00 0.00 0.10 0.63 0.27    0\nSafety              0.00 0.00 0.12 0.56 0.32    0\nExterior_Looks      0.01 0.02 0.30 0.47 0.20    0\nSpace_comfort       0.00 0.01 0.18 0.61 0.20    0\nTechnology          0.01 0.03 0.14 0.41 0.40    0\nAfter_Sales_Service 0.00 0.01 0.06 0.44 0.49    0\nResale_Value        0.06 0.11 0.28 0.21 0.34    0\nFuel_Type           0.00 0.00 0.17 0.58 0.26    0\nFuel_Efficiency     0.01 0.03 0.24 0.50 0.21    0\nColor               0.02 0.08 0.22 0.50 0.18    0\nMaintenance         0.00 0.01 0.27 0.44 0.28    0\nTest_drive          0.02 0.23 0.22 0.33 0.19    0\nProduct_reviews     0.00 0.08 0.10 0.43 0.39    0\nTestimonials        0.01 0.04 0.24 0.46 0.24    0\n\n\nCó thể tính toán thêm AVE và Shared Variance cho validity test\n\nBước 5: Đặt tên cho các biến ẩn"
  },
  {
    "objectID": "p04-06-factor-analysis.html#tài-liệu-tham-khảo",
    "href": "p04-06-factor-analysis.html#tài-liệu-tham-khảo",
    "title": "40  Phân tích nhân tố ẩn (factor analysis)",
    "section": "40.4 Tài liệu tham khảo",
    "text": "40.4 Tài liệu tham khảo\n\nhttp://www.theanalysisfactor.com/the-fundamental-difference-between-principal-component-analysis-and-factor-analysis/\nhttps://www.promptcloud.com/blog/exploratory-factor-analysis-in-r/\nhttp://minato.sip21c.org/swtips/factor-in-R.pdf"
  },
  {
    "objectID": "p05-01-chuoi-thoi-gian.html#thành-phần-của-chuỗi-thời-gian",
    "href": "p05-01-chuoi-thoi-gian.html#thành-phần-của-chuỗi-thời-gian",
    "title": "41  Giới thiệu về chuỗi thời gian",
    "section": "41.1 Thành phần của chuỗi thời gian",
    "text": "41.1 Thành phần của chuỗi thời gian\nTrong bất cứ chuỗi thời gian nào, cũng có 3 thành phần sau.\n\nXu hướng (trend) thể hiện chiều hướng tăng hay giảm dài hạn của chuỗi thời gian\nMùa vụ (seasonal) thể hiện sự biến đổi của chuỗi thời gian theo chu kỳ biết trước. Ví dụ, vào cuối tuần, khách hàng có xu hướng đi ăn nhà hàng nhiều hơn.\nChu kỳ kinh doanh (cyclic) thể hiện xu hướng biến đổi dài hạn của chuỗi thời gian, thường ít nhất hai năm. Chu kỳ kinh doanh khác với yếu tố mùa vụ ở chỗ, chu kỳ biến đổi của yếu tố mùa vụ thường là đã được biết trước và mang tính ngắn hạn. Do đó, yếu tố chu kỳ kinh doanh chỉ được đưa vào phân tích khi dữ liệu phân tích đủ lớn.\n\nKhi phân tích chuỗi thời gian tập trung vào 2 câu hỏi:\n\nCái gì đã xảy ra\nCái gì sẽ xảy ra, với độ tin cậy bao nhiêu phần trăm\n\nPhân tích chuỗi thời gian sử dụng rất nhiều trong việc dự báo tăng trưởng, lên kế hoạch. Mô hình này dùng rất nhiều trong Macroeconomics, Finance…\nLưu ý:\n\nĐể phân tích chuỗi thời gian, R lưu object dưới dạng ts (time series)\nChuỗi thời gian có 1 loạt các kỹ thuật và câu lệnh phân tích riêng biệt\n\nKhi lưu trữ trong R, chuỗi thời gian được lưu dưới dạng ts như sau.\n\n#Tạo chuỗi thời gian\nsales <- c(round(rnorm(48,10,2),0))\n#Tạo object ts\ntsales <- ts(sales, start = c(2003,1), frequency = 12)\ntsales\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2003  10  10  10  10  12   8  10  11   9  11   8   6\n2004   8   9   9  10   9   6   9  14  12  11  11  10\n2005   8  14  14   9   8  14  12   9  11   9  10  12\n2006  12  11  10  12  12   8  12  11   7  10  10   9\n\n\nHàm ts quy định dữ liệu tối thiểu là ngày và có các frequency được quy định như sau.\n\n\n\nData\nFrequency\n\n\n\n\nAnnual\n1\n\n\nQuarterly\n4\n\n\nMonthly\n12\n\n\nWeekly\n52\n\n\n\nĐể hiển thị biểu đồ với time series theo phong cách của ggplot2, ta có thể dùng package ggfortify.\n\nlibrary(tidyverse)\nlibrary(ggfortify)\n\ntsales %>% autoplot() + \n  theme_minimal()"
  },
  {
    "objectID": "p05-01-chuoi-thoi-gian.html#smoothing",
    "href": "p05-01-chuoi-thoi-gian.html#smoothing",
    "title": "41  Giới thiệu về chuỗi thời gian",
    "section": "41.2 Smoothing",
    "text": "41.2 Smoothing\nCentered Moving Average là kỹ thuật làm trơn để giảm đi các yếu tố gây nhiễu trong chuỗi thời gian.\n\\[S_t=\\frac{Y_{t-q}+...+Y_t+...+Y_{t+q}}{2q+1}\\]\n\\(S_t\\) được gọi là giá trị làm trơn (smoothed value) tại thời điểm t, \\(k=2q+1\\) là số quan sát được lấy giá trị trung bình. k thường là giá trị lẻ.\nMô hình trên được gọi là Moving Average (MA)\n\nlibrary(forecast)\nlibrary(ggfortify)\nlibrary(patchwork)\nNile\n\nTime Series:\nStart = 1871 \nEnd = 1970 \nFrequency = 1 \n  [1] 1120 1160  963 1210 1160 1160  813 1230 1370 1140  995  935 1110  994 1020\n [16]  960 1180  799  958 1140 1100 1210 1150 1250 1260 1220 1030 1100  774  840\n [31]  874  694  940  833  701  916  692 1020 1050  969  831  726  456  824  702\n [46] 1120 1100  832  764  821  768  845  864  862  698  845  744  796 1040  759\n [61]  781  865  845  944  984  897  822 1010  771  676  649  846  812  742  801\n [76] 1040  860  874  848  890  744  749  838 1050  918  986  797  923  975  815\n [91] 1020  906  901 1170  912  746  919  718  714  740\n\np1 <- Nile %>% autoplot() + labs(title = \"Original plot\")\np2 <- ma(Nile, 3) %>% autoplot() + labs(title = \"Moving Average with k=3\")\np3 <- ma(Nile, 7) %>% autoplot() + labs(title = \"Moving Average with k=9\")\np4 <- ma(Nile, 15) %>% autoplot() + labs(title = \"Moving Average with k=15\")\np1 + p2 + p3 + p4\n\n\n\n\nLưu ý: Các đường smoothing có thể xây dựng trên cùng 1 biểu đồ và sử dụng package forecast và ggfortify như sau.\n\nNile %>% \n  autoplot(series = \"Original\") +\n  autolayer(ma(Nile, 12), series = \"12-MA\", size = 1) +\n  theme_minimal() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "p05-01-chuoi-thoi-gian.html#phân-rã-các-thành-phần-chuỗi-thời-gian",
    "href": "p05-01-chuoi-thoi-gian.html#phân-rã-các-thành-phần-chuỗi-thời-gian",
    "title": "41  Giới thiệu về chuỗi thời gian",
    "section": "41.3 Phân rã các thành phần chuỗi thời gian",
    "text": "41.3 Phân rã các thành phần chuỗi thời gian\nBất kỳ mô hình times series nào cũng được cấu thành bởi ba yếu tố\n\nTrend: Xu hướng phát triển theo thời gian\nSeasonal: Yếu tố mùa vụ\nIrregular component (error): Các yếu tố gây nhiễu\n\nMô hình chuỗi thời gian cơ bản có hai dạng:\n\nMô hình cộng (Additive) - chuỗi thời gian do tổng các thành phần cấu thành tạo nên.\n\n\\[Y_t=Trend_t + Seasonal_t + Irregular_t\\]\n\nMô hình nhân (Multiplicative) - chuỗi thời gian do tích các thành phần cấu tạo nên.\n\n\\[Y_t=Trend_t * Seasonal_t * Irregular_t\\]\nMô hình trên tương đương với.\n\\[log(Y_t) = log(Trend_t) + log(Seasonal_t) + log(Irregular_t)\\]\nDo đó, để phân tích chuỗi thời gian, ta cũng cần bóc tách chuỗi thời gian thành 3 yếu tố như trên bằng hàm stl (Seasonal Decomposition of Time Series by Loess). Cấu trúc của câu lệnh như sau.\n\nstl(ts, s.window = , t.window =)\n\nLưu ý:\n\nHàm trên chỉ dùng được cho Additive model\nĐối với Multiplicative, ta cần biến đổi như sau;\n\n\\[log(Y_t) = log(Trend_t * Seasonal_t * Irregular_t)=log(Trend_t)+log(Seasonal_t)+log(Irregular_t)\\]\n\ns.window: kiểm soát tốc độ thay đổi của yếu tố thời vụ (seasonal) ảnh hưởng đên chuối thời gian thế nào. Nếu \\(s.window=\"period\"\\) thì yếu tố thời vụ sẽ giống hệt nhau qua các năm.\nt.window: kiểm soát tốc độ của yếu tố xu hướng (trend) thay đổi theo thời gian thế nào\n\n\np1 <- AirPassengers %>% autoplot + theme_minimal() +\n  labs(title = \"Original data\")\n\n#Chuyển sang dạng Additive\nlAirPassengers <- log(AirPassengers)\n\np2 <- lAirPassengers %>% autoplot + theme_minimal() +\n  labs(title = \"Log data\")\n\np1 + p2 + plot_layout(ncol = 1)\n\n\n\n#Phân tích thành phần ts\nfit <- stl(lAirPassengers, s.window = \"period\")\nautoplot(fit) + theme_bw() + labs(title = \"Result of decomposition\")\n\n\n\n#Quay lại dữ liệu gốc\nfit$time.series %>% head(7)\n\n            seasonal    trend   remainder\nJan 1949 -0.09164042 4.829389 -0.01924936\nFeb 1949 -0.11402828 4.830368  0.05434477\nMar 1949  0.01586585 4.831348  0.03558845\nApr 1949 -0.01402759 4.833377  0.04046325\nMay 1949 -0.01502478 4.835406 -0.02459053\nJun 1949  0.10978976 4.838166 -0.04268143\nJul 1949  0.21640041 4.840927 -0.06011517\n\n\nLưu ý: Nhìn vào bảng, ta có thể có các nhận định sau:\n\nTháng một, yếu tố thời vụ khiến khách hàng giảm 9% (seasonal=-0.091)\nTháng bảy, yếu tố thời vụ khiến khách hàng tăng 21% (seasonal = 0.216)\n\n\nfit$time.series %>% exp %>% head(7)\n\n          seasonal    trend remainder\nJan 1949 0.9124332 125.1344 0.9809347\nFeb 1949 0.8922327 125.2571 1.0558486\nMar 1949 1.0159924 125.3798 1.0362293\nApr 1949 0.9860703 125.6345 1.0412930\nMay 1949 0.9850875 125.8897 0.9757094\nJun 1949 1.1160434 126.2377 0.9582166\nJul 1949 1.2415994 126.5866 0.9416561\n\n#Phân tích xu hướng theo tháng và seasonal\nmonthplot(AirPassengers)\n\n\n\nseasonplot(AirPassengers, col = \"blue\", type = \"o\")"
  },
  {
    "objectID": "p05-01-chuoi-thoi-gian.html#exponential-forecasting-model",
    "href": "p05-01-chuoi-thoi-gian.html#exponential-forecasting-model",
    "title": "41  Giới thiệu về chuỗi thời gian",
    "section": "41.4 Exponential Forecasting Model",
    "text": "41.4 Exponential Forecasting Model\nExponential model là nhóm mô hình đơn giản nhưng có thể đưa ra kết quả dự báo khá chính xác chuỗi thời gian trong ngắn hạn. Nhóm mô hình này có 3 dạng:\n\nSingle Exponential Model (Simple Exponential Model): Áp dụng với chuỗi thời gian không có trend rõ ràng\nDouble Exponential Model (Hold Exponential Model): Áp dụng với chuỗi có xu hướng nhưng không có yếu tố mùa vụ rõ ràng\nTriple Exponential Model (Hold-Winters Exponential Model): Áp dụng với chuỗi có cả xu hướng và mùa vụ\n\n\n41.4.1 Single Exponential Model\nMô hình này được áp dụng khi không có xu hướng rõ ràng trong chuỗi thời gian. Quan sát tại thời điểm \\(t\\) được xác định dựa trên ảnh hưởng của các quan sát trước đó. Ví dụ chuỗi thời gian loại này như sau.\n\nnhtemp %>% \n  autoplot() +\n  theme_minimal()\n\n\n\n\n\\[Y_t=level + irregular_t\\]\nDự báo giá trị của \\(Y_{t+1}\\) (1-step ahead forecast) là:\n\\[Y_{t+1}=c_0Y_t + c_1Y_{t-1}+c_2Y_{t-2}+...\\]\nvới \\[c_i=\\alpha(1-\\alpha)^i\\], \\[\\sum c_i=1\\], \\[i=0,1,2...\\], \\[\\alpha\\in[0,1]\\]\nđo lường độ ảnh hưởng của các biến trước đó đến giá trị dự báo \\(Y_{t+1}\\). Do đó, \\(\\alpha\\) có đặc điểm sau:\n\ncàng gần 1, các biến càng gần có trọng số càng lớn\ncàng gần 0, các biến càng xa có trọng số càng lớn\n\n\nlibrary(forecast)\nfit <- ets(y = nhtemp, model = \"ANN\")\nfit\n\nETS(A,N,N) \n\nCall:\n ets(y = nhtemp, model = \"ANN\") \n\n  Smoothing parameters:\n    alpha = 0.1819 \n\n  Initial states:\n    l = 50.2762 \n\n  sigma:  1.1455\n\n     AIC     AICc      BIC \n265.9298 266.3584 272.2129 \n\n# Dự báo 5 quan sát tiếp theo\nforecast(fit,5)\n\n     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n1972       51.87031 50.40226 53.33835 49.62512 54.11549\n1973       51.87031 50.37817 53.36244 49.58828 54.15233\n1974       51.87031 50.35447 53.38614 49.55203 54.18858\n1975       51.87031 50.33113 53.40948 49.51634 54.22428\n1976       51.87031 50.30814 53.43247 49.48117 54.25944\n\naccuracy(fit)\n\n                    ME     RMSE       MAE       MPE   MAPE      MASE\nTraining set 0.1460657 1.126268 0.8951225 0.2419373 1.7489 0.7512408\n                     ACF1\nTraining set -0.006441923\n\n# \nautoplot(forecast(fit, 5), xlab=\"Year\", \n    ylab=expression(paste(\"Temperature (\", degree*F,\")\",)), \n    main=\"New Haven Annual Mean Temperature\") +\n  theme_minimal()\n\n\n\n\nGiải thích:\n\n\\(\\alpha=0.182\\) cho thấy rất nhiều quan sát được tính đến trong mô hình.\nĐộ chính xác của mô hình được tính bằng các chỉ số thông dụng sau\n\n\n\n\n\n\n\n\n\nChỉ số\nViết tắt\n      Cách tính\n\n\n\n\nMean error\nME\n\\(mean(\\epsilon_t)\\)\n\n\nRoot mean squared error\nRMSE\n\\(sqrt(mean(\\epsilon_t^2))\\)\n\n\nMean absolute error\nMAE\n\\(mean(\\epsilon_t)\\)\n\n\nMean percentage error\nMPE\n\\(mean(100*\\epsilon_t/Y_t)\\)\n\n\nMean absolute percentage error\nMAPE\n\\(mean( 100*\\epsilon_t/Y_t)\\)\n\n\n\n\n\n41.4.2 Hold & Holt-Winters exponential smoothing\nTương tự như simple exponential smoothing, Hold và Hold-Winters tính đến các yếu tố xu hướng và mùa vụ tại thời điểm hiện tại. Các yếu tố này bị ảnh hưởng bởi các yếu tố xu hướng và mùa vụ tương ứng trong quá khứ.\nHold exponential smoothing\n\\[Y_t=level + slope*t + irregular_t\\]\n\n\\(\\alpha\\): Độ ảnh hưởng của các biến y trong quá khứ đến biến hiện tại\n\\(\\beta\\): Độ ảnh hưởng của biến xu hướng trong quá khứ đến hiện tại\n\nHold-Winters exponential smoothing\n\\[Y_t=level + slope*t + s_t + irregular_t\\]\n\n\\(\\alpha\\): Độ ảnh hưởng của các biến \\(y\\) trong quá khứ đến biến hiện tại\n\\(\\beta\\): Độ ảnh hưởng của biến xu hướng trong quá khứ đến hiện tại\n\\(\\gamma\\): Độ ảnh hường của biến chu kỳ đến hiện tại\n\nLưu ý: Các ảnh hưởng của slope và chu kỳ (và ) đến giá trị hiện tại được tính theo công thức giống như alpha\n\nlibrary(forecast)\nlibrary(dplyr)\nfit <- ets(log(AirPassengers), model = \"AAA\")\nfit\n\nETS(A,A,A) \n\nCall:\n ets(y = log(AirPassengers), model = \"AAA\") \n\n  Smoothing parameters:\n    alpha = 0.6975 \n    beta  = 0.0031 \n    gamma = 1e-04 \n\n  Initial states:\n    l = 4.7925 \n    b = 0.0111 \n    s = -0.1045 -0.2206 -0.0787 0.0562 0.2049 0.2149\n           0.1146 -0.0081 -0.0059 0.0225 -0.1113 -0.0841\n\n  sigma:  0.0383\n\n      AIC      AICc       BIC \n-207.1694 -202.3123 -156.6826 \n\n#Dự báo\npred <- forecast(fit, 12)\nautoplot(pred, main=\"Forecast for Air Travel\", \n    ylab=\"Log(AirPassengers)\", xlab=\"Time\") +\n  theme_minimal()\n\n\n\n\n\n\n41.4.3 Tự động xây dựng mô hình\nHàm ets cho phép tự động lựa chọn mô hình tốt nhất theo phương pháp maximum likelihood.\n\nlibrary(forecast)\nlibrary(dplyr)\n(fit <- ets(AirPassengers)) \n\nETS(M,Ad,M) \n\nCall:\n ets(y = AirPassengers) \n\n  Smoothing parameters:\n    alpha = 0.7096 \n    beta  = 0.0204 \n    gamma = 1e-04 \n    phi   = 0.98 \n\n  Initial states:\n    l = 120.9939 \n    b = 1.7705 \n    s = 0.8944 0.7993 0.9217 1.0592 1.2203 1.2318\n           1.1105 0.9786 0.9804 1.011 0.8869 0.9059\n\n  sigma:  0.0392\n\n     AIC     AICc      BIC \n1395.166 1400.638 1448.623 \n\nforecast(fit) %>% autoplot + theme_minimal()"
  },
  {
    "objectID": "p05-01-chuoi-thoi-gian.html#tài-liệu-tham-khảo",
    "href": "p05-01-chuoi-thoi-gian.html#tài-liệu-tham-khảo",
    "title": "41  Giới thiệu về chuỗi thời gian",
    "section": "41.5 Tài liệu tham khảo",
    "text": "41.5 Tài liệu tham khảo\n\nForecasting: Principles & Methods\nR in Action - Chapter 15 - Time Series"
  },
  {
    "objectID": "p05-02-arima.html#giới-thiệu",
    "href": "p05-02-arima.html#giới-thiệu",
    "title": "42  Mô hình ARIMA",
    "section": "42.1 Giới thiệu",
    "text": "42.1 Giới thiệu\nARIMA là viết tắt của Auto-Regressive Integrated Moving Average\n\\[ARIMA(p,d,q) = AR(p) + I(d) + MA(q)\\]\nTrong đó:\n\np: Số bậc trong mô hình Auto-Regressive\nd: Số bậc trong mô hình Itegrated (số lần lấy \\(\\delta\\) để có chuỗi thời gian dừng)\nq: Số bậc trong mô hình Moving Agerage\n\nMô hình Auto-Regressive: Giá trị dự báo tại thời điểm hiện tại có quan hệ tuyến tính với \\(p\\) quan sát gần nhất trong quá khứ.\n\\[AR(p): Y_t = \\mu + \\beta_1Y_{t-1} + ... + \\beta_pY_{t-p} + \\epsilon_t\\]\nMô hình Moving-Average: Giá trị dự báo của quan sát tại thời điểm \\(t\\) có quan hệ tuyến tính với \\(q\\) sai số gần nhất trong quá khứ\n\\[MA(q):Y_t = \\mu + \\theta_1\\epsilon_{t-1} + ... + \\theta_p\\epsilon_{t-p} + \\epsilon_t\\] Integrated model\nGọi \\(Y_1,...,Y_t\\) là chuỗi thời gian gốc. Ta có:\n\n\\(d=0\\): \\(y_t=Y_t\\)\n\\(d=1\\): \\(y_t=Y_t-Y_{t-1}\\)\n\\(d=2\\): \\(y_t=(Y_t-Y_{t-1})-(Y_{t-1}-Y_{t-2})\\)\n\nVới d=2, còn được gọi là “the first difference of the first difference”. Sau khi được chuỗi dừng \\(y_t\\), ta có thể dự báo mô hình ARIMA như sau:\n\\[\\hat{y}_t=\\mu + \\phi_1y_{t-1}+...+\\phi_py_{t-p} + \\theta_1\\epsilon_{t-1}+...+\\theta_q\\epsilon_{t-q}\\]\nChuỗi thời gian dừng: Khi xây dựng mô hình ARIMA, ta cần phải tạo được chuỗi thời gian dừng. Chuỗi thời gian dừng là chuỗi như sau.\n\nGiá trị trung bình không đổi theo thời gian\nPhương sai không đổi theo thời gian (homoskedaticity)\n\n[Images/arima-01.png]\nKỹ thuật tạo chuỗi dừng:\n\nVới biến có var biến đổi: log\nVới biến có mean biến đổi: Sử dụng “Difference”\n\nTest chuỗi dừng: Test ADF (Augmented Dickey-Fuller)\n\n\\(H_0\\): Chuỗi không dừng (non-stationary)\n\\(H_1\\): Chuỗi dừng (stationary)\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(ggfortify)\n\ntheme_set(theme_minimal())\nNile %>% autoplot + labs(title = \"Non stationary\")\n\n\n\n# Chuỗi không dừng\nadf.test(Nile)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  Nile\nDickey-Fuller = -3.3657, Lag order = 4, p-value = 0.0642\nalternative hypothesis: stationary\n\n# Chuỗi dừng\ndiff(Nile)  %>% autoplot + theme_minimal() + labs(title = \"Stationary\")\n\n\n\ndiff(Nile) %>% adf.test()\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  .\nDickey-Fuller = -6.5924, Lag order = 4, p-value = 0.01\nalternative hypothesis: stationary"
  },
  {
    "objectID": "p05-02-arima.html#acf-pacf",
    "href": "p05-02-arima.html#acf-pacf",
    "title": "42  Mô hình ARIMA",
    "section": "42.2 ACF & PACF",
    "text": "42.2 ACF & PACF\nNhư vây, trong mô hình ARIMA ta phải xác định 3 yếu tố \\((p, d, q)\\). Để xác định các tham số này, ta có thể sử dụng đồ thị ACF & PACF.\n\nACF: Autocorrelation - đo correlation giữa các quan sát trong chuỗi\nPACF: Partial Auto Correlation - đo correlation giữa biến \\(Y_t\\) và \\(Y_{t-k}\\), loại bỏ các biến ở giữa chúng\n\nLựa chọn tham số trong ARIMA\n\n\n\n\n\n\n\n\nModel\n              ACF\n   PACF\n\n\n\n\nARIMA(p,d,0)\nGiảm dần đều về 0\nGiảm về 0 sau lag p\n\n\nARIMA(0,d,q)\nGiảm về 0 sau lag q Giảm dần đều về 0\n\n\n\nARIMA(p,d,q)\nGiảm dần đều về 0\nGiảm dần đều về 0\n\n\n\nXem xét ACF & PACF trong các chuỗi sau\n\nar1 <- arima.sim(list(ar=c(0.89)), n = 100)\nar2 <- arima.sim(list(ar=c(0.89, -0.4858)), n = 100)\nma1 <- arima.sim(n = 100, list(ma = c(-0.2279)))\nma2 <- arima.sim(n = 100, list(ma = c(-0.2279, 0.2488)))\narma <- arima.sim(n = 100, list(ar = c(0.8897, -0.4858), \n                                ma = c(-0.2279, 0.2488)))\n#Tạo function\ntseries.plot <- function(x, title = \"Title\") {\n  p1 <- autoplot(x) + theme_minimal() +\n    labs(title = title, y = \"Data\")\n  p2 <- ggPacf(x) + theme_minimal() +\n    labs(title = \"\")\n  p3 <- ggAcf(x) + theme_minimal() +\n    labs(title = \"\")\n  p <- p1 + (p2/p3)\n  return(p)\n}\n\ntseries.plot(ar1, \"AR1\")\n\n\n\ntseries.plot(ar2, \"AR2\")\n\n\n\ntseries.plot(ma1, \"ma1\")\n\n\n\ntseries.plot(ma2, \"ma2\")\n\n\n\ntseries.plot(arma, \"ARMA\")"
  },
  {
    "objectID": "p05-02-arima.html#ví-dụ-với-r",
    "href": "p05-02-arima.html#ví-dụ-với-r",
    "title": "42  Mô hình ARIMA",
    "section": "42.3 Ví dụ với R",
    "text": "42.3 Ví dụ với R\n\nlibrary(forecast)\nlibrary(tseries)\nNile %>% autoplot()\n\n\n\n#Tìm giá trị tối ưu của d để loại trend\nndiffs(Nile)\n\n[1] 1\n\ndNile <- diff(Nile, 1)\ndNile %>%  autoplot\n\n\n\n# Kiểm tra tính dừng\nadf.test(dNile)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  dNile\nDickey-Fuller = -6.5924, Lag order = 4, p-value = 0.01\nalternative hypothesis: stationary\n\n#Xem mô hình\np1 <- dNile %>% ggAcf() + labs(title = \"\")\np2 <- dNile %>% ggPacf() + labs(title = \"\")\np1 + p2\n\n\n\n#ACF và PACF đưa ra gợi ý mô hình ARIMA(0,1,1): ACF giảm về 0 sau lag 1, PACF giảm dần về 0\nfit <- arima(Nile, order = c(0,1,1))\nfit\n\n\nCall:\narima(x = Nile, order = c(0, 1, 1))\n\nCoefficients:\n          ma1\n      -0.7329\ns.e.   0.1143\n\nsigma^2 estimated as 20600:  log likelihood = -632.55,  aic = 1269.09\n\naccuracy(fit)\n\n                   ME     RMSE      MAE       MPE     MAPE     MASE      ACF1\nTraining set -11.9358 142.8071 112.1752 -3.574702 12.93594 0.841824 0.1153593\n\n#Đánh giá mô hình\npar(mfrow=c(1,1))\nnames(fit)\n\n [1] \"coef\"      \"sigma2\"    \"var.coef\"  \"mask\"      \"loglik\"    \"aic\"      \n [7] \"arma\"      \"residuals\" \"call\"      \"series\"    \"code\"      \"n.cond\"   \n[13] \"nobs\"      \"model\"    \n\nqqnorm(fit$residuals)\nqqline(fit$residuals)\n\n\n\nBox.test(fit$residuals, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  fit$residuals\nX-squared = 1.3711, df = 1, p-value = 0.2416\n\n#H0: Autocorrelation của residual bằng 0\n#H1: Autocorrelation của residual khác 0\n\n#Dự báo\nforecast(fit, 5)\n\n     Point Forecast    Lo 80     Hi 80    Lo 95    Hi 95\n1971       798.3673 614.4307  982.3040 517.0605 1079.674\n1972       798.3673 607.9845  988.7502 507.2019 1089.533\n1973       798.3673 601.7495  994.9851 497.6663 1099.068\n1974       798.3673 595.7063 1001.0284 488.4240 1108.311\n1975       798.3673 589.8381 1006.8965 479.4494 1117.285\n\nautoplot(forecast(fit,5))\n\n\n\n\nMô hình trên cho thấy ARIMA(0,1,1) phản ánh tốt số lượng:\n\nResidual có phân phối chuẩn\nResidual có autocorrelation bằng 0 (p value > 0.24)\n\nLưu ý: Ta có thể sử dụng auto.arima có sẵn trong package forecast\n\nauto_model <- auto.arima(Nile)\nauto_model\n\nSeries: Nile \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.2544  -0.8741\ns.e.  0.1194   0.0605\n\nsigma^2 = 20177:  log likelihood = -630.63\nAIC=1267.25   AICc=1267.51   BIC=1275.04\n\nforecast(auto_model, 12) %>% autoplot()"
  },
  {
    "objectID": "p05-02-arima.html#tài-liệu-tham-khảo",
    "href": "p05-02-arima.html#tài-liệu-tham-khảo",
    "title": "42  Mô hình ARIMA",
    "section": "42.4 Tài liệu tham khảo",
    "text": "42.4 Tài liệu tham khảo\n\nR in Action - Chapter 15 - Time Series\nhttps://otexts.com/fpp2/non-seasonal-arima.html"
  },
  {
    "objectID": "p05-03-phan-tich-chuoi-thoi-gian-voi-tidyquant.html#lấy-dữ-liệu-với-package-tidyquant",
    "href": "p05-03-phan-tich-chuoi-thoi-gian-voi-tidyquant.html#lấy-dữ-liệu-với-package-tidyquant",
    "title": "43  Phân tích chuỗi thời gian với tidyquant và timetk",
    "section": "43.1 Lấy dữ liệu với package tidyquant",
    "text": "43.1 Lấy dữ liệu với package tidyquant\nKhi sử dụng tidyquant, ta có thể lấy nhiều định dạng dữ liệu từ sản chứng khoán, thông dụng nhất bao gồm:\n\nThông tin công ty trên sàn chứng khoán\nGiá chứng khoán\nBáo cáo tài chính\nCổ tức\n\n\nlibrary(tidyquant)\nlibrary(timetk)\nlibrary(tidyverse)\n# Kiểm tra các option lấy dữ liệu\ntq_get_options()\n\n [1] \"stock.prices\"       \"stock.prices.japan\" \"dividends\"         \n [4] \"splits\"             \"economic.data\"      \"quandl\"            \n [7] \"quandl.datatable\"   \"tiingo\"             \"tiingo.iex\"        \n[10] \"tiingo.crypto\"      \"alphavantager\"      \"alphavantage\"      \n[13] \"rblpapi\"           \n\n\n\n# Lấy dữ liệu các công ty trên sàn chứng khoán\ntq_index(\"SP500\")\n\nLấy dữ liệu chỉ số chứng khoán\n\n# Lấy dữ liệu stock \naapl_price <- tq_get(\"AAPL\", \n       get = \"stock.prices\",\n       from = \"2010-01-01\")\n\n\naapl_price %>% head\n\n# A tibble: 6 × 7\n  date        open  high   low close    volume adjusted\n  <date>     <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>\n1 2010-01-04  30.5  30.6  30.3  30.6 123432400     20.3\n2 2010-01-05  30.7  30.8  30.5  30.6 150476200     20.3\n3 2010-01-06  30.6  30.7  30.1  30.1 138040000     20.0\n4 2010-01-07  30.2  30.3  29.9  30.1 119282800     20.0\n5 2010-01-08  30.0  30.3  29.9  30.3 111902700     20.1\n6 2010-01-11  30.4  30.4  29.8  30.0 115557400     19.9"
  },
  {
    "objectID": "p05-03-phan-tich-chuoi-thoi-gian-voi-tidyquant.html#trực-quan-hóa-với-geom_ma",
    "href": "p05-03-phan-tich-chuoi-thoi-gian-voi-tidyquant.html#trực-quan-hóa-với-geom_ma",
    "title": "43  Phân tích chuỗi thời gian với tidyquant và timetk",
    "section": "43.2 Trực quan hóa với geom_ma",
    "text": "43.2 Trực quan hóa với geom_ma\n\naapl_price %>% \n  select(date, close) %>% \n  ggplot(aes(date, close)) +\n  geom_line(col = \"grey\") +\n  geom_ma(ma_fun = SMA, n = 30, \n          color = \"blue\", size = 1) +\n  geom_ma(ma_fun = SMA,n = 300,\n          color = \"red\", size = 1.2) +\n  theme_bw()"
  },
  {
    "objectID": "p05-03-phan-tich-chuoi-thoi-gian-voi-tidyquant.html#chuyển-từ-dataframe-thành-xts",
    "href": "p05-03-phan-tich-chuoi-thoi-gian-voi-tidyquant.html#chuyển-từ-dataframe-thành-xts",
    "title": "43  Phân tích chuỗi thời gian với tidyquant và timetk",
    "section": "43.3 Chuyển từ dataframe thành xts",
    "text": "43.3 Chuyển từ dataframe thành xts\n\naapl_price %>% \n  timetk::tk_xts(date_var = date) %>% \n  head\n\n               open     high      low    close    volume adjusted\n2010-01-04 30.49000 30.64286 30.34000 30.57286 123432400 20.30787\n2010-01-05 30.65714 30.79857 30.46429 30.62571 150476200 20.34298\n2010-01-06 30.62571 30.74714 30.10714 30.13857 138040000 20.01940\n2010-01-07 30.25000 30.28571 29.86429 30.08286 119282800 19.98239\n2010-01-08 30.04286 30.28571 29.86572 30.28286 111902700 20.11524\n2010-01-11 30.40000 30.42857 29.77857 30.01572 115557400 19.93779"
  },
  {
    "objectID": "p05-06-du-bao-ts-voi-prophet.html#giới-thiệu",
    "href": "p05-06-du-bao-ts-voi-prophet.html#giới-thiệu",
    "title": "44  Dự báo chuỗi thời gian với prophet",
    "section": "44.1 Giới thiệu",
    "text": "44.1 Giới thiệu\nprophet là một hệ mã nguồn mở do Facebook xây dựng để dự báo chuỗi thời gian"
  },
  {
    "objectID": "p05-06-du-bao-ts-voi-prophet.html#ví-dụ-cơ-bản",
    "href": "p05-06-du-bao-ts-voi-prophet.html#ví-dụ-cơ-bản",
    "title": "44  Dự báo chuỗi thời gian với prophet",
    "section": "44.2 Ví dụ cơ bản",
    "text": "44.2 Ví dụ cơ bản\n\nlibrary(dplyr)\nlibrary(prophet)\nlibrary(ggplot2)\n#Step 1: Tạo data frame\nhistory <- data.frame(ds = seq(as.Date('2015-01-01'), \n                               as.Date('2016-01-01'), by = 'd'),\n                      y = 10*sin(1:366/200) + rnorm(366)/10)\nhistory %>% head\n\n          ds          y\n1 2015-01-01 0.01995739\n2 2015-01-02 0.02934285\n3 2015-01-03 0.19297714\n4 2015-01-04 0.15573405\n5 2015-01-05 0.03957832\n6 2015-01-06 0.44849733\n\n#Step 2: Tạo object prophet\nm <- prophet(history)\n#Step 3: Tạo future data frame\nfuture <- make_future_dataframe(m, periods = 365)\nfuture %>% str\n\n'data.frame':   731 obs. of  1 variable:\n $ ds: POSIXct, format: \"2015-01-01\" \"2015-01-02\" ...\n\n#Step 4: Forecast\nforecast <- predict(m, future)\nforecast %>% tail\n\n            ds    trend additive_terms additive_terms_lower\n726 2016-12-26 8.501695    0.012181806          0.012181806\n727 2016-12-27 8.498196   -0.009325680         -0.009325680\n728 2016-12-28 8.494697    0.016192950          0.016192950\n729 2016-12-29 8.491198    0.010090544          0.010090544\n730 2016-12-30 8.487699   -0.003712568         -0.003712568\n731 2016-12-31 8.484200   -0.012447151         -0.012447151\n    additive_terms_upper       weekly weekly_lower weekly_upper\n726          0.012181806  0.012181806  0.012181806  0.012181806\n727         -0.009325680 -0.009325680 -0.009325680 -0.009325680\n728          0.016192950  0.016192950  0.016192950  0.016192950\n729          0.010090544  0.010090544  0.010090544  0.010090544\n730         -0.003712568 -0.003712568 -0.003712568 -0.003712568\n731         -0.012447151 -0.012447151 -0.012447151 -0.012447151\n    multiplicative_terms multiplicative_terms_lower multiplicative_terms_upper\n726                    0                          0                          0\n727                    0                          0                          0\n728                    0                          0                          0\n729                    0                          0                          0\n730                    0                          0                          0\n731                    0                          0                          0\n    yhat_lower yhat_upper trend_lower trend_upper     yhat\n726   4.817304   12.62619    4.766141    12.62375 8.513877\n727   4.769093   12.57444    4.754269    12.64204 8.488870\n728   4.752107   12.66295    4.743340    12.65244 8.510890\n729   4.726643   12.68225    4.734230    12.66061 8.501288\n730   4.736178   12.71196    4.720865    12.67481 8.483986\n731   4.611060   12.71997    4.708564    12.68745 8.471753\n\n#Plot predictive\nplot(m, forecast) + theme_bw()\n\n\n\n#Plot component\nprophet_plot_components(m, forecast)"
  },
  {
    "objectID": "p05-06-du-bao-ts-voi-prophet.html#điều-chỉnh-mô-hình-tăng-trưởng",
    "href": "p05-06-du-bao-ts-voi-prophet.html#điều-chỉnh-mô-hình-tăng-trưởng",
    "title": "44  Dự báo chuỗi thời gian với prophet",
    "section": "44.3 Điều chỉnh mô hình tăng trưởng",
    "text": "44.3 Điều chỉnh mô hình tăng trưởng\n\nhistory$cap <- 8.5\nm <- prophet(history, growth = \"logistic\")\n#Step 3: Tạo future data frame\nfuture <- make_future_dataframe(m, periods = 365)\nfuture %>% str\n\n'data.frame':   731 obs. of  1 variable:\n $ ds: POSIXct, format: \"2015-01-01\" \"2015-01-02\" ...\n\nfuture$cap <- 8.5\n#Step 4: Forecast\nforecast <- predict(m, future)\nforecast %>% tail\n\n            ds    trend cap additive_terms additive_terms_lower\n726 2016-12-26 8.499999 8.5    0.011501673          0.011501673\n727 2016-12-27 8.499999 8.5   -0.004792505         -0.004792505\n728 2016-12-28 8.499999 8.5    0.026741261          0.026741261\n729 2016-12-29 8.499999 8.5    0.010044027          0.010044027\n730 2016-12-30 8.499999 8.5    0.001933590          0.001933590\n731 2016-12-31 8.499999 8.5   -0.025399320         -0.025399320\n    additive_terms_upper       weekly weekly_lower weekly_upper\n726          0.011501673  0.011501673  0.011501673  0.011501673\n727         -0.004792505 -0.004792505 -0.004792505 -0.004792505\n728          0.026741261  0.026741261  0.026741261  0.026741261\n729          0.010044027  0.010044027  0.010044027  0.010044027\n730          0.001933590  0.001933590  0.001933590  0.001933590\n731         -0.025399320 -0.025399320 -0.025399320 -0.025399320\n    multiplicative_terms multiplicative_terms_lower multiplicative_terms_upper\n726                    0                          0                          0\n727                    0                          0                          0\n728                    0                          0                          0\n729                    0                          0                          0\n730                    0                          0                          0\n731                    0                          0                          0\n    yhat_lower yhat_upper trend_lower trend_upper     yhat\n726   7.400333   9.576661    8.499999    8.499999 8.511501\n727   7.407550   9.607740    8.499999    8.499999 8.495207\n728   7.381026   9.637129    8.499999    8.499999 8.526740\n729   7.355499   9.642629    8.499999    8.499999 8.510043\n730   7.344128   9.603054    8.499999    8.499999 8.501933\n731   7.367233   9.531801    8.499999    8.499999 8.474600\n\n#Plot predictive\nplot(m, forecast) + theme_bw()\n\n\n\n#Plot component\nprophet_plot_components(m, forecast)"
  },
  {
    "objectID": "p05-06-du-bao-ts-voi-prophet.html#tài-liệu-tham-khảo",
    "href": "p05-06-du-bao-ts-voi-prophet.html#tài-liệu-tham-khảo",
    "title": "44  Dự báo chuỗi thời gian với prophet",
    "section": "44.4 Tài liệu tham khảo",
    "text": "44.4 Tài liệu tham khảo\n\nProphet package in Facebook DS team"
  },
  {
    "objectID": "p05-07-du-bao-ts-voi-timetk.html#dữ-liệu",
    "href": "p05-07-du-bao-ts-voi-timetk.html#dữ-liệu",
    "title": "45  Dự báo chuỗi thời gian với timetk",
    "section": "45.1 Dữ liệu",
    "text": "45.1 Dữ liệu\n\nlibrary(timetk)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tidyquant)\n\nbikes <- read_csv(\"data/day.csv\")\n# Select date and count\nbikes <- bikes %>%\n    select(dteday, cnt) %>%\n    rename(date = dteday)\nbikes %>% head\n\n# A tibble: 6 × 2\n  date         cnt\n  <date>     <dbl>\n1 2011-01-01   985\n2 2011-01-02   801\n3 2011-01-03  1349\n4 2011-01-04  1562\n5 2011-01-05  1600\n6 2011-01-06  1606\n\n\n\n# Visualize data and training/testing regions\nbikes %>%\n    ggplot(aes(x = date, y = cnt)) +\n    geom_rect(xmin = as.numeric(ymd(\"2012-07-01\")),\n              xmax = as.numeric(ymd(\"2013-01-01\")),\n              ymin = 0, ymax = 10000,\n              fill = palette_light()[[4]], alpha = 0.01) +\n    annotate(\"text\", x = ymd(\"2011-10-01\"), y = 7800,\n             color = palette_light()[[1]], label = \"Train Region\") +\n    annotate(\"text\", x = ymd(\"2012-10-01\"), y = 1550,\n             color = palette_light()[[1]], label = \"Test Region\") +\n    geom_point(alpha = 0.5, color = palette_light()[[1]]) +\n    labs(title = \"Bikes Sharing Dataset: Daily Scale\", x = \"\") +\n    theme_tq()\n\n\n\n\n\nDữ liệu train/test\n\n\n# Split into training and test sets\ntrain <- bikes %>%\n    filter(date < ymd(\"2012-07-01\"))\n\ntest <- bikes %>%\n    filter(date >= ymd(\"2012-07-01\"))\n\n\nTạo thêm dữ liệu pattern\n\n\ntrain_augmented <- train %>%\n    tk_augment_timeseries_signature()\ntrain_augmented %>% head\n\n# A tibble: 6 × 30\n  date         cnt  index.num  diff  year year.iso  half quarter month month.xts\n  <date>     <dbl>      <dbl> <dbl> <int>    <int> <int>   <int> <int>     <int>\n1 2011-01-01   985 1293840000    NA  2011     2010     1       1     1         0\n2 2011-01-02   801 1293926400 86400  2011     2010     1       1     1         0\n3 2011-01-03  1349 1294012800 86400  2011     2011     1       1     1         0\n4 2011-01-04  1562 1294099200 86400  2011     2011     1       1     1         0\n5 2011-01-05  1600 1294185600 86400  2011     2011     1       1     1         0\n6 2011-01-06  1606 1294272000 86400  2011     2011     1       1     1         0\n# ℹ 20 more variables: month.lbl <ord>, day <int>, hour <int>, minute <int>,\n#   second <int>, hour12 <int>, am.pm <int>, wday <int>, wday.xts <int>,\n#   wday.lbl <ord>, mday <int>, qday <int>, yday <int>, mweek <int>,\n#   week <int>, week.iso <int>, week2 <int>, week3 <int>, week4 <int>,\n#   mday7 <int>\n\n\n\nMô hình\n\n\nlibrary(broom)\nfit_lm <- lm(cnt ~ ., data = train_augmented)\nfit_lm %>% summary\n\n\nCall:\nlm(formula = cnt ~ ., data = train_augmented)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4511.2  -358.2   100.1   545.7  3018.9 \n\nCoefficients: (16 not defined because of singularities)\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.054e+08  1.572e+08   1.944 0.052477 .  \ndate          4.299e+02  2.182e+02   1.971 0.049312 *  \nindex.num            NA         NA      NA       NA    \ndiff                 NA         NA      NA       NA    \nyear         -1.556e+05  7.977e+04  -1.951 0.051611 .  \nyear.iso      5.045e+02  6.501e+02   0.776 0.438092    \nhalf          1.947e+03  7.885e+02   2.470 0.013851 *  \nquarter       8.368e+04  2.515e+04   3.327 0.000941 ***\nmonth        -4.019e+04  8.555e+03  -4.698 3.37e-06 ***\nmonth.xts            NA         NA      NA       NA    \nmonth.lbl.L          NA         NA      NA       NA    \nmonth.lbl.Q  -3.821e+03  2.873e+02 -13.299  < 2e-16 ***\nmonth.lbl.C   2.035e+03  5.761e+02   3.533 0.000447 ***\nmonth.lbl^4   4.329e+02  1.645e+02   2.632 0.008754 ** \nmonth.lbl^5  -1.837e+03  5.148e+02  -3.568 0.000393 ***\nmonth.lbl^6   4.286e+02  2.091e+02   2.050 0.040874 *  \nmonth.lbl^7  -1.705e+01  2.533e+02  -0.067 0.946379    \nmonth.lbl^8   1.442e+03  3.620e+02   3.983 7.77e-05 ***\nmonth.lbl^9          NA         NA      NA       NA    \nmonth.lbl^10  1.058e+03  2.374e+02   4.456 1.03e-05 ***\nmonth.lbl^11         NA         NA      NA       NA    \nday          -1.360e+03  2.784e+02  -4.886 1.38e-06 ***\nhour                 NA         NA      NA       NA    \nminute               NA         NA      NA       NA    \nsecond               NA         NA      NA       NA    \nhour12               NA         NA      NA       NA    \nam.pm                NA         NA      NA       NA    \nwday          3.785e+01  2.723e+01   1.390 0.165092    \nwday.xts             NA         NA      NA       NA    \nwday.lbl.L           NA         NA      NA       NA    \nwday.lbl.Q    2.572e+02  1.968e+02   1.306 0.191976    \nwday.lbl.C   -1.353e+02  1.649e+02  -0.820 0.412367    \nwday.lbl^4    9.018e+01  1.266e+02   0.713 0.476409    \nwday.lbl^5   -8.108e+01  1.061e+02  -0.764 0.444973    \nwday.lbl^6    1.682e+02  1.007e+02   1.671 0.095297 .  \nmday                 NA         NA      NA       NA    \nqday          9.277e+02  2.766e+02   3.353 0.000857 ***\nyday                 NA         NA      NA       NA    \nmweek         3.838e+02  2.768e+02   1.387 0.166124    \nweek         -2.162e+02  2.301e+02  -0.940 0.347856    \nweek.iso             NA         NA      NA       NA    \nweek2         6.469e+01  8.463e+01   0.764 0.444935    \nweek3         2.367e+01  4.756e+01   0.498 0.618948    \nweek4        -4.816e+00  3.845e+01  -0.125 0.900387    \nmday7        -1.355e+02  1.365e+02  -0.993 0.321347    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 884.1 on 517 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.7459,    Adjusted R-squared:  0.7322 \nF-statistic: 54.21 on 28 and 517 DF,  p-value: < 2.2e-16\n\nfit_lm %>%\n    augment() %>%\n    ggplot(aes(x = date, y = .resid)) +\n    geom_hline(yintercept = 0, color = \"red\") +\n    geom_point(color = palette_light()[[1]], alpha = 0.5) +\n    theme_tq() +\n    labs(title = \"Training Set: lm() Model Residuals\", x = \"\") +\n    scale_y_continuous(limits = c(-5000, 5000))\n\n\n\n\n\nTạo dữ liệu test\n\n\ntest_augmented <- test %>%\n    tk_augment_timeseries_signature()\ntest_augmented\n\n# A tibble: 184 × 30\n   date         cnt index.num  diff  year year.iso  half quarter month month.xts\n   <date>     <dbl>     <dbl> <dbl> <int>    <int> <int>   <int> <int>     <int>\n 1 2012-07-01  5531    1.34e9    NA  2012     2012     2       3     7         6\n 2 2012-07-02  6227    1.34e9 86400  2012     2012     2       3     7         6\n 3 2012-07-03  6660    1.34e9 86400  2012     2012     2       3     7         6\n 4 2012-07-04  7403    1.34e9 86400  2012     2012     2       3     7         6\n 5 2012-07-05  6241    1.34e9 86400  2012     2012     2       3     7         6\n 6 2012-07-06  6207    1.34e9 86400  2012     2012     2       3     7         6\n 7 2012-07-07  4840    1.34e9 86400  2012     2012     2       3     7         6\n 8 2012-07-08  4672    1.34e9 86400  2012     2012     2       3     7         6\n 9 2012-07-09  6569    1.34e9 86400  2012     2012     2       3     7         6\n10 2012-07-10  6290    1.34e9 86400  2012     2012     2       3     7         6\n# ℹ 174 more rows\n# ℹ 20 more variables: month.lbl <ord>, day <int>, hour <int>, minute <int>,\n#   second <int>, hour12 <int>, am.pm <int>, wday <int>, wday.xts <int>,\n#   wday.lbl <ord>, mday <int>, qday <int>, yday <int>, mweek <int>,\n#   week <int>, week.iso <int>, week2 <int>, week3 <int>, week4 <int>,\n#   mday7 <int>\n\nyhat_test <- predict(fit_lm, newdata = test_augmented)\npred_test <- test %>%\n    add_column(yhat = yhat_test) %>%\n    mutate(.resid = cnt - yhat)\npred_test\n\n# A tibble: 184 × 4\n   date         cnt  yhat .resid\n   <date>     <dbl> <dbl>  <dbl>\n 1 2012-07-01  5531 6671. -1140.\n 2 2012-07-02  6227 6684.  -457.\n 3 2012-07-03  6660 6889.  -229.\n 4 2012-07-04  7403 6750.   653.\n 5 2012-07-05  6241 6982.  -741.\n 6 2012-07-06  6207 7007.  -800.\n 7 2012-07-07  4840 7004. -2164.\n 8 2012-07-08  4672 6660. -1988.\n 9 2012-07-09  6569 6672.  -103.\n10 2012-07-10  6290 6878.  -588.\n# ℹ 174 more rows\n\n\n\nSo sánh kết quả\n\n\nggplot(aes(x = date), data = bikes) +\n    geom_rect(xmin = as.numeric(ymd(\"2012-07-01\")),\n              xmax = as.numeric(ymd(\"2013-01-01\")),\n              ymin = 0, ymax = 10000,\n              fill = palette_light()[[4]], alpha = 0.01) +\n    annotate(\"text\", x = ymd(\"2011-10-01\"), y = 7800,\n             color = palette_light()[[1]], label = \"Train Region\") +\n    annotate(\"text\", x = ymd(\"2012-10-01\"), y = 1550,\n             color = palette_light()[[1]], label = \"Test Region\") + \n    geom_point(aes(x = date, y = cnt), data = train, alpha = 0.5, color = palette_light()[[1]]) +\n    geom_point(aes(x = date, y = cnt), data = pred_test, alpha = 0.5, color = palette_light()[[1]]) +\n    geom_point(aes(x = date, y = yhat), data = pred_test, alpha = 0.5, color = palette_light()[[2]]) +\n    theme_tq() \n\n\n\n\n\nĐộ chính xác mô hình\n\n\ntest_residuals <- pred_test$.resid\npct_err <- test_residuals/pred_test$cnt * 100 # Percentage error\n\nme   <- mean(test_residuals, na.rm=TRUE)\nrmse <- mean(test_residuals^2, na.rm=TRUE)^0.5\nmae  <- mean(abs(test_residuals), na.rm=TRUE)\nmape <- mean(abs(pct_err), na.rm=TRUE)\nmpe  <- mean(pct_err, na.rm=TRUE)\n\nerror_tbl <- tibble(me, rmse, mae, mape, mpe)\nerror_tbl\n\n# A tibble: 1 × 5\n     me  rmse   mae  mape   mpe\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1 -113. 1466. 1062.  176. -163.\n\n\n\nForecast\n\n\n# Extract bikes index\nidx <- bikes %>%\n    tk_index()\n\n# Get time series summary from index\nbikes_summary <- idx %>%\n    tk_get_timeseries_summary()\n\n# Tạo dữ liệu mới\nidx_future <- idx %>%\n    tk_make_future_timeseries(n_future = 180)\n\ndata_future <- idx_future %>%\n    tk_get_timeseries_signature() %>%\n    rename(date = index)\n\npred_future <- predict(fit_lm, newdata = data_future)\n\n#Tạo dữ liệu mới\nbikes_future <- data_future %>%\n    select(date) %>%\n    add_column(cnt = pred_future)\n\n\nbikes %>%\n    ggplot(aes(x = date, y = cnt)) +\n    geom_rect(xmin = as.numeric(ymd(\"2012-07-01\")),\n              xmax = as.numeric(ymd(\"2013-01-01\")),\n              ymin = 0, ymax = 10000,\n              fill = palette_light()[[4]], alpha = 0.01) +\n    geom_rect(xmin = as.numeric(ymd(\"2013-01-01\")),\n              xmax = as.numeric(ymd(\"2013-07-01\")),\n              ymin = 0, ymax = 10000,\n              fill = palette_light()[[3]], alpha = 0.01) +\n    annotate(\"text\", x = ymd(\"2011-10-01\"), y = 7800,\n             color = palette_light()[[1]], label = \"Train Region\") +\n    annotate(\"text\", x = ymd(\"2012-10-01\"), y = 1550,\n             color = palette_light()[[1]], label = \"Test Region\") +\n    annotate(\"text\", x = ymd(\"2013-4-01\"), y = 1550,\n             color = palette_light()[[1]], label = \"Forecast Region\") +\n    geom_point(alpha = 0.5, color = palette_light()[[1]]) +\n    geom_point(aes(x = date, y = cnt), data = bikes_future,\n               alpha = 0.5, color = palette_light()[[2]]) +\n    geom_smooth(aes(x = date, y = cnt), data = bikes_future,\n                method = 'loess') + \n    labs(title = \"Bikes Sharing Dataset: 6-Month Forecast\", x = \"\") +\n    theme_tq()\n\n\n\n\n\nĐộ chính xác mô hình\n\n\n# Calculate standard deviation of residuals\ntest_resid_sd <- sd(test_residuals)\n\nbikes_future <- bikes_future %>%\n    mutate(\n        lo.95 = cnt - 1.96 * test_resid_sd,\n        lo.80 = cnt - 1.28 * test_resid_sd,\n        hi.80 = cnt + 1.28 * test_resid_sd,\n        hi.95 = cnt + 1.96 * test_resid_sd\n        )\n\nbikes %>%\n    ggplot(aes(x = date, y = cnt)) +\n    geom_point(alpha = 0.5, color = palette_light()[[1]]) +\n    geom_ribbon(aes(ymin = lo.95, ymax = hi.95), data = bikes_future, \n                fill = \"#D5DBFF\", color = NA, size = 0) +\n    geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key), data = bikes_future,\n                fill = \"#596DD5\", color = NA, size = 0, alpha = 0.8) +\n    geom_point(aes(x = date, y = cnt), data = bikes_future,\n               alpha = 0.5, color = palette_light()[[2]]) +\n    geom_smooth(aes(x = date, y = cnt), data = bikes_future,\n                method = 'loess', color = \"white\") + \n    labs(title = \"Bikes Sharing Dataset: 6-Month Forecast with Prediction Intervals\", x = \"\") +\n    theme_tq()\n\n\n\n\n\nlibrary(tidyquant)\nbikes %>% \n  ggplot(aes(date, cnt)) +\n  geom_point(alpha = 0.4) +\n  geom_ma(n = 7, col = \"darkred\", size = 0.9) +\n  theme_tq()"
  },
  {
    "objectID": "p05-07-du-bao-ts-voi-timetk.html#tài-liệu-tham-khảo",
    "href": "p05-07-du-bao-ts-voi-timetk.html#tài-liệu-tham-khảo",
    "title": "45  Dự báo chuỗi thời gian với timetk",
    "section": "45.2 Tài liệu tham khảo",
    "text": "45.2 Tài liệu tham khảo\n\nhttp://www.business-science.io/code-tools/2017/10/28/demo_week_h2o.html"
  },
  {
    "objectID": "p05-08-abnomaly-detection.html#phân-tách-các-thành-phần-chuỗi-thời-gian",
    "href": "p05-08-abnomaly-detection.html#phân-tách-các-thành-phần-chuỗi-thời-gian",
    "title": "46  Abnomaly detection trong times series",
    "section": "46.1 Phân tách các thành phần chuỗi thời gian",
    "text": "46.1 Phân tách các thành phần chuỗi thời gian\n\ntidyverse_cran_downloads %>%\n    time_decompose(count, method = \"stl\", frequency = \"auto\", trend = \"auto\")\n\n# A time tibble: 6,375 × 6\n# Index:         date\n# Groups:        package [15]\n   package date       observed season trend remainder\n   <chr>   <date>        <dbl>  <dbl> <dbl>     <dbl>\n 1 broom   2017-01-01     1053 -1007. 1708.    352.  \n 2 broom   2017-01-02     1481   340. 1731.   -589.  \n 3 broom   2017-01-03     1851   563. 1753.   -465.  \n 4 broom   2017-01-04     1947   526. 1775.   -354.  \n 5 broom   2017-01-05     1927   430. 1798.   -301.  \n 6 broom   2017-01-06     1948   136. 1820.     -8.11\n 7 broom   2017-01-07     1542  -988. 1842.    688.  \n 8 broom   2017-01-08     1479 -1007. 1864.    622.  \n 9 broom   2017-01-09     2057   340. 1887.   -169.  \n10 broom   2017-01-10     2278   563. 1909.   -194.  \n# ℹ 6,365 more rows"
  },
  {
    "objectID": "p05-08-abnomaly-detection.html#xác-định-abnomalize",
    "href": "p05-08-abnomaly-detection.html#xác-định-abnomalize",
    "title": "46  Abnomaly detection trong times series",
    "section": "46.2 Xác định abnomalize",
    "text": "46.2 Xác định abnomalize\nĐể xác định các quan sát bất thường, ta dùng hàm anomalize:\n\nmethood = \"iqr\": Sử dụng Inter Quantile Range để xác định quan sát bất thường\nmax_anoms = 0.2: Tham số xác định tỷ lệ phần trăm quan sát có thể là quan sát bất thường\n\n\ntidyverse_cran_downloads %>%\n    time_decompose(count, method = \"stl\", frequency = \"auto\", trend = \"auto\") %>%\n    anomalize(remainder, method = \"iqr\", alpha = 0.05, max_anoms = 0.2)\n\n# A time tibble: 6,375 × 9\n# Index:         date\n# Groups:        package [15]\n   package date       observed season trend remainder remainder_l1 remainder_l2\n   <chr>   <date>        <dbl>  <dbl> <dbl>     <dbl>        <dbl>        <dbl>\n 1 broom   2017-01-01     1053 -1007. 1708.    352.         -1725.        1704.\n 2 broom   2017-01-02     1481   340. 1731.   -589.         -1725.        1704.\n 3 broom   2017-01-03     1851   563. 1753.   -465.         -1725.        1704.\n 4 broom   2017-01-04     1947   526. 1775.   -354.         -1725.        1704.\n 5 broom   2017-01-05     1927   430. 1798.   -301.         -1725.        1704.\n 6 broom   2017-01-06     1948   136. 1820.     -8.11       -1725.        1704.\n 7 broom   2017-01-07     1542  -988. 1842.    688.         -1725.        1704.\n 8 broom   2017-01-08     1479 -1007. 1864.    622.         -1725.        1704.\n 9 broom   2017-01-09     2057   340. 1887.   -169.         -1725.        1704.\n10 broom   2017-01-10     2278   563. 1909.   -194.         -1725.        1704.\n# ℹ 6,365 more rows\n# ℹ 1 more variable: anomaly <chr>\n\n\nPhân tách các quan sát bất thường trong chuỗi thời gian.\n\ntidyverse_cran_downloads %>%\n    \n    # Select a single time series\n    filter(package == \"lubridate\") %>%\n    ungroup() %>%\n    \n    # Anomalize\n    time_decompose(count, method = \"stl\", frequency = \"auto\", trend = \"auto\") %>%\n    anomalize(remainder, method = \"iqr\", alpha = 0.05, max_anoms = 0.2) %>%\n    \n    # Plot Anomaly Decomposition\n    plot_anomaly_decomposition() +\n    ggtitle(\"Lubridate Downloads: Anomaly Decomposition\")"
  },
  {
    "objectID": "p05-08-abnomaly-detection.html#xác-định-vùng-quan-sát-bất-thường",
    "href": "p05-08-abnomaly-detection.html#xác-định-vùng-quan-sát-bất-thường",
    "title": "46  Abnomaly detection trong times series",
    "section": "46.3 Xác định vùng quan sát bất thường",
    "text": "46.3 Xác định vùng quan sát bất thường\n\ntidyverse_cran_downloads %>%\n    # Select single time series\n    filter(package == \"lubridate\") %>%\n    ungroup() %>%\n    # Anomalize\n    time_decompose(count, method = \"stl\", frequency = \"auto\", trend = \"auto\") %>%\n    anomalize(remainder, method = \"iqr\", alpha = 0.05, max_anoms = 0.2) %>%\n    time_recompose() %>%\n    # Plot Anomaly Decomposition\n    plot_anomalies(time_recomposed = TRUE) +\n    ggtitle(\"Lubridate Downloads: Anomalies Detected\")"
  },
  {
    "objectID": "p05-08-abnomaly-detection.html#tài-liệu-tham-khảo",
    "href": "p05-08-abnomaly-detection.html#tài-liệu-tham-khảo",
    "title": "46  Abnomaly detection trong times series",
    "section": "46.4 Tài liệu tham khảo",
    "text": "46.4 Tài liệu tham khảo\n\nhttps://www.business-science.io/code-tools/2018/04/08/introducing-anomalize.html"
  },
  {
    "objectID": "p06-01-nlp-co-ban.html#giới-thiệu",
    "href": "p06-01-nlp-co-ban.html#giới-thiệu",
    "title": "47  Xử lý ngôn ngữ tự nhiên cơ bản",
    "section": "47.1 Giới thiệu",
    "text": "47.1 Giới thiệu\nBên cạnh các định dạng dữ liệu cơ bản (dạng bảng), dữ liệu dạng text ngày càng đóng vai trò quan trọng và đòi hỏi cần được xử lý và phân tích hiệu quả. Trong phần này, chúng ta sẽ đi qua các cách xử lý dữ liệu text cơ bản có thể áp dụng trong R với tidytext và h2o.\nLưu ý: Khi xử lý dữ liệu định dạng tiếng Việt, ta cần sử dụng Macbook hoặc Linux trên cloud hoặc cài đặt qua docker.\nDữ liệu text có 3 dạng:\n\nString: Text thuần túy\nCorpus: Text và metadata\nDocument-term matrix: Dạng sparse matrix, dòng là document, cột là term đã được phân rã\n\nPhân tích khám phá dữ liệu text thường trải qua ba giai đoạn:\n\nUnnest token: Phân rã text thành các cụm 2 hoặc 3 âm, được gọi là tokens\nLoại bỏ các cụm, từ vô nghĩa\nVizualize kết quả thành định dạng words hoặc tiếp tục xây dựng thành các nhóm mô hình dự báo"
  },
  {
    "objectID": "p06-01-nlp-co-ban.html#unnest_tokens",
    "href": "p06-01-nlp-co-ban.html#unnest_tokens",
    "title": "47  Xử lý ngôn ngữ tự nhiên cơ bản",
    "section": "47.2 Unnest_tokens",
    "text": "47.2 Unnest_tokens\nUnnest tokens là một kỹ thuật cho phép tách các từ trong words thành từng dòng\n\nlibrary(tidytext)\nlibrary(tidyverse)\ntext <- c(\"Because I could not stop for Death -\",\n          \"He kindly stopped for me -\",\n          \"The Carriage held but just Ourselves -\",\n          \"and Immortality\")\ntext_df <- data_frame(line = 1:4, text = text)\ntext_df\n\n# A tibble: 4 × 2\n   line text                                  \n  <int> <chr>                                 \n1     1 Because I could not stop for Death -  \n2     2 He kindly stopped for me -            \n3     3 The Carriage held but just Ourselves -\n4     4 and Immortality                       \n\n#Sử dụng tokens\n#Tokens theo từ\ntext_df %>%\n  unnest_tokens(word, text)\n\n# A tibble: 20 × 2\n    line word       \n   <int> <chr>      \n 1     1 because    \n 2     1 i          \n 3     1 could      \n 4     1 not        \n 5     1 stop       \n 6     1 for        \n 7     1 death      \n 8     2 he         \n 9     2 kindly     \n10     2 stopped    \n11     2 for        \n12     2 me         \n13     3 the        \n14     3 carriage   \n15     3 held       \n16     3 but        \n17     3 just       \n18     3 ourselves  \n19     4 and        \n20     4 immortality\n\n#Tokens theo cụm 2 từ\ntext_df %>% \n  unnest_tokens(ngram, text, token = \"ngrams\", n = 2)\n\n# A tibble: 16 × 2\n    line ngram          \n   <int> <chr>          \n 1     1 because i      \n 2     1 i could        \n 3     1 could not      \n 4     1 not stop       \n 5     1 stop for       \n 6     1 for death      \n 7     2 he kindly      \n 8     2 kindly stopped \n 9     2 stopped for    \n10     2 for me         \n11     3 the carriage   \n12     3 carriage held  \n13     3 held but       \n14     3 but just       \n15     3 just ourselves \n16     4 and immortality"
  },
  {
    "objectID": "p06-01-nlp-co-ban.html#loại-bỏ-các-từ-thưa",
    "href": "p06-01-nlp-co-ban.html#loại-bỏ-các-từ-thưa",
    "title": "47  Xử lý ngôn ngữ tự nhiên cơ bản",
    "section": "47.3 Loại bỏ các từ thưa",
    "text": "47.3 Loại bỏ các từ thưa\nTrong ví dụ trên, ta chưa loại bỏ các cụm từ không cần thiết. Khi phân tích, cần loại bỏ các từ này để tránh bị nhiễu\n\nstop_words <- data.frame(word = c(\"i\", \"not\", \"but\", \"and\"))\n\n#Loại bỏ stopword\ntext_df %>%\n  unnest_tokens(word, text) %>% \n  anti_join(stop_words) %>% \n  arrange(line)\n\n# A tibble: 16 × 2\n    line word       \n   <int> <chr>      \n 1     1 because    \n 2     1 could      \n 3     1 stop       \n 4     1 for        \n 5     1 death      \n 6     2 he         \n 7     2 kindly     \n 8     2 stopped    \n 9     2 for        \n10     2 me         \n11     3 the        \n12     3 carriage   \n13     3 held       \n14     3 just       \n15     3 ourselves  \n16     4 immortality\n\n#Đếm tần xuất\ntext_df %>%\n  unnest_tokens(word, text) %>% \n  count(word, sort = T)\n\n# A tibble: 19 × 2\n   word            n\n   <chr>       <int>\n 1 for             2\n 2 and             1\n 3 because         1\n 4 but             1\n 5 carriage        1\n 6 could           1\n 7 death           1\n 8 he              1\n 9 held            1\n10 i               1\n11 immortality     1\n12 just            1\n13 kindly          1\n14 me              1\n15 not             1\n16 ourselves       1\n17 stop            1\n18 stopped         1\n19 the             1"
  },
  {
    "objectID": "p06-01-nlp-co-ban.html#xây-dựng-wordcloud",
    "href": "p06-01-nlp-co-ban.html#xây-dựng-wordcloud",
    "title": "47  Xử lý ngôn ngữ tự nhiên cơ bản",
    "section": "47.4 Xây dựng wordcloud",
    "text": "47.4 Xây dựng wordcloud\n\nlibrary(wordcloud)\nlibrary(janeaustenr)\nlibrary(stringr)\n\ntidy_books <- austen_books() %>%\n  group_by(book) %>%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\", \n                                                 ignore_case = TRUE)))) %>%\n  ungroup() %>%\n  unnest_tokens(word, text)\n\n\ntidy_books %>%\n  anti_join(stop_words) %>%\n  count(word) %>% \n  with(wordcloud(word, n, max.words = 100), \n       colors=brewer.pal(2, \"Pal2\"))"
  },
  {
    "objectID": "p06-01-nlp-co-ban.html#tài-liệu-tham-khảo",
    "href": "p06-01-nlp-co-ban.html#tài-liệu-tham-khảo",
    "title": "47  Xử lý ngôn ngữ tự nhiên cơ bản",
    "section": "47.5 Tài liệu tham khảo",
    "text": "47.5 Tài liệu tham khảo\n\nhttps://www.tidytextmining.com/"
  },
  {
    "objectID": "p06-04-text-classification.html#giới-thiệu",
    "href": "p06-04-text-classification.html#giới-thiệu",
    "title": "48  Phân loại text",
    "section": "48.1 Giới thiệu",
    "text": "48.1 Giới thiệu\nKhi xử lý dữ liệu text, bên cạnh các bước cơ bản như tìm trực quan hóa tần suất, số lượng từ, xây dựng wordcloud, còn có nhiều kỹ thuật khác ứng dụng học máy trong xử lý ngôn ngữ. Trong chương này, ta sẽ tìm hiểu cách thức phân loại dữ liệu text.\nKhác với định dạng bảng thông thường, khi xây dựng mô hình dự báo trong text, ta cần phải biến dữ liệu từ dạng text sang dạng bảng. Dữ liệu mới (dạng bảng) sẽ được sử dụng trong việc xây dựng mô hình. Trong các kỹ thuật dạng này, word2vec của h2o là thuật toán vượt trội, biến dữ liệu text bản gốc thành dữ liệu mới giúp xây dựng mô hình dự báo hiệu quả.\nKhái quát hơn, các bước của việc dự báo dữ liệu text diễn ra như sau.\n\nThực hiện word2vec:\n\nTokenize words\nLoại bỏ các từ thừa\nXây mô hình word2vec\nTest thử mô hình với các từ gần nghĩa\n\nSử dụng mô hình word2vec để biến đổi text thành vector\nXây dựng mô hình dự báo như bình thường\nSử dụng word2vec để biến đổi text (validation) thành vector và dự báo với mô hình vừa xây"
  },
  {
    "objectID": "p06-04-text-classification.html#thực-hành-với-r",
    "href": "p06-04-text-classification.html#thực-hành-với-r",
    "title": "48  Phân loại text",
    "section": "48.2 Thực hành với R",
    "text": "48.2 Thực hành với R\n\nlibrary(tidyverse)\nlibrary(h2o)\nh2o.init()\n\n#df <- read_csv(\"data/craigslistJobTitles.csv\")\n\njob.titles.path = \"data/craigslistJobTitles.csv\"\n\njob.titles <- h2o.importFile(job.titles.path, \n                             destination_frame = \"jobtitles\",\n                             col.names = c(\"category\", \"jobtitle\"), \n                             col.types = c(\"Enum\", \"String\"), header = F)\n\nSTOP_WORDS = c(\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\",\n               \"there\",\"all\",\"we\",\"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\n               \"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\n               \"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\"from\",\"com\",\"org\",\"like\",\"likes\",\"so\")\n\n\n# Hàm tokenize trong H2O\n\ntokenize <- function(sentences, stop.words = STOP_WORDS) {\n  tokenized <- h2o.tokenize(sentences, \"\\\\\\\\W+\")\n  \n  # convert to lower case\n  tokenized.lower <- h2o.tolower(tokenized)\n  # remove short words (less than 2 characters)\n  tokenized.lengths <- h2o.nchar(tokenized.lower)\n  tokenized.filtered <- tokenized.lower[is.na(tokenized.lengths) || tokenized.lengths >= 2,]\n  # remove words that contain numbers\n  tokenized.words <- tokenized.filtered[h2o.grep(\"[0-9]\", tokenized.filtered, invert = TRUE, output.logical = TRUE),]\n  \n  # remove stop words\n  tokenized.words[is.na(tokenized.words) || (! tokenized.words %in% STOP_WORDS),]\n}\n\n# Xây hàm mô hình dự báo với word2vec và GBM (mô hình phân loại)\npredict <- function(job.title, w2v, gbm) {\n  words <- tokenize(as.character(as.h2o(job.title)))\n  job.title.vec <- h2o.transform(w2v, words, aggregate_method = \"AVERAGE\")\n  h2o.predict(gbm, job.title.vec)\n}\n\n\nXây dựng mô hình word2vec\n\n\nprint(\"Break job titles into sequence of words\")\nwords <- tokenize(job.titles$jobtitle)\n\nprint(\"Build word2vec model\")\nw2v.model <- h2o.word2vec(words, sent_sample_rate = 0, epochs = 10)\n\n# Kiểm tra lại mô hình\nprint(\"Sanity check - find synonyms for the word 'teacher'\")\nh2o.findSynonyms(w2v.model, \"supervisor\", count = 5)\n\n\nViệc kiểm tra các từ gần nghĩa chỉ có tác dụng với các từ có trong mô hình\n\n\nword.df <- words %>% as.data.frame() %>% rename(word = C1)\nprint(word.df %>% filter(word == \"hello\"))\n\n\nXây dựng mô hình phân loại công việc\n\n\nprint(\"Calculate a vector for each job title\")\njob.title.vecs <- h2o.transform(w2v.model, words, aggregate_method = \"AVERAGE\")\n\nprint(\"Prepare training&validation data (keep only job titles made of known words)\")\njob.title.vecs %>% dim\nvalid.job.titles <- !is.na(job.title.vecs$C1)\n\ndata <- h2o.cbind(job.titles[valid.job.titles, \"category\"], job.title.vecs[valid.job.titles, ])\ndata %>% names\n\n# Chia train và test\ndata.split <- h2o.splitFrame(data, ratios = 0.8)\n\nprint(\"Build a basic GBM model\")\ngbm.model <- h2o.gbm(x = names(job.title.vecs), y = \"category\",\n                     training_frame = data.split[[1]], validation_frame = data.split[[2]])\n\n# Dự báo với job description mới\nprint(\"Predict!\")\nprint(predict(\"school teacher having holidays every month\", w2v.model, gbm.model))\nprint(predict(\"developer with 3+ Java experience, jumping\", w2v.model, gbm.model))\nprint(predict(\"Financial accountant CPA preferred\", w2v.model, gbm.model))\nprint(predict(\"strong finance background\", w2v.model, gbm.model))\nprint(predict(\"Need someone who can sell beer\", w2v.model, gbm.model))\nprint(predict(\"answer customer based on their request\", w2v.model, gbm.model))\n\n# Dự báo được cả các word không có trong mô hình bằng cách lấy trung bình tất cả các category\nprint(predict(\"việt nam\", w2v.model, gbm.model))"
  },
  {
    "objectID": "p06-04-text-classification.html#tài-liệu-tham-khảo",
    "href": "p06-04-text-classification.html#tài-liệu-tham-khảo",
    "title": "48  Phân loại text",
    "section": "48.3 Tài liệu tham khảo",
    "text": "48.3 Tài liệu tham khảo\n\nhttps://www.slideshare.net/0xdata/nlp-with-h2o\nhttps://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/"
  },
  {
    "objectID": "p06-07-nlp-api.html#azure",
    "href": "p06-07-nlp-api.html#azure",
    "title": "49  Sử dụng API từ các dịch vụ đám mây",
    "section": "49.1 Azure",
    "text": "49.1 Azure\nAzure cho phép sử dụng dịch vụ điện toán đám mây để phân tích sentiment analysis\n\n#Text Analysis with Azure Machine Learning\n#Key to Text Analysis: \n#Key 1: 01d3b74966574fee929ee6b6462ff2f3\n#Key 2: 80a9b1f8d635412b9f4804e51bfbcef6\n\nmy_subscription_key <- \"01d3b74966574fee929ee6b6462ff2f3\"\n#Packages\nlibrary(httr)\nlibrary(jsonlite)\n\n# Below is the Request body for the API having text \n# id 1 = Negative sentiments, id 2 = Positive sentiments\n\nrequest_body <- data.frame(\n  language = c(\"en\",\"en\"),\n  id = c(\"1\",\"2\"),\n  text = c(\"This is wasted! I'm angry\",\n           \"This is awesome! Good Job Team! appreciated\")\n)\n\n# Converting the Request body(Dataframe) to Request body(JSON)\n\nrequest_body_json <- toJSON(list(documents = request_body), auto_unbox = TRUE)\n\n# Below we are calling API (Adding Request headers using add_headers)\n\nresult <- POST(\"https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment\",\n               body = request_body_json,\n               add_headers(.headers = \n                             c(\"Content-Type\"=\"application/json\",\n                               \"Ocp-Apim-Subscription-Key\"=\"01d3b74966574fee929ee6b6462ff2f3\")))\nOutput <- content(result)\n\n# Show Output\nOutput\n\n\n\n# Detect language ---------------------------------------------------------\n\n# Below is the Request body for the API\nrequest_body <- data.frame(\n  id = \"1\",\n  text = \"Bài báo như dở hơi! Thật là khốn nạn! Làm như kít\",\n  stringsAsFactors = FALSE\n)\n\n# Converting the Request body(Dataframe) to Request body(JSON)\nrequest_body_json <- toJSON(list(documents = request_body), auto_unbox = TRUE)\n\n# Below we are calling API (Adding Request headers using add_headers)\n# Here parameter numberOfLanguagesToDetect=1\n\nresult <- POST(\"https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/languages?numberOfLanguagesToDetect=1\",\n               body = request_body_json ,\n               add_headers(.headers = c(\"Content-Type\"=\"application/json\",\n                                        \"Ocp-Apim-Subscription-Key\"=my_subscription_key))\n)\nOutput <- content(result)\nOutput"
  },
  {
    "objectID": "p06-07-nlp-api.html#tài-liệu-tham-khảo",
    "href": "p06-07-nlp-api.html#tài-liệu-tham-khảo",
    "title": "49  Sử dụng API từ các dịch vụ đám mây",
    "section": "49.2 Tài liệu tham khảo",
    "text": "49.2 Tài liệu tham khảo\n\nhttps://bigdataenthusiast.wordpress.com/"
  },
  {
    "objectID": "p06-10-ocr-tesseract.html#giới-thiệu-ocr",
    "href": "p06-10-ocr-tesseract.html#giới-thiệu-ocr",
    "title": "50  OCR với tesseract",
    "section": "50.1 Giới thiệu OCR",
    "text": "50.1 Giới thiệu OCR\nOCR (Optical Character Recognition) là công nghệ cho phép nhận diện chữ scan và chuyển đổi thành dữ liệu. Với R, ta có thể dùng package tesseract để sử dụng công nghệ OCR.\nXem ví dụ dưới đây.\n\nlibrary(tesseract)\neng <- tesseract(\"eng\")\ntext <- tesseract::ocr(\"data/ocr-image-en.png\", engine = eng)\ncat(text)\n\nThis is a lot of 12 point text to test the\nocr code and see if it works on all types\nof file format.\n\nThe quick brown dog jumped over the\nlazy fox. The quick brown dog jumped\nover the lazy fox. The quick brown dog\njumped over the lazy fox. The quick\nbrown dog jumped over the lazy fox."
  },
  {
    "objectID": "p06-10-ocr-tesseract.html#ocr-với-các-ngôn-ngữ-khác-nhau",
    "href": "p06-10-ocr-tesseract.html#ocr-với-các-ngôn-ngữ-khác-nhau",
    "title": "50  OCR với tesseract",
    "section": "50.2 OCR với các ngôn ngữ khác nhau",
    "text": "50.2 OCR với các ngôn ngữ khác nhau\nTa có thể tham khảo các ngôn ngữ được tesseract hỗ trợ ở đây [https://github.com/tesseract-ocr/tessdata]. Ta có thể download các ngôn ngữ khác với câu lệnh sau.\n\n# Download tiếng Việt\ntesseract_download(\"vie\")\n\nTrong trường hợp không download trực tiếp được từ R, ta có thể download thủ công và copy dữ liệu download vào đường dẫn sau.\n\ntesseract_info()$datapath\n\nLưu ý: file dữ liệu là vie.traineddata trong thư mục data\nKiểm tra thử tiếng Việt.\n\n# tesseract_download(\"vie\")\nvie <- tesseract(\"vie\")\ntext <- tesseract::ocr(\"data/ocr-image-vn.png\", engine = vie)\ncat(text)"
  },
  {
    "objectID": "p06-10-ocr-tesseract.html#xử-lý-file-pdf",
    "href": "p06-10-ocr-tesseract.html#xử-lý-file-pdf",
    "title": "50  OCR với tesseract",
    "section": "50.3 Xử lý file pdf",
    "text": "50.3 Xử lý file pdf\nVới các file pdf đã scan, ta cần chuyển đổi thành dạng png và sau đó xử lý như bình thường. Lưu ý: Hệ thống chữ viết tay được scan vẫn là vấn đề khó khăn lớn trong lĩnh vực OCR.\n\n# Scan pdf file\npngfile <- pdftools::pdf_convert('data/ocr-scan.pdf', dpi = 600)\n\n\ntext1 <- tesseract::ocr(\"data/ocr-scan-01.png\", engine = vie)\ncat(text1)\ntext2 <- tesseract::ocr(\"data/ocr-scan-02.png\", engine = vie)\ncat(text2)"
  },
  {
    "objectID": "p06-10-ocr-tesseract.html#tài-liệu-tham-khảo",
    "href": "p06-10-ocr-tesseract.html#tài-liệu-tham-khảo",
    "title": "50  OCR với tesseract",
    "section": "50.4 Tài liệu tham khảo",
    "text": "50.4 Tài liệu tham khảo\n\nhttps://cran.r-project.org/web/packages/tesseract/vignettes/intro.html"
  },
  {
    "objectID": "p07-02-causal-impact.html#giới-thiệu",
    "href": "p07-02-causal-impact.html#giới-thiệu",
    "title": "51  Causal Impact",
    "section": "51.1 Giới thiệu",
    "text": "51.1 Giới thiệu\nPhương pháp này dùng để phân tích ảnh hưởng của sự kiện đến chuỗi thời gian. Ví dụ:\n\nThay đổi tag name của website ảnh hưởng đến organic traffic như thế nào?\nThay đổi ASO ảnh hưởng đến organic download của app như thế nào?\n\nGiả định (Assumption)\n\nTồn tại cặp biến Y và X, trong đó Y là biến cần đo ảnh hưởng của sự kiện A tại thời điểm t, X là biến tham chiếu (control variable). Mối quan hệ giữa Y & X có các giả định sau:\n\nX không bị ảnh hưởng bởi sự kiện A\nY & X có mối tương quan và “covariate” không đổi theo thời gian\n\nCác khái niệm:\n\nPred-period: Thời gian trước khi xảy ra sự kiện A\nPost-period: Thời gian sau khi xảy ra sự kiện A\nX: Control variable\n\n\nPackages CausalImpact:\n\ndevtools::install_github(\"google/CausalImpact\", force = T)"
  },
  {
    "objectID": "p07-02-causal-impact.html#ví-dụ-1",
    "href": "p07-02-causal-impact.html#ví-dụ-1",
    "title": "51  Causal Impact",
    "section": "51.2 Ví dụ 1",
    "text": "51.2 Ví dụ 1\nTạo dữ liệu với 100 quan sát có tương quan không đổi theo thời gian x và y. Từ quan sát thứ 71, xảy ra sự kiện A khiến y tăng thêm trung bình 10 đơn vị nhưng không ảnh hưởng đến x.\n\nlibrary(dplyr)\nlibrary(CausalImpact)\nset.seed(1)\n#Tạo mô hình ARIMA(0.999,0,0) hay AR(0.999)\nx <- 100 + arima.sim(model = list(ar = 0.999), n = 100)\ny <- 1.2 * x + rnorm(100)\ny[71:100] <- y[71:100] + 10\n#Tạo thời gian\ntime.points <- seq.Date(as.Date(\"2016-01-01\"), by = 1, length.out = 100)\ndata <- zoo(cbind(y, x),time.points)\nhead(data)\ntail(data)\nmatplot(data, type = \"l\")\n#Xác định thời gian xảy ra sự kiện\npre.period <- as.Date(c(\"2016-01-01\", \"2016-03-11\"))\npost.period <- as.Date(c(\"2016-03-12\", \"2016-04-09\"))\n#Chạy mô hình\nimpact <- CausalImpact(data, pre.period, post.period)\nplot(impact)\nsummary(impact)\nsummary(impact, \"report\")\n\nGiải thích\n\nGiá trị trung bình của chuỗi y đo được là 117 trong khi dự báo nếu không xảy ra A, y chỉ có gia trị 107\nẢnh hưởng (effect) của A lên y là 10 đơn vị/ngày. Trong toàn bộ giai đoạn là 294"
  },
  {
    "objectID": "p07-02-causal-impact.html#ví-dụ-2",
    "href": "p07-02-causal-impact.html#ví-dụ-2",
    "title": "51  Causal Impact",
    "section": "51.3 Ví dụ 2",
    "text": "51.3 Ví dụ 2\n\nx <- 100 + arima.sim(model = list(ar = 0.999), n = 120)\nx[101:120] <- 110 + arima.sim(model = list(ar = 0.999), n = 20)\nplot(x)\ny <- 1.2 * x + rnorm(120)\ny[101:120] <- 1.1*y[101:120] + 10\ndata <- cbind(y, x)\npre.period <- c(1, 100)\npost.period <- c(101, 120)\nimpact <- CausalImpact(data, pre.period, post.period)\nplot(impact)"
  },
  {
    "objectID": "p07-02-causal-impact.html#tài-liệu-tham-khảo",
    "href": "p07-02-causal-impact.html#tài-liệu-tham-khảo",
    "title": "51  Causal Impact",
    "section": "51.4 Tài liệu tham khảo",
    "text": "51.4 Tài liệu tham khảo\n\nGoogle Causal Impact"
  },
  {
    "objectID": "p07-03-collaborative-filtering.html#giới-thiệu",
    "href": "p07-03-collaborative-filtering.html#giới-thiệu",
    "title": "52  Collaborative Filtering",
    "section": "52.1 Giới thiệu",
    "text": "52.1 Giới thiệu\nKhi sử dụng internet chúng ta có thể gặp rất nhiều hiện tượng như sau:\n\nKhi kết thúc 1 video, Youtube tự động chuyển sang 1 video khác mà khả năng cao người dùng sẽ thích\nFacebook suggest những quảng cáo dựa trên hành vi của người dùng\nFacebook gợi ý kết bạn\nKhi mua hàng trên Amazon hệ thống sẽ gợi ý những món hành có thể mua cùng (Frequently bought together)\nFacebook gợi ý kết bạn\n\nBằng cách quảng cáo sản phẩm hướng vào các đối tượng tiềm năng sẽ làm cho hiệu quả của Marketing tăng lên cũng như hiệu quả của kinh doanh. Thuật toán đứng đằng sau những ứng dụng như này có tên gọi chung là Recommendation Systems. Recommendation Systems có tuổi đời khá trẻ so với các thuật toán Machine Learning khác vì nó xuất hiện từ nhu cầu bùng nổ của Internet cách đây 10-15 năm trước.\nCác Recommendation Systems thường được chia thành 2 nhóm lớn:\n\nContent-based systems: hệ thống sẽ recommend dựa trên đặc tính của Item. VD: một User xem rất nhiều video trên Youtube thể loại nhạc trẻ Việt của Sơn Tùng thì hệ thống Recommendation Systems sẽ gợi ý cho Users này một số video cùng thể loại như Chạy ngay đi. Đặc điểm của Content-based Recommendation Systems là việc xây dựng mô hình cho mỗi user không phụ thuộc vào các users khác mà phụ thuộc vào profile của mỗi items. Việc làm này có lợi thế là tiết kiệm bộ nhớ và thời gian tính toán. Cách tiếp cận này yêu cầu việc sắp xếp các Item vào các nhóm rõ ràng, tuy nhiên nếu Item không có nhóm cụ thể hoặc khó xác định nhóm thì việc sử dụng Content-based Systems là không thể\nNeighborhood-based Collaborative Filtering (Khi nói về Collaborarative Filtering ta có thể ngầm hiểu đây là phương pháp NBCF): Ý tưởng cơ bản của NBCF là xác định mức độ quan tâm của một user tới một item dựa trên các users khác gần giống với user này. Việc gần giống nhau giữa các users có thể được xác định thông qua mức độ quan tâm của các users này tới các items khác mà hệ thống đã biết. VD: A,T,N đều thích xem video về nhạc trẻ, ngoài ra T và A thích xem video của Chim sẻ đi nắng. Dựa trên thông tin của T và A, hệ thống sẽ tính toán sự tương quan giữa 3 users và dự đoán rằng N sẽ thích xem video của Chim Sẻ Đi Nắng và gợi ý cho người dùng này\n\nTrong bài viết này ta sẽ đi vào nghiên cứu Collaborative Filtering do có tính ứng dụng cao hơn trong thực tế"
  },
  {
    "objectID": "p07-03-collaborative-filtering.html#phương-pháp",
    "href": "p07-03-collaborative-filtering.html#phương-pháp",
    "title": "52  Collaborative Filtering",
    "section": "52.2 Phương pháp",
    "text": "52.2 Phương pháp\nCollaborative Filtering được chia thành 2 nhóm: User-based CF và Item-based CF\n\n52.2.1 User-user CF\nPhương pháp này sẽ tính toán sự tương quan giữa các users với nhau (similarity).\nCác bước tính toán:\n\nBước 1: Xây dựng ma trận rating matrix dựa trên mức độ đánh giá các bộ phim của các khách hàng này\n\n\n\n\n\nBước 2: Normalise dữ liệu rating: Việc đầu tiên CF sẽ tính trung bình các rating của từng User, sau đó sẽ tính giá trị giữa các các rating đến các giá trị trung bình này. Bản chất của việc normalise dữ liệu đến từ việc tính cách của khách hàng luôn khác nhau (có những người khi xem bộ phim thấy rất hay sẽ đánh giá 5đ, tuy nhiên có những người khó tính hơn chỉ đánh giá 4). Do vậy, khi tính toán trung bình sẽ mang lại mức đánh giá là tốt nhất khi những giá trị lớn hơn trung bình là bộ phim khách hàng thích còn dưới trung bình là không thích.\nBước 3: Tính toán Similarities giữa các Users và Items : Phương pháp thường được sử dụng là Cosine Similarity giữa các Users với nhau. Giá trị của Cosin Similarity giữa các Users sẽ trong khoảng [-1,1]. Giá trị 1 thể hiện 2 vector hoàn toàn tương đồng với nhau, với - 1 sẽ thể hiện ngược lại qua đó sẽ tạo ra ma trận Similarity. Từ ma trận Similarity ta sẽ biết những Users nào có hành vi giống với Users nào.\nBước 4: Sử dụng concept của phương pháp K-nearest neighbor để dự báo giá trị rating bị missing. CF sẽ lựa chọn k Users có mức độ similarity cao nhất (tức các khách hàng có hành vi giống nhất)\nBước 5: Tính giá trị dự báo rating items dựa trên trung bình điều hòa giá trị similarity và rating của các k users giống nó nhất\n\n\n\n52.2.2 Item-Item CF\nPhương pháp này sẽ tính toán sự tương quan giữa các Item với nhau (similarity). Các bước tính toán tương tự tuy nhiên sẽ thực hiện việc tính toán similarity theo các Item với nhau"
  },
  {
    "objectID": "p07-03-collaborative-filtering.html#thực-hành",
    "href": "p07-03-collaborative-filtering.html#thực-hành",
    "title": "52  Collaborative Filtering",
    "section": "52.3 Thực hành",
    "text": "52.3 Thực hành\nData được sử dụng là MovieLense ghi nhận lịch sử rating phim của 943 khách hàng với 1664 bộ phim. Các bộ phim được rating sẽ có giá trị từ 1 đến 5. Mục tiêu của bài thực hành là xây dựng và kiểm định recommendation model để suggest cho khách hàng nên xem bộ phim gì tiếp theo.\n\nBước 1: Chia bộ data thành 2 phần: Train (chiếm 90% dữ liệu), Test (10% còn lại). Ở đây ta sẽ chia Test thành 2 data nhỏ là Test_known dùng để predict và Test_unknown dùng để kiểm định sự chính xác của mô hình\n\n\nlibrary(recommenderlab)\nrm(list = ls())\ndata(MovieLense)\nMovieLense\n\n943 x 1664 rating matrix of class 'realRatingMatrix' with 99392 ratings.\n\nMovieLenseMeta[1:5,1:10]\n\n              title year                                                    url\n1  Toy Story (1995) 1995  http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)\n2  GoldenEye (1995) 1995    http://us.imdb.com/M/title-exact?GoldenEye%20(1995)\n3 Four Rooms (1995) 1995 http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)\n4 Get Shorty (1995) 1995 http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)\n5    Copycat (1995) 1995      http://us.imdb.com/M/title-exact?Copycat%20(1995)\n  unknown Action Adventure Animation Children's Comedy Crime\n1       0      0         0         1          1      1     0\n2       0      1         1         0          0      0     0\n3       0      0         0         0          0      0     0\n4       0      1         0         0          0      1     0\n5       0      0         0         0          0      0     1\n\nevaluation_value <- evaluationScheme(MovieLense, \n                                     method = \"split\", \n                                     train = 0.9,\n                                     given = 12)\ntrain <- getData(evaluation_value, \"train\")\ntest_known <- getData(evaluation_value, \"known\")\ntest_unknown <- getData(evaluation_value, \"unknown\")\n\n\nBước 2: Xây dựng mô hình CF dựa trên tập Train và Predict trên tập Test_known\n\n\nubcf_model <- Recommender(train, \n                        \"UBCF\",\n                        param = list(method = \"Cosine\", nn = 50))\npred_ubcf <- predict(ubcf_model, test_known, type=\"ratings\") \nas(pred_ubcf, \"matrix\")[,1:5]\n\n    Toy Story (1995) GoldenEye (1995) Four Rooms (1995) Get Shorty (1995)\n7           4.543528         4.772981          3.353097          4.131824\n19          3.685692               NA                NA                NA\n22          3.167089         2.311927          2.364524          2.823718\n29          3.740471         3.267218          3.038683          3.600142\n30          4.238410               NA                NA                NA\n50          3.412825         4.062730          2.788885          4.499896\n62          3.542684         2.919052          4.604978                NA\n68          3.714204         3.178541                NA          3.940187\n69          3.621213         3.661522                NA          4.445255\n76          3.660263         3.766094          3.014380          3.476181\n85          3.942974         3.658348          2.648327          3.340485\n89          4.130466         4.024194                NA          4.546296\n96                NA         5.338936          4.647617          4.901235\n101               NA         2.484458                NA          2.636576\n109         2.901133         3.213083          2.564042          2.525700\n114         4.078156         3.095387          3.853506          3.478724\n127         4.256744         2.462963          2.428553          3.734947\n132         3.403300         3.413847                NA          3.268841\n205         3.353220         3.487289          2.168619          2.908384\n211         3.589176         3.201258          2.364774          3.482136\n232         4.041246               NA                NA          3.982559\n236         3.835126         3.270642          2.802515          3.270642\n242               NA         3.754213                NA          4.235629\n280               NA               NA          3.669040          3.450591\n287         4.427634         4.212963                NA          4.348765\n299         3.796863         3.286966                NA          3.638302\n311         3.504586         3.107527          2.608515          2.946214\n338         4.047676               NA          3.820217                NA\n344               NA         3.493827          3.627771          2.343759\n349         3.535746         2.941395                NA          2.950841\n363         2.816105               NA                NA          2.932524\n379         4.376821         3.726353          3.769332          3.641275\n389         4.022727         2.891045          2.740000          3.495590\n392         5.050338         4.824731          4.334053          4.627320\n394         4.451410               NA          3.659336          4.104624\n399               NA         2.679437                NA          2.639015\n402         4.470856         4.206140          3.904372          2.785501\n416         3.976141         2.253030          2.500000          2.341104\n422               NA         3.932761                NA          3.675838\n424               NA               NA          1.812189          3.525281\n440         4.148104         3.545006          2.811464          4.278845\n444         4.200120         3.448535          1.903048          3.793380\n445         2.319939         1.715646                NA          1.627242\n467         3.972681         3.417767          3.478065          3.859165\n472         4.236698         4.190860          3.966878          3.891351\n474         4.080915         3.270919          3.531283          3.996482\n488         3.424169         3.353976          2.876612          3.333546\n496         3.343741         2.743133          1.958333          2.775089\n501         3.934577         3.747607          3.807140          3.182489\n503         4.720824               NA          4.175000                NA\n510         3.328534         2.722064          2.293518          3.030214\n511         4.123730         3.522961          3.219811          3.893733\n523         4.694358               NA          3.615010          2.847222\n535         4.169381               NA          3.666450          3.638344\n537         3.156909         3.052164          2.295833          3.176452\n548         3.591550         2.937309                NA          2.699887\n551         3.309823         2.856693          1.919733          3.752890\n553         4.244671               NA          3.760551          3.485748\n556         4.425683         3.340580          3.406667          4.146386\n559         3.800216         4.005602                NA                NA\n560         3.906907         4.166024          1.705128          3.827183\n562         3.761696               NA          2.986800          2.249300\n564         3.634958         3.521222          4.050926          3.411416\n592         3.899320         3.706140          2.331059          3.655390\n597               NA         3.452090          4.049462          4.173419\n604         3.462162         2.979136          1.458952          2.175421\n607         4.057029         3.398252          3.335879          4.052098\n610         3.716197         3.682423          2.848495          3.006559\n626         2.727959         2.289127                NA          1.845611\n627         3.122037         2.959048          2.509042          1.586665\n631         3.209021         3.097027          2.886561          3.843011\n660         2.577523         2.357527          2.382779          2.060272\n682         3.296970         2.446390          1.953333          2.565298\n688         4.876648         4.593558          4.155557          4.784818\n714         3.648914         3.383620                NA          4.458953\n716         3.881929         4.582287          3.647497                NA\n720         4.365464         3.666711          2.800182          4.244592\n747         4.113946         3.805907          4.091304          4.071564\n753         3.992889               NA          3.379167          3.517040\n766         3.433891         2.986252          3.526502          3.145142\n790         3.672034         2.702952          4.688312          3.312156\n803         4.131312         4.160262          4.189972          3.957532\n817         2.709073         2.876344                NA          3.123732\n825         3.785102         3.248892          4.217593          3.552232\n838               NA               NA          3.800267          4.884146\n841         4.042908         3.575553          3.929379          4.060529\n856         3.792176         3.612300          2.734099          3.384836\n867         4.584611               NA          3.987705          4.484568\n874         3.897123         3.321777                NA          3.577232\n885         3.516869         3.166667          3.649282          3.971139\n893         3.573697         4.856557          1.621795          3.590879\n902         3.730849               NA          1.455128          4.252971\n907         4.790137               NA          3.656667          4.065432\n914         3.206660         3.000610          2.535938          3.407978\n924               NA         3.405058          3.237705          3.533469\n    Copycat (1995)\n7         4.632681\n19        3.189655\n22        3.217054\n29        4.119042\n30        2.433300\n50        3.337443\n62        2.827401\n68        4.202631\n69        4.514269\n76        3.558533\n85        3.515593\n89        4.116558\n96              NA\n101       2.905541\n109       2.454545\n114       3.865428\n127       3.592298\n132       3.159868\n205       2.452165\n211       3.179599\n232       3.627543\n236       3.204545\n242       5.060548\n280       4.117191\n287       3.787879\n299       3.136096\n311       2.883257\n338       3.696970\n344       2.004781\n349       3.444795\n363       2.823389\n379       3.940508\n389       2.930151\n392       4.671111\n394             NA\n399       2.016449\n402       3.832646\n416       3.461267\n422       1.004695\n424       2.954545\n440       3.711980\n444       3.737463\n445       2.272019\n467       4.227221\n472       3.773188\n474       3.454545\n488       1.519424\n496       3.565133\n501       3.868294\n503       3.911364\n510       2.398139\n511       3.922957\n523             NA\n535       3.686720\n537       2.905881\n548       2.871212\n551       3.373131\n553       4.009804\n556       2.986985\n559       3.030303\n560       3.844176\n562             NA\n564       3.156960\n592       3.841667\n597       4.012855\n604             NA\n607             NA\n610       3.811387\n626       2.042965\n627       2.676471\n631       3.320012\n660       1.681628\n682       0.906879\n688       4.674694\n714       4.321562\n716       3.759804\n720       3.534661\n747       3.805907\n753       3.770053\n766       1.956460\n790       3.494889\n803       3.730140\n817       2.863636\n825       3.509804\n838       4.266207\n841       3.945761\n856       3.362760\n867       4.278571\n874       3.938062\n885       3.063993\n893             NA\n902             NA\n907             NA\n914       1.890273\n924       3.778709\n\n\n\nBước 3: Kiểm định mô hình trên tập Test_unknown\n\n\nacc_ubcf <- calcPredictionAccuracy(pred_ubcf, test_unknown)\nas(acc_ubcf,\"matrix\")\n\n          [,1]\nRMSE 1.1541052\nMSE  1.3319587\nMAE  0.9121051\n\n\n\nBước 4: Tiếp theo ta sẽ sử dựng mô hình Item based CF để tính toán các chỉ số accuracy\n\n\nibcf_model <- Recommender(train, \n                        \"IBCF\",\n                        param = list(method = \"Cosine\", nn = 50))\n\nAvailable parameter (with default values):\nk    =  30\nmethod   =  cosine\nnormalize    =  center\nnormalize_sim_matrix     =  FALSE\nalpha    =  0.5\nna_as_zero   =  FALSE\nverbose  =  FALSE\n\npred_ibcf <- predict(ibcf_model, test_known, type=\"ratings\") \nacc_ibcf <- calcPredictionAccuracy(pred_ibcf, test_unknown)\nas(acc_ibcf,\"matrix\")\n\n         [,1]\nRMSE 1.676796\nMSE  2.811644\nMAE  1.212329\n\nacc <- rbind(UBCF = acc_ubcf, IBCF = acc_ibcf)\nacc\n\n         RMSE      MSE       MAE\nUBCF 1.154105 1.331959 0.9121051\nIBCF 1.676796 2.811644 1.2123288\n\n\n\nBước 5: Sau khi thấy được mô hình User basd CF có chất lượng tốt hơn ta sẽ áp dụng mô hình này để đưa gợi ý khách hàng xem 5 bộ phim khác\n\n\nrecommended_next_film <- predict(ibcf_model, MovieLense[1:10,], n=5)\nas(recommended_next_film, \"list\")\n\n$`0`\n[1] \"Entertaining Angels: The Dorothy Day Story (1996)\"\n[2] \"Big Bang Theory, The (1994)\"                      \n[3] \"Visitors, The (Visiteurs, Les) (1993)\"            \n[4] \"They Made Me a Criminal (1939)\"                   \n[5] \"New York Cop (1996)\"                              \n\n$`1`\n[1] \"They Made Me a Criminal (1939)\"                   \n[2] \"New York Cop (1996)\"                              \n[3] \"Entertaining Angels: The Dorothy Day Story (1996)\"\n[4] \"Big Bang Theory, The (1994)\"                      \n[5] \"Other Voices, Other Rooms (1997)\"                 \n\n$`2`\n[1] \"Entertaining Angels: The Dorothy Day Story (1996)\"\n[2] \"New York Cop (1996)\"                              \n[3] \"Legal Deceit (1997)\"                              \n[4] \"Other Voices, Other Rooms (1997)\"                 \n[5] \"Hush (1998)\"                                      \n\n$`3`\n[1] \"Men of Means (1998)\"                              \n[2] \"Entertaining Angels: The Dorothy Day Story (1996)\"\n[3] \"Mirage (1995)\"                                    \n[4] \"Visitors, The (Visiteurs, Les) (1993)\"            \n[5] \"Further Gesture, A (1996)\"                        \n\n$`4`\n[1] \"They Made Me a Criminal (1939)\"                   \n[2] \"Coldblooded (1995)\"                               \n[3] \"Visitors, The (Visiteurs, Les) (1993)\"            \n[4] \"Entertaining Angels: The Dorothy Day Story (1996)\"\n[5] \"King of New York (1990)\"                          \n\n$`5`\n[1] \"They Made Me a Criminal (1939)\"                           \n[2] \"I Don't Want to Talk About It (De eso no se habla) (1993)\"\n[3] \"Daniel Defoe's Robinson Crusoe (1996)\"                    \n[4] \"Very Natural Thing, A (1974)\"                             \n[5] \"Walk in the Sun, A (1945)\"                                \n\n$`6`\n[1] \"Beans of Egypt, Maine, The (1994)\" \"Death in Brunswick (1991)\"        \n[3] \"New York Cop (1996)\"               \"All Things Fair (1996)\"           \n[5] \"They Made Me a Criminal (1939)\"   \n\n$`7`\n[1] \"Marlene Dietrich: Shadow and Light (1996) \"       \n[2] \"All Things Fair (1996)\"                           \n[3] \"Entertaining Angels: The Dorothy Day Story (1996)\"\n[4] \"New York Cop (1996)\"                              \n[5] \"Visitors, The (Visiteurs, Les) (1993)\"            \n\n$`8`\n[1] \"Daniel Defoe's Robinson Crusoe (1996)\"\n[2] \"Mat' i syn (1997)\"                    \n[3] \"B. Monkey (1998)\"                     \n[4] \"They Made Me a Criminal (1939)\"       \n[5] \"Other Voices, Other Rooms (1997)\"     \n\n$`9`\n[1] \"Legal Deceit (1997)\"                       \n[2] \"Scarlet Letter, The (1926)\"                \n[3] \"They Made Me a Criminal (1939)\"            \n[4] \"Marlene Dietrich: Shadow and Light (1996) \"\n[5] \"New York Cop (1996)\""
  },
  {
    "objectID": "p07-04-survival-analysis.html#giới-thiệu",
    "href": "p07-04-survival-analysis.html#giới-thiệu",
    "title": "53  Survival Analysis",
    "section": "53.1 Giới thiệu",
    "text": "53.1 Giới thiệu\nSurvival analysis được sử dụng trong phân tích thời gian xảy ra sự kiện. VD:\n\nThời gian chuyển sang AIDS của bệnh nhâ HIV\nThời gian khách hàng ngưng sử dụng dịch vụ sau khi đăng ký\n\nBiến được sử dụng trong phân tích là thời gian.\nĐối với survival analysis, dữ liệu có các đặc trưng sau:\n\nBiến phụ thuộc là biến thời gian\nDữ liệu có đặc trưng censor.\n\nRight censoring là thuật ngữ mô tả sự kiện diễn ra sau khoảng thời gian phân tích. Ví dụ, nghiên cứ 100 bệnh nhân HIV trong khoảng thời gian 1 năm. Trong thời gian đó, có 10 bệnh nhân chuyển qua AIDS, 90 bệnh nhân còn lại chưa qua AIDS trong một năm đó nhưng ta đều biết rằng họ SẼ chuyển thành AIDS. Dữ liệu kiểu vậy được gọi là right censoring. Thời gian bệnh nhân CHÍNH XÁC chuyển qua AIDS nằm ngoài khoảng thời gian quan sát.\nLeft censoring: Thời gian diễn ra sự kiện trước khoảng thời gian diễn ra quan sát\n\n\nĐối với các vấn đề như trên, việc dự báo họ chuyển thành AIDS hay không không quan trọng. Quan trọng là KHI NÀO họ chuyển thành AIDS.\nLưu ý:\n\nĐối với survival analysis, ta thường không có đủ dữ liệu về đối tượng. VD: Trong 100 bệnh nhân tham gia, có bệnh nhân vào nghiên cứu từ tháng 6, tức là khi kết thúc nghiên cứu, bệnh nhân đó chỉ có dữ liệu 6 tháng\nNếu ta có đầy đủ dữ liệu vè thời gian khách hàng/bệnh nhân từ khi quan sát đến khi xảy ra sự kiện, ta có thể dùng regression thông thường"
  },
  {
    "objectID": "p07-04-survival-analysis.html#lý-thuyết",
    "href": "p07-04-survival-analysis.html#lý-thuyết",
    "title": "53  Survival Analysis",
    "section": "53.2 Lý thuyết",
    "text": "53.2 Lý thuyết\nGọi T là thời gian xảy ra sự kiện từ khi quan sát, với cdf F(.) và pdf f(.), ta có:\n\\(F(t) = P(T \\leq t) = \\int_0^tf(x)dx\\) và \\(f(t) = \\frac{dF(t)}{dt}\\)\n\\[f(t)=\\lim_{dt \\to 0^+} \\frac{F(t + dt) - F(t)}{dt} =\n\\lim_{dt \\to 0^+} \\frac{P(t < t \\leq t + dt)}{dt}\\]\nSurvival function S(t):\n\\[S(t) = P(T > t) = 1 - F(t) = \\int_t^{\\infty}f(x)dx\\]\nVới t = 0, S(t) = 1. Với t tiến đến \\(\\infty\\), S(t) tiến đến 0\nBởi vì:\n\\[F(x) = 1 - S(x)\\]\nĐạo hàm hai vế, ta được:\n\\[f(t) = -\\frac{dS}{dt}\\]\n**Hàm nguy cơ \\(h(t)\\) hay \\(\\lambda(t)\\) (hazard function) cho ta biết xác suất xảy ra sự kiện tại ngay thời điểm t.\n\\[h(t) = \\lim_{dt \\to 0}\\frac{(t < T \\leq t + dt | T > t)}{dt} =\n\\lim_{dt \\to 0}\\frac{F(t+dt)}{S(t)dt}=\\frac{f(t)}{S(t)} =\n\\frac{-dS(t)/dt}{S(t)} = -\\frac{dlog(S(t))}{dt}\\]\nHàm nguy cơ tích lũy (cumulative hazard function):\n\\[H(t) = \\int_0^th(u)du = -log(S(t))\\]\nTừ đó ta có:\n\\[S(t) = e^{-H(t)} = exp(-H(t))\\]\nLưu ý:\n\nHàm survival S(t) cho ta biết xác suất tích lũy sống qua thời điểm xảy ra sự kiện T\nHàm F(t) cho ta biết xác suất tích lũy chỉ sống đến thời điểm T\nHàm nguy cơ h(t) cho ta biết xác suất xảy ra sự kiện NGAY tại thời điểm T.\n\nVD: Xác suất người chết ở tuối 100 rất thấp vì rất ít người sống đến 100 (F(t)). Tuy nhiên, xác suất người chết ở tuổi 100 với điều kiện người đó ĐÃ bước sang tuổi 100 sẽ cao hơn rất nhiều\n\n53.2.1 Ước lượng Kaplan-Meier cho F(t)\nXác suất xảy ra hai sự kiện A1 và A2 đồng thời như sau:\n\\[P(A_1 \\cap A_2) = P(A_2|A_1)*P(A_1)\\]\nDo đó, hàm survival (S) cho ta biết xác suất các đối tượng sống sót qua thời gian t. Bản chất của hàm Survival là xác suất có điều kiện như sau:\nP(sống qua t + 1) = P(sống qua t)*P(sống qua t+1| sống qua t)\n\\(\\hat{S}_t=\\prod_{k=1}^{t}{p_k}\\)\n\\[\\hat{p_i} = 1 - \\hat{q_i} =\n1 - \\frac{d_i}{n_i} =\n\\frac{n_i-d_i}{n_i}\\]\nTrong đó:\n\n\\(\\hat{p_i}\\) là xác suất sống sót sau thời điểm i\n\\(n_i\\) là số đối tượng quan sát tại khung thời gian i\n\\(d_i\\) là số đối tượng trải qua sự kiện tại thời điểm i\n\\(\\hat{q_i}\\) là xác suất các đối tượng trải qua sự kiện tại thời điểm i\n\nVí dụ:\n\nLưu ý:\n\nĐường ước lượng KM chỉ dốc xuống khi có sự kiện xảy ra\nSau thời điểm t = 48, ta không thể ước lượng S(t) được nữa, đường \\(\\hat{S}(t)\\) này được gọi là defective survival function\n\n\n\n53.2.2 Ước lượng hàm nguy cơ\n\\[\\tilde{h}(t_i) = \\frac{d_i}{n_i}\\]\nTrong khoảng thời gian từ \\(t_i \\leq T \\leq t_{i+1}\\), với i là khoảng thời gian xảy ra sự kiện, ta có:\n\\[\\hat{h}_t = \\frac{d_i}{n_i(t_{i+1}-t_i)}\\]\nVới ví dụ AML, ta có:\n\\[\\hat{h}_{23} = \\frac{1}{7}\\] \\[\\hat{h}_{28} = \\hat{h}_{30}=\\frac{1}{7 * (31-23)} = \\frac{1}{56}\\]\nHàm nguy cơ tích lũy\n\\[\\hat{H}(t) = -log(\\hat{S}(t))\\]\nVới ví dụ AML:\n\\[\\hat{H}_{26} = -log(\\hat{S}(26)) = -log(0.614) = 0.488\\]\n\\[\\tilde{H}(26) = \\frac{1}{11} + \\frac{1}{10} + \\frac{1}{8} +\n\\frac{1}{7} = 0.4588\\]\nLưu ý:\n\nTrong survival analysis, các sự kiện được gọi là death, nhưng có thể có rất nhiều cách tiếp cận khác nhau: thời gian mua sản phẩm thứ 2 sau sản phẩm thứ nhất, thời gian KH rời bỏ doanh nghiệp…\n\nƯớc lượng này được gọi là ước lượng Kaplan-Meier hay product limit estimate\n\nHàm nguy cơ (h hay ) cho ta biết xác suất xảy ra biến cố tại thời điểm t.\n\nGiả sử có 18 bệnh nhân như sau:\n\nDấu * cho biết bệnh nhân vẫn tiếp tục sử dụng thiết bị\n\nGiải thích:\n\nỞ thời gian t=2 (tuần 10-18), có một người ngưng sử dụng, xác suất ngưng sử dụng là \\(\\frac{1}{18}\\). Xác suất còn sử dụng p là \\(\\frac{17}{18}\\). Xác suất sống sót t = 2 là \\(1*\\frac{17}{18}=\\frac{17}{18}=0.9445\\)\nỞ thời gian t=3 (tuần 19-29), có một người ngưng sử dụng, xác suất ngưng sử dụng là \\(\\frac{1}{15}\\). Xác suất còn sử dụng là \\(\\frac{14}{15}\\). Xác suất sống sót của tất cả sample qua t = 3 là \\(\\frac{14}{15} * \\frac{17}{18}\\)"
  },
  {
    "objectID": "p07-04-survival-analysis.html#thực-hành",
    "href": "p07-04-survival-analysis.html#thực-hành",
    "title": "53  Survival Analysis",
    "section": "53.3 Thực hành",
    "text": "53.3 Thực hành\n\nlibrary(dplyr)\nlibrary(survival)\nlibrary(KMsurv)\nlibrary(ggfortify)\nlibrary(ISwR)\ntheme_set(theme_minimal())\n\ndata(melanom)\nmelanom %>% head\n\n   no status days ulc thick sex\n1 789      3   10   1   676   2\n2  13      3   30   2    65   2\n3  97      2   35   2   134   2\n4  16      3   99   2   290   1\n5  21      1  185   1  1208   2\n6 469      1  204   1   484   2\n\n#Tạo status == 1 là death\nmsurv <- with(melanom, Surv(days, status == 1))\n\n#Lưu ý: Các status là 2 và 3 được coi như uncensored\nmelanom %>% head\n\n   no status days ulc thick sex\n1 789      3   10   1   676   2\n2  13      3   30   2    65   2\n3  97      2   35   2   134   2\n4  16      3   99   2   290   1\n5  21      1  185   1  1208   2\n6 469      1  204   1   484   2\n\nmsurv %>% head\n\n[1]  10+  30+  35+  99+ 185  204 \n\n#Phân tích\nmfit <- survfit(Surv(days, status == 1) ~ 1, data = melanom)\nmfit %>% summary\n\nCall: survfit(formula = Surv(days, status == 1) ~ 1, data = melanom)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  185    201       1    0.995 0.00496        0.985        1.000\n  204    200       1    0.990 0.00700        0.976        1.000\n  210    199       1    0.985 0.00855        0.968        1.000\n  232    198       1    0.980 0.00985        0.961        1.000\n  279    196       1    0.975 0.01100        0.954        0.997\n  295    195       1    0.970 0.01202        0.947        0.994\n  386    193       1    0.965 0.01297        0.940        0.991\n  426    192       1    0.960 0.01384        0.933        0.988\n  469    191       1    0.955 0.01465        0.927        0.984\n  529    189       1    0.950 0.01542        0.920        0.981\n  621    188       1    0.945 0.01615        0.914        0.977\n  629    187       1    0.940 0.01683        0.907        0.973\n  659    186       1    0.935 0.01748        0.901        0.970\n  667    185       1    0.930 0.01811        0.895        0.966\n  718    184       1    0.925 0.01870        0.889        0.962\n  752    183       1    0.920 0.01927        0.883        0.958\n  779    182       1    0.915 0.01981        0.877        0.954\n  793    181       1    0.910 0.02034        0.871        0.950\n  817    180       1    0.904 0.02084        0.865        0.946\n  833    178       1    0.899 0.02134        0.859        0.942\n  858    177       1    0.894 0.02181        0.853        0.938\n  869    176       1    0.889 0.02227        0.847        0.934\n  872    175       1    0.884 0.02272        0.841        0.930\n  967    174       1    0.879 0.02315        0.835        0.926\n  977    173       1    0.874 0.02357        0.829        0.921\n  982    172       1    0.869 0.02397        0.823        0.917\n 1041    171       1    0.864 0.02436        0.817        0.913\n 1055    170       1    0.859 0.02474        0.812        0.909\n 1062    169       1    0.854 0.02511        0.806        0.904\n 1075    168       1    0.849 0.02547        0.800        0.900\n 1156    167       1    0.844 0.02582        0.794        0.896\n 1228    166       1    0.838 0.02616        0.789        0.891\n 1252    165       1    0.833 0.02649        0.783        0.887\n 1271    164       1    0.828 0.02681        0.777        0.883\n 1312    163       1    0.823 0.02713        0.772        0.878\n 1435    161       1    0.818 0.02744        0.766        0.874\n 1506    159       1    0.813 0.02774        0.760        0.869\n 1516    155       1    0.808 0.02805        0.755        0.865\n 1548    152       1    0.802 0.02837        0.749        0.860\n 1560    150       1    0.797 0.02868        0.743        0.855\n 1584    148       1    0.792 0.02899        0.737        0.851\n 1621    146       1    0.786 0.02929        0.731        0.846\n 1667    137       1    0.780 0.02963        0.725        0.841\n 1690    134       1    0.775 0.02998        0.718        0.836\n 1726    131       1    0.769 0.03033        0.712        0.831\n 1933    110       1    0.762 0.03085        0.704        0.825\n 2061     95       1    0.754 0.03155        0.694        0.818\n 2062     94       1    0.746 0.03221        0.685        0.812\n 2103     90       1    0.737 0.03290        0.676        0.805\n 2108     88       1    0.729 0.03358        0.666        0.798\n 2256     80       1    0.720 0.03438        0.656        0.791\n 2388     75       1    0.710 0.03523        0.645        0.783\n 2467     69       1    0.700 0.03619        0.633        0.775\n 2565     63       1    0.689 0.03729        0.620        0.766\n 2782     57       1    0.677 0.03854        0.605        0.757\n 3042     52       1    0.664 0.03994        0.590        0.747\n 3338     35       1    0.645 0.04307        0.566        0.735\n\nmfit %>% \n  autoplot + \n  scale_x_continuous(breaks = seq(0, 6000, 1000)) +\n  labs(title = \"Survival function\")\n\n\n\n\n\n53.3.1 Survival function theo giới tính\n\nmfit.bysex <- survfit(Surv(days, status == 1) ~ sex, \n                      data = melanom)\nmfit.bysex\n\nCall: survfit(formula = Surv(days, status == 1) ~ sex, data = melanom)\n\n        n events median 0.95LCL 0.95UCL\nsex=1 126     28     NA      NA      NA\nsex=2  79     29     NA    2388      NA\n\nmfit.bysex %>% \n  autoplot() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\nmfit.bysex %>% \n  autoplot(fun = \"cloglog\")\n\n\n\n\nLưu ý: Các điểm * trên đồ thị là các sự kiện xảy ra thực tế\n\n\n53.3.2 So sánh hàm sống sót của 2 nhóm\n\n\\(H_0\\): \\(S_1(.) = S_2(.)\\)\n\\(H_1\\): \\(S_1(.) \\neq S_2(.)\\)\n\n\nsurvdiff(Surv(time,status) ~ x, data=aml)\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ x, data = aml)\n\n                 N Observed Expected (O-E)^2/E (O-E)^2/V\nx=Maintained    11        7    10.69      1.27       3.4\nx=Nonmaintained 12       11     7.31      1.86       3.4\n\n Chisq= 3.4  on 1 degrees of freedom, p= 0.07 \n\n\nVới p < 0.05, ta giữ lại \\(H_0\\)"
  },
  {
    "objectID": "p07-04-survival-analysis.html#phương-pháp-parametric",
    "href": "p07-04-survival-analysis.html#phương-pháp-parametric",
    "title": "53  Survival Analysis",
    "section": "53.4 Phương pháp parametric",
    "text": "53.4 Phương pháp parametric\nPhương trình Cox:\n\\[h(t) = \\lambda(t)e^{\\beta_1x_1 + ... + \\beta_nx_n}\\]\nTrong đó:\n\n\\(\\lambda(t)\\) là hàm nguy cơ nếu không tính đến các ảnh hưởng của x (còn được gọi là baseline hazard function)\n\n\nmelanom %>% head\n\n   no status days ulc thick sex\n1 789      3   10   1   676   2\n2  13      3   30   2    65   2\n3  97      2   35   2   134   2\n4  16      3   99   2   290   1\n5  21      1  185   1  1208   2\n6 469      1  204   1   484   2\n\ncox.model <- coxph(Surv(days, status == 1) ~ sex + ulc, \n                   data = melanom)\ncox.model %>% summary\n\nCall:\ncoxph(formula = Surv(days, status == 1) ~ sex + ulc, data = melanom)\n\n  n= 205, number of events= 57 \n\n       coef exp(coef) se(coef)      z Pr(>|z|)    \nsex  0.5165    1.6761   0.2667  1.937   0.0528 .  \nulc -1.4180    0.2422   0.2969 -4.775 1.79e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\nsex    1.6761     0.5966    0.9938    2.8268\nulc    0.2422     4.1289    0.1353    0.4334\n\nConcordance= 0.719  (se = 0.033 )\nLikelihood ratio test= 32.16  on 2 df,   p=1e-07\nWald test            = 28.59  on 2 df,   p=6e-07\nScore (logrank) test = 33.51  on 2 df,   p=5e-08\n\n\nGiải thích:\n\nKhi sex tăng thêm 1 đơn vị (chuyển sang nhóm 1 sang 2) thì nguy cơ trải qua sự kiện tăng thêm 0.939 lần"
  },
  {
    "objectID": "p07-04-survival-analysis.html#tài-liệu-tham-khảo",
    "href": "p07-04-survival-analysis.html#tài-liệu-tham-khảo",
    "title": "53  Survival Analysis",
    "section": "53.5 Tài liệu tham khảo",
    "text": "53.5 Tài liệu tham khảo\n\nChương 13, phân tích với R - Nguyễn Văn Tuấn\nR for Everyone"
  },
  {
    "objectID": "p07-05-vintage-analysis.html#giới-thiệu",
    "href": "p07-05-vintage-analysis.html#giới-thiệu",
    "title": "54  Vintage analysis",
    "section": "54.1 Giới thiệu",
    "text": "54.1 Giới thiệu\nVintage analysis là kỹ thuật phân tích giữa các nhóm khác nhau theo thời gian. Vintage analysis được sử dụng lần đầu khi so sánh chất lượng giữa các nhóm khách hàng bằng cách xem tỷ lệ nợ xấu của khách hàng sau cùng 1 khoảng thời gian. Ví dụ, tỷ lệ nợ xấu của khách hàng vay tín dụng sau 6 tháng, 12 tháng theo năm vay. Tuy nhiên, bên cạnh hoạt động quản trị rủi ro, Vintage Analysis còn có rất nhiều ứng dụng trong các hoạt động phân tích khám phá dữ liệu khác nhau như Marketing, Business Analaytics. Vintage Analysis có 2 loại cơ bản sau:\n\nPhân tích tỷ lệ theo chiều thời gian: Đây là loại phân tích vintage gốc của quản trị rủi ro. Trong loại phân tích này, dữ liệu được chia thành các nhóm chỉ theo chiều thời gian. Chỉ số được đăng ký Ví dụ: Phân tích tỷ lệ convert của marketing theo tháng/quý đăng ký hồ sơ\nPhân tích tỷ lệ dịch chuyển theo nhóm: Loại phân tích này được mở rộng từ loại hình phân tích gốc, cho phép phân tích không chỉ theo chiều thời gian mà còn nhiều chiều khác nhau. Xét về bản chất, phân tích vintage theo nhóm 1 chỉ là 1 trường hợp đặc biệt\n\nĐể thuận tiện, ta sẽ phân tích tập dữ liệu marketing như dưới đây.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(lubridate)\nlibrary(scales)\nload(\"data/marketing.rda\")\nmarketing %>% head\nmarketing %>% summary\n\nTrong tập dữ liệu trên, mỗi dòng ứng với 1 quan sát (khách hàng).\n\nBiến start: Thời gian khách hàng đăng ký mua sản phẩm (hoặc nhận được thông tin quảng cáo, etc.)\nBiến event: Thời gian khách hàng thực sự mua sản phẩm\n\n\nanalyze_vintage_group <- function(data, start, end, group1, group2, type = c(\"month\", \"day\", \"year\")){\n  group1 <- enquo(group1)\n  group2 <- enquo(group2)\n  start <- enquo(start)\n  end <- enquo(end)\n  result <- data %>%\n    mutate(time = interval(!!start, !!end) %>% time_length(type)) %>% \n    group_by(!!group1, !!group2, time) %>% \n    summarise(no = n()) %>% \n    ungroup %>%\n    group_by(!!group1, !!group2) %>%\n    arrange(time) %>% \n    mutate(cumno = cumsum(no)) %>%\n    mutate(total_no = sum(no)) %>%\n    mutate(percent = cumno/total_no) %>%\n    na.omit() %>% \n    ungroup\n  return(result)\n}"
  },
  {
    "objectID": "p07-05-vintage-analysis.html#phân-tích-vintage-loại-1",
    "href": "p07-05-vintage-analysis.html#phân-tích-vintage-loại-1",
    "title": "54  Vintage analysis",
    "section": "54.2 Phân tích vintage loại 1",
    "text": "54.2 Phân tích vintage loại 1\nTrong loại phân tích này, ta sẽ xem xét tỷ lệ chuyển đổi theo thời gian giữa các nhóm khách hàng, dựa vào thời gian đăng ký.\n\n# Lấy tập dữ liệu test\n\ndf <- marketing %>% \n  filter(start <= event | is.na(event))\n\ndf <- df %>% \n  mutate(group = sample(c(1,2,3), size = nrow(.), replace = T)) %>% \n  mutate(month = floor_date(start, \"months\"))\n\nresult <- analyze_vintage_group(df, start, event, start, start, type = \"day\")\n# Ví dụ khách hàng đăng ký vào ngày `20180823`\nresult %>% filter(start == ymd(\"2018-08-23\"))\n\nVới tập dữ liệu trên, ta có thể diễn giải như sau:\n\nCó 47 khách hàng đăng ký mua sản phẩm vào ngày 2019-08-23\nNgay trong ngày đầu tiên đăng ký (time = 0), có 2.13% khách hàng mua sản phẩm\nSau 50 ngày (time = 50), có 10 khách hàng, tương ứng với 221.3% khách hàng mua sản phẩm\n\n\nresult %>% \n  ggplot(aes(time, percent)) +\n  geom_line(aes(group = start), col = \"darkgrey\") +\n  geom_smooth() +\n  theme(legend.position = \"none\") +\n  theme_light() +\n  scale_color_tableau() +\n  scale_y_continuous(labels = percent) +\n  labs(x = \"Days since registered\", y = \"Conversion rate\",\n       title = \"Conversion rate of different registration time\")"
  },
  {
    "objectID": "p07-05-vintage-analysis.html#phân-tích-vintage-loại-2",
    "href": "p07-05-vintage-analysis.html#phân-tích-vintage-loại-2",
    "title": "54  Vintage analysis",
    "section": "54.3 Phân tích vintage loại 2",
    "text": "54.3 Phân tích vintage loại 2\nTrong loại hình phân tích này, ta mở rộng loại hình phân tích, không chỉ xem xét yếu tố theo thời gian mà mở rộng toàn bộ phân tích theo nhiều nhóm khác nhau.\n\nresult_group <- df %>%\n  analyze_vintage_group(start, event, group, month, type = \"day\")\n\n\nresult_group %>% \n  filter(group == 1 & month == ymd(\"20180901\"))\n\nresult_group %>% \n  filter(group == 2 & month == ymd(\"20180901\"))\n\n\nresult_group %>% \n  mutate(group = as.factor(group)) %>% \n  ggplot(aes(time, percent)) +\n  geom_line(aes(col = group), size = 1) +\n  theme(legend.position = \"none\") +\n  theme_light() +\n  scale_color_tableau() +\n  facet_wrap(~month, scale = \"free\") +\n  scale_y_continuous(labels = percent) +\n  labs(x = \"Days since registered\", y = \"Conversion rate\",\n       title = \"Conversion rate of different registration time\") +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "p07-05-vintage-analysis.html#tài-liệu-tham-khảo",
    "href": "p07-05-vintage-analysis.html#tài-liệu-tham-khảo",
    "title": "54  Vintage analysis",
    "section": "54.4 Tài liệu tham khảo",
    "text": "54.4 Tài liệu tham khảo\n\nhttps://charting-ahead.corsairs.network/vintage-analysis-a-visual-primer-490eff0dcb5a"
  },
  {
    "objectID": "p07-07-spatial-data.html#giới-thiệu",
    "href": "p07-07-spatial-data.html#giới-thiệu",
    "title": "55  Spatial data",
    "section": "55.1 Giới thiệu",
    "text": "55.1 Giới thiệu\nSpatial data là loại dữ liệu chứa các thông tin về “thuộc tính hình học”(geometry), “hình dạng” (shape) hoặc “vị trí”(location) của một đối tượng địa lý và thường kết hợp với các attributes chứa thông tin liên quan khác. Định dạng dữ liệu spatial data cho phép chúng ta xây dựng các biểu đồ với R hiệu quả.\nSpatial Analytics có rất nhiều cách thức phát triển khác nhau. Tuy nhiên, để thuận tiện và dễ dàng trong việc ứng dụng, ta tập trung vào 2 nhóm sau.\n\nBiểu đồ tĩnh (static map): sf, tmap, ggplot2\nBiểu đồ động (interactive map): leaflet"
  },
  {
    "objectID": "p07-07-spatial-data.html#cấu-trúc-dữ-liệu-spatial",
    "href": "p07-07-spatial-data.html#cấu-trúc-dữ-liệu-spatial",
    "title": "55  Spatial data",
    "section": "55.2 Cấu trúc dữ liệu spatial",
    "text": "55.2 Cấu trúc dữ liệu spatial\nMột loại định dạng dữ liệu được dùng phổ biến với spatial data là vector data. Loại dữ liệu này thường được lưu dưới dạng shape (.shp),để đọc định dạng này chúng ta sử dụng function read_sp trong package sf\n\nlibrary(sf)\nlibrary(tidyverse)\nlnd_sport <- sf::read_sf(\"data/spatial-data/london_sport.shp\")\nprint(lnd_sport)\n\nĐịnh dạng dữ liệu này bao gồm hai cấu phần chính:\n\ngeometry: Là tật hợp các điểm kinh độ và vĩ độ xác định đường biên của một đối tượng trên hệ trục tọa độ.\nattribute: Các dữ liệu thông thường của đối tượng (quan sát) được chứa trong raster.\n\n\n55.2.1 Geometry data\n\ngeom_type <- st_geometry(lnd_sport)\ngeom_type\n\nCác thuộc tính hình học lưu trong geometry:\n\ngeometry type: thuộc tính hình học\ndimension : Hệ trục XY\nbbox (Bounding box): các tọa độ lớn nhất, nhỏ nhất của objects\nproj4string: thông tin hệ tọa độ địa lý\n\n\n\n55.2.2 Attribute data\nPhần dữ liệu này chúng ta có thể biến đổi và phân tích như dữ liệu bảng thông thường.\n\nLondon_pop <- lnd_sport %>% select(name, Pop_2001)\nLondon_pop %>% names\nBromley <- London_pop %>% filter(name == \"Bromley\")\nBromley %>% head\n\n\n\n55.2.3 Biểu đồ cơ bản\n\nplot(lnd_sport)"
  },
  {
    "objectID": "p07-07-spatial-data.html#xây-dựng-biểu-đồ-tĩnh",
    "href": "p07-07-spatial-data.html#xây-dựng-biểu-đồ-tĩnh",
    "title": "55  Spatial data",
    "section": "55.3 Xây dựng biểu đồ tĩnh",
    "text": "55.3 Xây dựng biểu đồ tĩnh\n\n55.3.1 Biểu đồ cơ bản\ntmap là package phổ biến nhất để vẽ các biểu đồ tĩnh, tương tự như ggplot2, package tmap cũng xây dựng theo hướng layer approach. Với ggplot2 mới nhất, ta có thể dễ dàng xây dựng bản đồ.\n\nlibrary(\"tmap\")\ndata(NLD_muni,NLD_prov)\n\ntm_shape(NLD_muni, projection=\"rd\") + \n#fill by population per km2\n    tm_fill(\"population\",\n           convert2density=TRUE,\n           style=\"kmeans\",\n           palette = \"Greens\",\n           title=\"Population per km2\") + \n  tm_borders(alpha= .5) +  \n#combine with province data \ntm_shape(NLD_prov) +\n  tm_borders(lwd=2, alpha = .5) +\n  tm_text(\"name\", \n          size = .6, \n          alpha = 0.9, \n          shadow=TRUE,\n          bg.color=\"white\", \n          bg.alpha=.25) +\n  tm_layout(asp = 0) \n\n\n\n55.3.2 Facet theo Province\n\ntm_shape(NLD_muni,projection=\"rd\") + \n  #fill by population per km2\n  tm_fill(\"population\",\n          convert2density=TRUE,\n          style=\"kmeans\",\n          palette = \"Greens\",\n          title=\"Population per km2\") + \n  tm_borders(alpha= .5) +\n  tm_facets(by=\"province\",\n            free.coords=TRUE,\n            drop.shapes=TRUE)\n\n\n\n55.3.3 Vẽ biểu đồ với ggplot2\nTrước khi có package sf, khi cần vẽ map bằng ggplot2 ta cần convert dữ liệu spatial về dạng data.frame, tuy nhiên version mới của ggplot2 đã cập nhật thêm thuộc tính geom_sf - có thể vẽ trực tiếp trên sf objects mà không cần convert về data.frame.\n\nlibrary(\"tmap\")\ndata(NLD_muni,NLD_prov)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(RColorBrewer)\n\npal <- brewer.pal(4, \"YlGn\")\n\nNLD_muni %>% \n  st_as_sf() %>% # Convert sang sf\n  ggplot() +\n  geom_sf(aes(fill = population),\n          color = \"light grey\") +\n  scale_fill_gradientn(colors = pal, trans = \"log10\",\n                         name =\"population per km2\", labels = comma) +\n  theme_minimal() +\n  labs(title = \"Population density in London\")"
  },
  {
    "objectID": "p07-07-spatial-data.html#xây-dựng-biểu-đồ-động-với-leaflet",
    "href": "p07-07-spatial-data.html#xây-dựng-biểu-đồ-động-với-leaflet",
    "title": "55  Spatial data",
    "section": "55.4 Xây dựng biểu đồ động với leaflet",
    "text": "55.4 Xây dựng biểu đồ động với leaflet\nleaflet là package cho phép chúng ta xây dựng biểu đồ động (interactive) rất chuyên nghiệp với dữ liệu đầu vào là tọa độ địa lý theo từng điểm. leaflet sử dụng pipe operator trong việc tạo các lớp biểu đồ\n\nlibrary(dplyr)\nlibrary(leaflet)\nlibrary(leaflet.extras)\nlibrary(data.table)\nload(\"data/location_data.rda\")\nload(\"data/branch_address.rda\")\n\n\nleaflet() %>% \n  addProviderTiles(providers$CartoDB.Positron) %>%\n  setView(108.2772, 14.05832, zoom = 5) %>% \n  addMarkers(data = branch_address,\n             lng = ~lon_branch, \n             lat = ~lat_branch,\n             popup = ~as.character(branch_name)) %>% \n   addHeatmap(data = location_data, \n             lng = ~lon, \n             lat = ~lat, \n             radius = 7, \n             blur = 22) %>% \n  addCircleMarkers(data = location_data,\n                   lng = ~lon, lat = ~lat, \n                   clusterOptions = markerClusterOptions()) \n\n\n# Save map thành file html\nsaveWidget(leaflet_object, file = \"./map_transaction.html\")"
  },
  {
    "objectID": "p07-07-spatial-data.html#cách-lấy-tọa-độ-chính-xác-từ-google-api",
    "href": "p07-07-spatial-data.html#cách-lấy-tọa-độ-chính-xác-từ-google-api",
    "title": "55  Spatial data",
    "section": "55.5 Cách lấy tọa độ chính xác từ Google API",
    "text": "55.5 Cách lấy tọa độ chính xác từ Google API\nKhi phân tích dữ liệu tọa độ địa lý ở thị trường Việt nam, một trong các điểm khó nhất là lấy được chính xác kinh độ và vĩ độ của khách hàng từ địa chỉ thực tế. Để lấy được tọa độ địa lý, ta có thể sử dụng package ggmap và đăng ký key API .\n\nBước 1: Tạo Google Project từ website console.could.google.com\nBước 2: Enable API google geocoding\nBước 3: Tạo Credential trong project và lấy key\nBước 4: Sử dụng ggmap để gọi API từ Google\n\n\nlibrary(dplyr)\nlibrary(ggmap)\n\n\n######API theo email hoangducanh89\nregister_google(\n  key = \"AIzaSyD7H6f5qkECD5t85fOmUmFZ9ELJykSmQ4A\",\n  account_type = \"standard\",\n  day_limit = 100000\n)\n\n# Kiểm tra quotas\ngeocodeQueryCheck()\ngetOption(\"ggmap\")\n\ngeocode(\"89 Láng Hạ\", \"latlona\")\n\n\ndf <- data.frame(\n  # customer id\n  recid = c(1,2),\n  # customer address\n  address = c(\"89 Láng Hạ, Hà Nội\", \"20 Nguyen Chi Thanh, Dong Da, Hanoi\"))\ndf\n#Đặt progress bar\ncustomer.address <- data.frame()\nfor (i in 1:nrow(df)) {\n  print(paste0(i, \" customers - \", round(i*100/nrow(df),2), \"%\"))\n  #Kiểm tra query còn lại\n  tryCatch({\n    address <- geocode(df$address[[i]], \"latlona\", override_limit = T)\n    address$id <- df$recid[[i]]\n    address <- address %>% select(4, 1:3)\n    customer.address <- rbind(customer.address, address)\n  }, error = function(e) {\n    print(\"error\")\n    cat(\"\\n\")\n  },\n  # Đặt warning để loại các trường hợp lỗi API trả về từ Google \n  warning = function(w) {\n    print(\"warning\")\n    cat(\"\\n\")\n  })\n}"
  },
  {
    "objectID": "p07-07-spatial-data.html#tính-toán-khoảng-cách-giữa-hai-điểm",
    "href": "p07-07-spatial-data.html#tính-toán-khoảng-cách-giữa-hai-điểm",
    "title": "55  Spatial data",
    "section": "55.6 Tính toán khoảng cách giữa hai điểm",
    "text": "55.6 Tính toán khoảng cách giữa hai điểm\nKhi phân tích về việc tối ưu hóa cho logistic, ta thường xuyên phải tính toán khoảng cách giữa hai hoặc nhiều điểm. Để xử lý vấn đề trên, ta có thể dùng package geosphere.\nCách tính khoảng cách dựa vào tọa độ của hai điểm như sau.\n\nlibrary(geosphere)\ndistm(c(10, 6), c(9.9,6.1))/1e3\n\n\n# Tạo dữ liệu ví dụ\nlibrary(data.table)\ndf <- location_data %>% \n  head(10) %>% \n  select(-address) %>% \n  bind_cols(branch_address %>% \n              head(10) %>% \n              select(lon_branch, lat_branch)) %>% \n  rename(lon_customer = lon, \n         lat_customer = lat)\n\ndf <- df %>% setDT\ndf %>% head\n\n\nfor (i in 1:nrow(df)){\n  df[i, distance := distm(c(lon_customer, lat_customer),\n                                  c(lon_branch, lat_branch),\n                                  fun = distVincentyEllipsoid) %>% as.numeric]\n}\n\ndf$distance <- df$distance/1000\n\ndf %>% head"
  },
  {
    "objectID": "p07-07-spatial-data.html#tài-liệu-tham-khảo",
    "href": "p07-07-spatial-data.html#tài-liệu-tham-khảo",
    "title": "55  Spatial data",
    "section": "55.7 Tài liệu tham khảo",
    "text": "55.7 Tài liệu tham khảo\n\nhttps://rstudio.github.io/leaflet/"
  },
  {
    "objectID": "p07-08-anomaly-detection.html#giới-thiệu",
    "href": "p07-08-anomaly-detection.html#giới-thiệu",
    "title": "56  Abnomaly detection",
    "section": "56.1 Giới thiệu",
    "text": "56.1 Giới thiệu\nAbnomaly detection là một nhánh ứng dụng của phân tích dữ liệu cho phép phát hiện những quan sát bất thường trong cả tập dữ liệu mà không cần target. Do đó, có thể coi abnomaly detection thuộc và nhóm unsupervised learning. Các"
  },
  {
    "objectID": "p07-08-anomaly-detection.html#thực-hành",
    "href": "p07-08-anomaly-detection.html#thực-hành",
    "title": "56  Abnomaly detection",
    "section": "56.2 Thực hành",
    "text": "56.2 Thực hành\n\nlibrary(dplyr)\nlibrary(h2o)\nlibrary(ggplot2)\nh2o.init(nthreads = -1)\n\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    /tmp/Rtmpgtrx6F/file13dd72b17157/h2o_r110786_started_from_r.out\n    /tmp/Rtmpgtrx6F/file13dd6f073ecf/h2o_r110786_started_from_r.err\n\n\nStarting H2O JVM and connecting: ...... Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         7 seconds 871 milliseconds \n    H2O cluster timezone:       UTC \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.40.0.1 \n    H2O cluster version age:    2 months and 9 days \n    H2O cluster name:           H2O_started_from_R_r110786_btk939 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   0.24 GB \n    H2O cluster total cores:    1 \n    H2O cluster allowed cores:  1 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.2.3 (2023-03-15) \n\n\n\ndf <- mtcars %>% \n  mutate(id = 1:nrow(.)) %>% \n  mutate_at(vars(gear, carb, cyl, vs, am), as.factor)\ndf %>% head\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb id\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  1\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  2\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1  3\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1  4\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2  5\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1  6\n\nh2o_mtcars <- df %>% as.h2o\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\nBước 1: Sử dụng deep learning để tự học về cấu trúc dữ liệu\n\n\nmodel <- h2o.deeplearning(x = 1:11,\n                          training_frame = h2o_mtcars,\n                          autoencoder = TRUE,\n                          activation = \"RectifierWithDropout\",\n                          hidden = c(50, 50, 50),\n                          epochs = 100)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nmodel %>% summary\n\nModel Details:\n==============\n\nH2OAutoEncoderModel: deeplearning\nModel Key:  DeepLearning_model_R_1681785482376_1 \nStatus of Neuron Layers: auto-encoder, gaussian distribution, Quadratic loss, 7,877 weights/biases, 99.3 KB, 3,200 training samples, mini-batch size 1\n  layer units             type dropout       l1       l2 mean_rate rate_rms\n1     1    27            Input  0.00 %       NA       NA        NA       NA\n2     2    50 RectifierDropout 50.00 % 0.000000 0.000000  0.195489 0.398028\n3     3    50 RectifierDropout 50.00 % 0.000000 0.000000  0.002550 0.001273\n4     4    50 RectifierDropout 50.00 % 0.000000 0.000000  0.000993 0.000472\n5     5    27        Rectifier      NA 0.000000 0.000000  0.000864 0.000856\n  momentum mean_weight weight_rms mean_bias bias_rms\n1       NA          NA         NA        NA       NA\n2 0.000000   -0.004565   0.178320  0.499220 0.100656\n3 0.000000   -0.015240   0.146344  0.922803 0.099024\n4 0.000000   -0.032398   0.134862  0.898634 0.049866\n5 0.000000   -0.014998   0.157386 -0.039085 0.205516\n\nH2OAutoEncoderMetrics: deeplearning\n** Reported on training data. **\n\nTraining Set Metrics: \n=====================\n\nMSE: (Extract with `h2o.mse`) 0.07473235\nRMSE: (Extract with `h2o.rmse`) 0.2733722\n\n\n\n\n\n\nScoring History: \n            timestamp   duration  training_speed    epochs iterations\n1 2023-04-18 02:38:17  0.998 sec 0.00000 obs/sec   0.00000          0\n2 2023-04-18 02:38:18  1.715 sec    4500 obs/sec 100.00000         10\n      samples training_rmse training_mse\n1    0.000000       0.53345      0.28457\n2 3200.000000       0.27337      0.07473\n\nVariable Importances: (Extract with `h2o.varimp`) \n=================================================\n\nVariable Importances: \n  variable relative_importance scaled_importance percentage\n1       hp            1.000000          1.000000   0.061064\n2     vs.1            0.908028          0.908028   0.055448\n3     qsec            0.863335          0.863335   0.052719\n4     disp            0.845461          0.845461   0.051627\n5   carb.8            0.790535          0.790535   0.048273\n\n---\n           variable relative_importance scaled_importance percentage\n22             am.0            0.596973          0.596973   0.036453\n23 carb.missing(NA)            0.000000          0.000000   0.000000\n24 gear.missing(NA)            0.000000          0.000000   0.000000\n25  cyl.missing(NA)            0.000000          0.000000   0.000000\n26   vs.missing(NA)            0.000000          0.000000   0.000000\n27   am.missing(NA)            0.000000          0.000000   0.000000\n\n\n\nBước 2: Xác định outlier\n\n\n# Tính errors\nerrors <- h2o.anomaly(model, h2o_mtcars, per_feature = FALSE)\nerrors <- errors %>% as.data.frame() %>% \n  mutate(id = 1:nrow(.))\nerrors %>% head\n\n  Reconstruction.MSE id\n1         0.11022173  1\n2         0.10936504  2\n3         0.04366813  3\n4         0.10114024  4\n5         0.04088753  5\n6         0.10564159  6\n\n\n\nerrors %>%\n  arrange(Reconstruction.MSE) %>% \n  mutate(new_id = 1:nrow(.)) %>% \n  ggplot(aes(new_id, Reconstruction.MSE)) +\n  geom_point() +\n  theme_minimal() +\n  geom_hline(yintercept = 0.12, linetype = \"dashed\", \n             col = \"darkred\", size = 0.8)\n\n\n\n\n\nBước 3: Lọc các outlier\n\n\noutlier <- errors %>% filter(Reconstruction.MSE > 0.12)\n\ndf %>% filter(id %in% outlier$id)\n\n                mpg cyl  disp  hp drat    wt qsec vs am gear carb id\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2 27\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2 28\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4 29\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.5  0  1    5    6 30\nMaserati Bora  15.0   8 301.0 335 3.54 3.570 14.6  0  1    5    8 31"
  },
  {
    "objectID": "p07-08-anomaly-detection.html#tài-liệu-tham-khảo",
    "href": "p07-08-anomaly-detection.html#tài-liệu-tham-khảo",
    "title": "56  Abnomaly detection",
    "section": "56.3 Tài liệu tham khảo",
    "text": "56.3 Tài liệu tham khảo\n\nhttps://www.slideshare.net/ds_mi/h2o-for-iot-jofai-joe-chow-h2o\n[https://towardsdatascience.com/credit-card-fraud-detection-using-autoencoders-in-h2o-399cbb7ae4f1]"
  },
  {
    "objectID": "p07-10-shiny.html#giới-thiệu",
    "href": "p07-10-shiny.html#giới-thiệu",
    "title": "57  Shiny apps",
    "section": "57.1 Giới thiệu",
    "text": "57.1 Giới thiệu\nXây dựng Shiny App giúp cho người dùng (end user) dễ dàng hơn trong việc sử dụng các công cụ Analytics\n\nShiny app bao gồm 2 file:\n\nui.R: Xác định UI của app\nserver.R: Xác định các câu lệnh để thực hiện trong server\n\n2 file của shiny cần đặt cùng trong 1 thư mục\n\n\nlibrary(shiny)\nrunExample(\"01_hello\")"
  },
  {
    "objectID": "p07-10-shiny.html#phân-tích-ui-và-server",
    "href": "p07-10-shiny.html#phân-tích-ui-và-server",
    "title": "57  Shiny apps",
    "section": "57.2 Phân tích UI và SERVER",
    "text": "57.2 Phân tích UI và SERVER\n\n57.2.1 UI\n\nlibrary(shiny)\n\n# fluiidPage cho phép tạo UI responsive\nshinyUI(fluidPage(\n  \n  # Tên ứng dụng\n  titlePanel(\"Ứng dụng đầu tiên của Đức Anh\"),\n  \n  # Sidebar với input dạng slider cho số  bins \n  sidebarLayout(\n    sidebarPanel(\n      #Input cho bins dạng slider\n       sliderInput(\"bins\",           #Tên input\n                   \"Số lượng bins:\", #Title cho input\n                   min = 1,\n                   max = 50,\n                   value = 30),      #Giá trị mặc định\n       #Input cho mean dạng nhập số\n       numericInput(\"mean\",\n                    \"Mean value\",\n                    value = 0),\n       #Input cho standard deviation\n       numericInput(\"sd\",\n                    \"Standard deviation\",\n                    value = 1)\n    ),\n    \n    # main Panel xác định kết quả đầu ra\n    mainPanel(\n       plotOutput(\"distPlot\")       #Ouput của distPlot\n    )\n  )\n))\n\n###SERVER\n\nlibrary(shiny)\n\n# shiny Server được tính bằng hàm của input và ouput\nshinyServer(function(input, output) {\n#distPlot tương ứng với trên UI.R\n#renderPlot trả ra kết quả biểu đồ\n  output$distPlot <- renderPlot({\n    \n    #Tạo vector phân phối chuẩn có mean từ mean của input, sd từ sd của input\n    x    <- rnorm(1000, input$mean, input$sd)\n    bins <- seq(min(x), max(x), length.out = input$bins + 1)\n    \n    # Vẽ plot\n    hist(x, breaks = bins, col = 'darkgray', border = 'white',\n         main = \"Biểu đồ phân phối chuẩn\")\n    })\n  \n})"
  },
  {
    "objectID": "p07-10-shiny.html#ui-1",
    "href": "p07-10-shiny.html#ui-1",
    "title": "57  Shiny apps",
    "section": "57.3 UI",
    "text": "57.3 UI\n\nTham khảo danh sách widget cho Shiny tại Shiny widget gallery\nLayout của UI có thể tham khảo tại RStudio\n\n\n57.3.1 Các ứng dụng đơn giản\n\nNhập text\n\n\n# ui.R\n\nlibrary(shiny)\n\nshinyUI(fluidPage(\n  titlePanel(\"Tên ứng dụng cho Shiny\"),\n  \n  sidebarLayout(\n    sidebarPanel( h1(\"Các Input đầu vào\"),\n                  h5(\"Giá trị của bins, mean\"),\n                  numericInput(\"bins\", \"Bins\", value = 1),\n                  sliderInput(\"mean\",\"Mean\", value = 5, min = 1, max = 10)),\n    mainPanel(\"Main Panel trả ra kết quả\")\n  )\n))\n\n\nChèn ảnh:\n\nCho ảnh vào folder “www” nằm cùng vị trí với UI.R & SERVER.R\nẢnh có thể nằm ở vị trí title, main Panel hoặc Slider Panel trong app\n\n\n\n# ui.R\nlibrary(shiny)\n\nshinyUI(fluidPage(\n  img(src = \"logo.png\", height = 50, width = 200 ),\n  titlePanel(\"Tên ứng dụng cho Shiny\"),\n  \n  sidebarLayout(\n    sidebarPanel( \n                  h1(\"Các Input đầu vào\"),\n                  h5(\"Giá trị của bins, mean\"),\n                  numericInput(\"bins\", \"Bins\", value = 1),\n                  sliderInput(\"mean\",\"Mean\", value = 5, min = 1, max = 10)),\n    mainPanel(\"Main Panel trả ra kết quả\")\n  )\n))\n\n\n\n57.3.2 Widget trong Shiny\nTa có thể thêm widget tương tự như thêm các thành phần của HTML. Widget cần các thành phần sau:\n\nTên widget: Người dùng sẽ không thấy tên này nhưng tên sẽ được sử dụng thành các biến về sau.\nLabel: Nhãn cho widget, để người dùng dễ theo dõi hơn\n\n\nlibrary(shiny)\n\nshinyUI(fluidPage(\n  img(src = \"logo.png\", height = 50, width = 200 ),\n  titlePanel(\"Tên ứng dụng cho Shiny\"),\n  \n  sidebarLayout(\n    sidebarPanel( \n                  h1(\"Các Input đầu vào\"),\n                  numericInput(\"bins\", \"Bins\", value = 1),\n                  sliderInput(\"mean\",\"Mean\", value = 5, min = 1, max = 10),\n                  selectInput(\"value\",\"Value\",c(\"a\"= \"a\", \"b\" = \"b\")),\n                  fileInput(\"data\", \"Import dữ liệu\")),\n    mainPanel(\"Main Panel trả ra kết quả\")\n  )\n))\n\n###Output trên UI\nĐể tạo kết quả trên Shiny, cần thực hiện 2 bước:\n\nTạo 1 object trên UI.R\nXây dựng object đó trên SERVER.R\n\nCác output thông dụng trên Shiny gồm có:\n\nplotOutput\ntableOutput\ntextOutput\n\nĐể hiển thị kết quả của output, cần thực hiện các hàm trả ra kết quả:\n\nrenderPlot\nrenderText\nrenderTable\n\nTất cả các giá trị từ widget đều là input và được sử dụng trong phân tích\n\n#ui.r\nlibrary(shiny)\n\nshinyUI(fluidPage(\n  img(src = \"logo.png\", height = 50, width = 200 ),\n  titlePanel(\"Tên ứng dụng cho Shiny\"),\n  \n  sidebarLayout(\n    sidebarPanel( \n                  h1(\"Các Input đầu vào\"),\n                  textInput(\"text\", \"Text\")),\n    mainPanel(\"Kết quả hiển thị với text trong output\",\n              textOutput(\"text1\"))\n  )\n))\n\n\n#server.r\n\nlibrary(shiny)\n\n# shiny Server được tính bằng hàm của input và ouput\nshinyServer(function(input, output) {\n  #distPlot tương ứng với trên UI.R\n  #renderPlot trả ra kết quả biểu đồ\n  output$text1 <- renderText({\n    input$text\n  })\n  \n})\n\nBên cạnh đó, ta có thể dùng shiny dashboard để có giao diện chuyên nghiệp hơn khi xây dựng ứng dụng."
  },
  {
    "objectID": "p07-10-shiny.html#case-studies",
    "href": "p07-10-shiny.html#case-studies",
    "title": "57  Shiny apps",
    "section": "57.4 Case Studies",
    "text": "57.4 Case Studies\n\nXem thêm tất cả các ứng dụng đã được xây trong thư mục shiny dashboard"
  },
  {
    "objectID": "p07-10-shiny.html#tài-liệu-tham-khảo",
    "href": "p07-10-shiny.html#tài-liệu-tham-khảo",
    "title": "57  Shiny apps",
    "section": "57.5 Tài liệu tham khảo",
    "text": "57.5 Tài liệu tham khảo\n\nXem thêm shiny"
  },
  {
    "objectID": "p07-15-quantile-regression.html#giới-thiệu",
    "href": "p07-15-quantile-regression.html#giới-thiệu",
    "title": "58  Hồi quy phân vị",
    "section": "58.1 Giới thiệu",
    "text": "58.1 Giới thiệu\nHồi quy phân vị (Quantile regression) là phương pháp ước lượng các tham số hồi quy trên từng phân vị của biến phụ thuộc sao cho tổng chênh lệch tuyệt đối của hàm hồi quy tại mỗi phân vị đó là nhỏ nhất. Nói cách khác, hồi quy phân vị giúp xác định tác động của biến độc lập đến từng phân vị của biến phụ thuộc."
  },
  {
    "objectID": "p07-15-quantile-regression.html#xây-dựng-mô-hình",
    "href": "p07-15-quantile-regression.html#xây-dựng-mô-hình",
    "title": "58  Hồi quy phân vị",
    "section": "58.2 Xây dựng mô hình",
    "text": "58.2 Xây dựng mô hình\nKhác với mô hình hồi quy thông thường, mô hình hồi quy phân vị được xây dựng như sau:\n\\[Y = \\beta_{0(\\tau)} + \\beta_{1(\\tau)}X_1 + ... + \\beta_{n(\\tau)}X_n + e_i\\]\nTrong đó, \\(\\tau\\) là giá trị phân vị\n\\[Q_\\tau(Y) = \\beta_{0(\\tau)} + \\beta_{1(\\tau)}X_1 + ... + \\beta_{n(\\tau)}X_n\\]\nCác giá trị \\(\\beta_{i(\\tau)}\\) được ước lượng với mục tiêu tối thiểu quantile loss function tại mỗi phân vị \\(\\tau\\):\n\\[min \\sum \\begin{cases} e_i * \\tau & , Y >= Q_\\tau(Y)\\\\ e_i * (\\tau - 1) & , Y < Q_\\tau(Y)\\end{cases}\\]"
  },
  {
    "objectID": "p07-15-quantile-regression.html#so-sánh-với-ols",
    "href": "p07-15-quantile-regression.html#so-sánh-với-ols",
    "title": "58  Hồi quy phân vị",
    "section": "58.3 So sánh với OLS",
    "text": "58.3 So sánh với OLS\n\nTác động của biến độc lập đối với biến phụ thuộc:\n\nOLS: trung bình của biến phụ thuộc\nQuantile regression: từng phân vị của biến phụ thuộc\n\nẢnh hưởng của các quan sát bất thường (outliers):\n\nOLS: chịu tác động lớn, các quan sát bất thường cần được loại bỏ để ước lượng OLS không chị chệch\nQuantile regression: có tính ổn định, ít chịu tác động bởi outliers\n\nĐiều kiện về phân phối của biến phụ thuộc:\n\nOLS: phân phối chuẩn, đối xứng quanh giá trị trung bình\nQuantile regression: không cần đối xứng quanh giá trị trung bình. Do đó, có thể thể hiện tác động khác nhau của biến độc lập đến biến phụ thuộc ở từng phân vị"
  },
  {
    "objectID": "p07-15-quantile-regression.html#thực-hành-với-r",
    "href": "p07-15-quantile-regression.html#thực-hành-với-r",
    "title": "58  Hồi quy phân vị",
    "section": "58.4 Thực hành với R",
    "text": "58.4 Thực hành với R\nĐể xây dựng quantile regression, ta có thể sử dụng package quantreg\n\nlibrary(dplyr)\nlibrary(quantreg)\nlibrary(broom)\ndata(\"engel\")\nengel %>% head\n\n    income  foodexp\n1 420.1577 255.8394\n2 541.4117 310.9587\n3 901.1575 485.6800\n4 639.0802 402.9974\n5 750.8756 495.5608\n6 945.7989 633.7978\n\nmodel <- rq(foodexp ~ income, data = engel, tau = c(0.25, 0.5, 0.75))\nmodel %>% tidy\n\n# A tibble: 6 × 5\n  term        estimate conf.low conf.high   tau\n  <chr>          <dbl>    <dbl>     <dbl> <dbl>\n1 (Intercept)   95.5     67.6     135.     0.25\n2 income         0.474    0.417     0.511  0.25\n3 (Intercept)   81.5     47.1     135.     0.5 \n4 income         0.560    0.480     0.613  0.5 \n5 (Intercept)   62.4     27.8     120.     0.75\n6 income         0.644    0.574     0.705  0.75\n\n\nVới kết quả mô hình trên, ta có thể thấy nếu income tăng 1 đơn vị, chi tiêu cho thực phẩm (foodexp) ở các phân vị 25, 50 và 75 sẽ tăng lần lượt 0.474, 0.56 và 0.66 đơn vị.\nKhác với OLS, hồi quy phân vị cho ta bức tranh toàn diện hơn về ảnh hưởng giữa thu nhập và mức chi tiêu. Đặc biệt, nếu xây dựng mô hình với tất cả các mức phân vị (và với điều kiện đủ dữ liệu), ta có thể trực quan hóa ảnh hưởng của income với foodexp theo từng mốc phân vị như sau.\n\nresult <- rq(foodexp ~ income,\n            data = engel,\n            tau = seq(0.1, 0.95, by = 0.01)) %>% tidy %>% \n  as.data.frame() %>% \n  filter(term == \"income\") \n\nresult %>%\n  ggplot(aes(tau)) +\n  geom_line(aes(tau, estimate), col = \"darkred\", size = 1)  +\n  theme_minimal() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"darkblue\",\n              alpha = 0.3) +\n  labs(title = \"Effect of income on food expenditure for all percentile\")"
  },
  {
    "objectID": "p07-15-quantile-regression.html#tài-liệu-tham-khảo",
    "href": "p07-15-quantile-regression.html#tài-liệu-tham-khảo",
    "title": "58  Hồi quy phân vị",
    "section": "58.5 Tài liệu tham khảo",
    "text": "58.5 Tài liệu tham khảo\n\nhttps://data.library.virginia.edu/getting-started-with-quantile-regression/"
  },
  {
    "objectID": "p07-16-retention-analysis.html#giới-thiệu",
    "href": "p07-16-retention-analysis.html#giới-thiệu",
    "title": "59  Retention analysis",
    "section": "59.1 Giới thiệu",
    "text": "59.1 Giới thiệu\nPhân tích tỷ lệ giữ chân khách hàng (retention analysis) là một kỹ thuật chia khách hàng thành các nhóm theo thời gian (cohort) để so sánh, phân tích hành vi theo thời gian, giúp đánh giá tỷ lệ khách hàng tiếp tục sử dụng sản phẩm, dịch vụ thay đổi như thế nào. Phân tích retention với cohort được sử đụng đặc biệt rộng rãi với Digital Analytics, giúp phân tích tỷ lệ sử dụng website/app với các nhóm khác nhau.\nVí dụ sử dụng Cohort trong Google Analytics"
  },
  {
    "objectID": "p07-16-retention-analysis.html#hai-loại-cohort",
    "href": "p07-16-retention-analysis.html#hai-loại-cohort",
    "title": "59  Retention analysis",
    "section": "59.2 Hai loại cohort",
    "text": "59.2 Hai loại cohort\nPhụ thuộc vào cách tính toán của cohort, ta có thể chia cohort thành hai loại khác nhau.\n\nCohort không trùng lặp: Chia tập khách hàng thành các các cohort khác nhau và mỗi khách hàng chỉ thuộc 1 cohort. Cách phân tích này phù hợp với việc đánh giá chất lượng khách hàng theo thời điểm. Ví dụ, khách hàng chơi game, chúng ta có thể chia thao ngày khách hàng bắt đầu login và chơi game.\nCohort trùng lặp: Tương tự với cách trên nhưng cho phép 1 khách hàng có thể nằm ở nhiều cohort. Cách phân tích này phù hợp với việc phân tích đánh giá hành vi trong 1 khoảng thời gian. Cách phân tích này phù hợp với những trường hợp đánh giá tổng quan tỷ lệ sử dụng của khách hàng trong 1 khoảng thời gian mà không quan tâm đến thời điểm đăng ký.\n\n\nlibrary(dplyr)\n# Cohort không trùng lặp\ncalculate_retention_ordinary <- function (data, type = c(\"percentage\", \"actual\")) \n{\n  type <- match.arg(type)\n  library(dplyr)\n  library(lubridate)\n  if (type == \"percentage\") {\n    data <- data %>% distinct\n    names(data)[1:2] <- c(\"date\", \"ID\")\n    cohort <- unique(data$date) %>% sort\n    no.cohort <- length(cohort)\n    cohort.model <- data.frame(date = cohort)\n    for (i in cohort[1:no.cohort]) {\n      ID <- data %>% group_by(ID) %>% summarise(min_date = min(date)) %>% \n        filter(min_date == i) %>% distinct(ID)\n      x <- ID$ID\n      df <- data %>% filter(ID %in% x & date >= i) %>% \n        group_by(date) %>% summarise(no.cus = n())\n      df <- df %>% mutate(no.cus.per = no.cus/max(no.cus)) %>% \n        select(-no.cus)\n      cohort.model <- left_join(cohort.model, df, by = \"date\")\n    }\n    names(cohort.model)[2:dim(cohort.model)[[2]]] <- paste0(\"Cohort-\", \n      cohort)\n    return(cohort.model)\n  }\n  else {\n    data <- data %>% distinct\n    names(data)[1:2] <- c(\"date\", \"ID\")\n    cohort <- unique(data$date) %>% sort\n    no.cohort <- length(cohort)\n    cohort.model <- data.frame(date = cohort)\n    for (i in cohort[1:no.cohort]) {\n      ID <- data %>% group_by(ID) %>% summarise(min_date = min(date)) %>% \n        filter(min_date == i) %>% distinct(ID)\n      x <- ID$ID\n      df <- data %>% filter(ID %in% x & date >= i) %>% \n        group_by(date) %>% summarise(no.cus = n())\n      cohort.model <- left_join(cohort.model, df, by = \"date\")\n    }\n    names(cohort.model)[2:dim(cohort.model)[[2]]] <- paste0(\"Cohort-\", \n      cohort)\n    return(cohort.model)\n  }\n}\n\n# Cohort trùng lặp\ncalculate_retention <- function (data, type = c(\"percentage\", \"actual\")) \n{\n  type <- match.arg(type)\n  library(dplyr)\n  library(lubridate)\n  if (type == \"percentage\") {\n    data <- data %>% distinct()\n    names(data)[1:2] <- c(\"date\", \"ID\")\n    cohort <- unique(data$date) %>% sort\n    no.cohort <- length(cohort)\n    cohort.model <- data.frame(date = cohort)\n    for (i in cohort[1:no.cohort]) {\n      ID <- data %>% filter(date == i) %>% distinct(ID)\n      x <- ID$ID\n      df <- data %>% filter(ID %in% x & date >= i) %>% \n        group_by(date) %>% summarise(no.cus = n())\n      df <- df %>% mutate(no.cus.per = no.cus/max(no.cus)) %>% \n        select(-no.cus)\n      cohort.model <- left_join(cohort.model, df, by = \"date\")\n    }\n    names(cohort.model)[2:dim(cohort.model)[[2]]] <- paste0(\"Cohort-\", \n      cohort)\n    return(cohort.model)\n  }\n  else {\n    data <- data %>% distinct()\n    names(data)[1:2] <- c(\"date\", \"ID\")\n    cohort <- unique(data$date) %>% sort\n    no.cohort <- length(cohort)\n    cohort.model <- data.frame(date = cohort)\n    for (i in cohort[1:no.cohort]) {\n      ID <- data %>% filter(date == i) %>% distinct(ID)\n      x <- ID$ID\n      df <- data %>% filter(ID %in% x & date >= i) %>% \n        group_by(date) %>% summarise(no.cus = n())\n      cohort.model <- left_join(cohort.model, df, by = \"date\")\n    }\n    names(cohort.model)[2:dim(cohort.model)[[2]]] <- paste0(\"Cohort-\", \n      cohort)\n    return(cohort.model)\n  }\n}\n\n\ndf <- data.frame(\n  date = c(2, 2, 2, 1, 1, 5, 3, 8, 8, 9, 3, 3, 4, 5),\n  id =  c(1, 2, 3, 7, 8, 2, 5, 1, 2, 7, 3, 2, 1, 1)\n) %>% arrange(date)\ndf\n\n   date id\n1     1  7\n2     1  8\n3     2  1\n4     2  2\n5     2  3\n6     3  5\n7     3  3\n8     3  2\n9     4  1\n10    5  2\n11    5  1\n12    8  1\n13    8  2\n14    9  7\n\n\n\n59.2.1 Cohort không trùng lặp\n\ndf\n\n   date id\n1     1  7\n2     1  8\n3     2  1\n4     2  2\n5     2  3\n6     3  5\n7     3  3\n8     3  2\n9     4  1\n10    5  2\n11    5  1\n12    8  1\n13    8  2\n14    9  7\n\ndf %>% calculate_retention_ordinary()\n\n  date Cohort-1  Cohort-2 Cohort-3 Cohort-4 Cohort-5 Cohort-8 Cohort-9\n1    1      1.0        NA       NA       NA       NA       NA       NA\n2    2       NA 1.0000000       NA       NA       NA       NA       NA\n3    3       NA 0.6666667        1       NA       NA       NA       NA\n4    4       NA 0.3333333       NA       NA       NA       NA       NA\n5    5       NA 0.6666667       NA       NA       NA       NA       NA\n6    8       NA 0.6666667       NA       NA       NA       NA       NA\n7    9      0.5        NA       NA       NA       NA       NA       NA\n\n\nVới cách phân tích không trùng lặp, ta có thể đưa ra được các nhận định như sau:\n\nTừ tháng thứ 4, không có khách hàng đăng ký mới\nTỷ lệ sử dụng sau 1 tháng của Cohort-1 là 0%\nTỷ lệ sử dụng sau 1 tháng của Cohort-2 là 66.7%\n\n\n\n59.2.2 Cohort trùng lặp\n\ndf\n\n   date id\n1     1  7\n2     1  8\n3     2  1\n4     2  2\n5     2  3\n6     3  5\n7     3  3\n8     3  2\n9     4  1\n10    5  2\n11    5  1\n12    8  1\n13    8  2\n14    9  7\n\ndf %>% calculate_retention(type = \"actual\")\n\n  date Cohort-1 Cohort-2 Cohort-3 Cohort-4 Cohort-5 Cohort-8 Cohort-9\n1    1        2       NA       NA       NA       NA       NA       NA\n2    2       NA        3       NA       NA       NA       NA       NA\n3    3       NA        2        3       NA       NA       NA       NA\n4    4       NA        1       NA        1       NA       NA       NA\n5    5       NA        2        1        1        2       NA       NA\n6    8       NA        2        1        1        2        2       NA\n7    9        1       NA       NA       NA       NA       NA        1\n\ndf %>% calculate_retention()\n\n  date Cohort-1  Cohort-2  Cohort-3 Cohort-4 Cohort-5 Cohort-8 Cohort-9\n1    1      1.0        NA        NA       NA       NA       NA       NA\n2    2       NA 1.0000000        NA       NA       NA       NA       NA\n3    3       NA 0.6666667 1.0000000       NA       NA       NA       NA\n4    4       NA 0.3333333        NA        1       NA       NA       NA\n5    5       NA 0.6666667 0.3333333        1        1       NA       NA\n6    8       NA 0.6666667 0.3333333        1        1        1       NA\n7    9      0.5        NA        NA       NA       NA       NA        1\n\n\nVới cách sử dụng cohort trùng lặp, ta có thể đưa ra được các nhận định như sau:\n\nTrong 100 khách hàng có sử dụng sản phẩm tại tháng thứ 2, có 67% tiếp tục sử dụng trong tháng thứ 3, 33% tiếp tục sử dụng trong tháng thứ 4."
  },
  {
    "objectID": "p07-16-retention-analysis.html#tài-liệu-tham-khảo",
    "href": "p07-16-retention-analysis.html#tài-liệu-tham-khảo",
    "title": "59  Retention analysis",
    "section": "59.3 Tài liệu tham khảo",
    "text": "59.3 Tài liệu tham khảo\n\nhttps://neilpatel.com/blog/cohort-analysis-google-analytics/"
  },
  {
    "objectID": "p07-20-network-analysis.html#giới-thiệu",
    "href": "p07-20-network-analysis.html#giới-thiệu",
    "title": "60  Network analysis",
    "section": "60.1 Giới thiệu",
    "text": "60.1 Giới thiệu\nNetwork analysis(phân tích mạng lưới) là một nhánh kỹ thuật phân tích cho phép tìm sự liên kết giữa các đối tượng với nhau.\n\nCác đối tượng trong một mạng liên kết được gọi là nodes hoặc vertices\nChiều quan hệ giữa các đối tượng được gọi là edge. Mối quan hệ giữa các node có thể là vô hướng (undirected) hoặc có hướng (directed)\n\n\nlibrary(tidyverse)\n# library(ggraph)\nlibrary(tidygraph)\nlibrary(igraph)\nlibrary(visNetwork)\n\ndf <- data.frame(\n  from = c(1, 2, 3),\n  to = c(2, 1, 1)\n)\n\nnode <- data.frame(id = c(1,2,3,4, 5)) %>% \n  mutate(label = \"Node\")\n\nedge <- data.frame(\n  from = c(1, 2, 3, 2),\n  to = c(2, 1, 4, 3)\n) %>% \n  mutate(label = \"Edge\")\n\nvisNetwork(node, edge) %>% \n  visEdges(arrows = \"to\") %>% \n  visLayout() %>% \n  visHierarchicalLayout(direction = \"LR\")"
  },
  {
    "objectID": "p07-20-network-analysis.html#các-chỉ-số-cơ-bản",
    "href": "p07-20-network-analysis.html#các-chỉ-số-cơ-bản",
    "title": "60  Network analysis",
    "section": "60.2 Các chỉ số cơ bản",
    "text": "60.2 Các chỉ số cơ bản\nĐể xây dựng network, ta có thể sử dụng 2 dataframe chứa node và edge như sau.\n\nnode <- data.frame(id = c(1,2,3,4, 5, 6, 7, 8)) %>% \n  mutate(group = c(\"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\", \"A\"),\n         label = id) %>% \n  mutate(color = c(\"darkred\", \"darkred\", NA, NA, NA, \"darkred\", NA, NA))\n\nedge <- data.frame(\n  from = c(2,3,2,4, 1, 3, 7, 8),\n  to = c(4, 4, 3, 1, 1, 6, 6, 7))\n\nnetwork <- tbl_graph(\n  nodes = node, \n  edges = edge,\n  directed = T\n)\n\nvisNetwork(node, edge) %>% \n  visEdges(arrows = \"to\") %>% \n  visOptions(highlightNearest = list(degree = 2,\n                                     enabled = T,\n                                     hover = T,\n                                     algorithm = \"hierarchical\")) %>% \n  visHierarchicalLayout(direction = \"LR\")\n\n\n\n\n\n\n60.2.1 Khoảng cách (distances)\nKhoảng cách giữa các node được đo lường bằng số lần chuyển tiếp để có thể đi từ node này sang nốt khác. Khoảng cách này có thể được tính toán bằng hàm distances.\nLưu ý: Nên sử dụng algorithm = \"johnson\" để xác định chính xác khoảng cách ngắn nhất khi network là có hướng.\n\ndistances(network, algorithm = \"johnson\")\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    2    2    1  Inf    3    4    5\n[2,]    2    0    1    1  Inf    2    3    4\n[3,]    2    1    0    1  Inf    1    2    3\n[4,]    1    1    1    0  Inf    2    3    4\n[5,]  Inf  Inf  Inf  Inf    0  Inf  Inf  Inf\n[6,]    3    2    1    2  Inf    0    1    2\n[7,]    4    3    2    3  Inf    1    0    1\n[8,]    5    4    3    4  Inf    2    1    0\n\n\nKết quả trên đọc hiểu với node 2 như sau:\n\nNode 2 (dòng thứ 2) mất 2 bước để có thể đi đến node 1 (2 > 4 > 1)\nNode 2 không liên kết với node 5 nên khoảng cách giữa 2 node sẽ là Inf\nNode 2 khoảng cách đến chính nó bằng 0\n\nBên cạnh đó, ta cũng có thể tìm được khoảng cách từ các node đến 1 node bất kỳ như sau\n\n# Khoảng cách từ các node đến node 4\ndistances(network, to = 4, algorithm = \"johnson\")\n\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    1\n[4,]    0\n[5,]  Inf\n[6,]    2\n[7,]    3\n[8,]    4\n\n\n\n# Khoảng cách ngắn nhất từ node 2 đến node 1\nall_shortest_paths(network, from = 2, to = 1)\n\n$res\n$res[[1]]\n+ 3/8 vertices, from 781096b:\n[1] 2 4 1\n\n\n$nrgeo\n[1] 1 1 1 1 0 1 0 0\n\n# Khoảng cách ngắn nhất từ node 3 đến node 5\nall_shortest_paths(network, from = 3, to = 5)\n\n$res\nlist()\n\n$nrgeo\n[1] 1 0 1 1 0 1 0 0"
  },
  {
    "objectID": "p07-20-network-analysis.html#ứng-dụng",
    "href": "p07-20-network-analysis.html#ứng-dụng",
    "title": "60  Network analysis",
    "section": "60.3 Ứng dụng",
    "text": "60.3 Ứng dụng\nNetwork analysis được sử dụng rộng rãi trong phân tích mạng xã hội hoặc có thể sử dụng để phân tích data flow khi đọc các jobs của dữ liệu"
  },
  {
    "objectID": "p07-20-network-analysis.html#tài-liệu-tham-khảo",
    "href": "p07-20-network-analysis.html#tài-liệu-tham-khảo",
    "title": "60  Network analysis",
    "section": "60.4 Tài liệu tham khảo",
    "text": "60.4 Tài liệu tham khảo\n\nvisNetwork in R https://datastorm-open.github.io/visNetwork/\nigraph in R https://igraph.org/r/"
  },
  {
    "objectID": "p07-30-python-voi-r.html#giới-thiệu",
    "href": "p07-30-python-voi-r.html#giới-thiệu",
    "title": "61  Sử dụng Python với R",
    "section": "61.1 Giới thiệu",
    "text": "61.1 Giới thiệu\nPython và R là 2 ngôn ngữ được sử dụng nhiều nhất trong Data Science. Mỗi ngôn ngữ đều có các ưu nhược điểm riêng và đã có rất nhiều các cuộc tranh luận xem ngôn ngữ nào ưu việt hơn. Python ưu việt hơn khi xây dựng mô hình Deep Learning và làm việc với các hệ thống khác nhau, xây dựng API. R lại nổi trội hơn về phân tích khám phá và trực quan hóa với tidyverse. Chưa kể, xây dựng Data Science App với Shiny tốt hơn Python rất nhiều (hiện giờ mới chỉ có streamlit có thể cạnh tranh)\nTuy nhiên, ta không nên chỉ bám vào 1 ngôn ngữ mà có thể sử dụng cả 2 bổ trợ cho nhau. Với RStudio, ta có thể sử dụng cả R & Python, gọi các object qua lại giữa hai ngôn ngữ với sự ra đời của reticulate.\nĐể gọi 1 object từ R với Python, ta sử dụng method với r như sau\n\nimport pandas as pd\nr.iris.head()\n\nĐể gọi 1 object từ python, ta sử dụng S3 với object py như sau:\n\nx_py = [1, 2, 3, 4]\n# Gọi từ R\npy$x_py"
  },
  {
    "objectID": "p07-30-python-voi-r.html#deploy-shiny-app-có-sử-dụng-reticulate",
    "href": "p07-30-python-voi-r.html#deploy-shiny-app-có-sử-dụng-reticulate",
    "title": "61  Sử dụng Python với R",
    "section": "61.2 Deploy shiny app có sử dụng reticulate",
    "text": "61.2 Deploy shiny app có sử dụng reticulate\nCách đơn giản nhất để deploy shiny app là deploy với shinyapps.io. Khi xây app có sử dụng python, do môi trường của shinyapps.io khác so với môi trường của máy local. Ta cần lưu ý các điểm sau:\n\nHệ điều hành: Máy local dùng Windows, shinyapps.io dùng Linux. Do đó, với Windows, ta nên dùng conda_environment trong khi với Linux ta cần dùng virtual_environment từ pip\nTên virtual environment được sử dụng của app local và trên server khác nhau\nPython không dùng trực tiếp được trong shiny. Ta chỉ có thể xây các hàm sẵn có với Python và load vào môi trường R, sau đó sử dụng như R function như sau reticulate::source_python(\"soucre.py\")\nPackages cài đặt khác nhau\n\nĐể đảm bảo môi trường của Python giữa máy local và server, ta cần tạo python-environemnt tương có tên và sử dụng packages giống nhau. Ta có thể làm như sau\n\nif (Sys.info()[['sysname']] == 'Windows'){\n  py_env_check <- reticulate::conda_list()\n  if (sum(grepl(\"python_environment\", py_env_check$name)) == 0) {\n    reticulate::conda_create(envname = \"python_environment\")\n    reticulate::conda_install(\"python_environment\",\n                              packages = c(\"pandas\", \"numpy\", \"unidecode\", \"xlrd\"))\n  } else {\n    print(\"Your environment is ready\")\n  }\n  \n  reticulate::use_condaenv('python_environment', required = T)\n} else {\n  py_env_check <- reticulate::virtualenv_list()\n  if (sum(grepl(\"python_environment\", py_env_check)) == 0) {\n    reticulate::virtualenv_create(envname = \"python_environment\")\n    reticulate::virtualenv_install(\"python_environment\",\n                              packages = c(\"pandas\", \"numpy\", \"unidecode\", \"xlrd\"))\n  } else {\n    print(\"Your environment is ready\")\n  }\n  \n  reticulate::use_virtualenv('python_environment', required = T)\n}"
  },
  {
    "objectID": "p10-01-cai-dat-rstudio-server-tren-amazon.html#giói-thiệu",
    "href": "p10-01-cai-dat-rstudio-server-tren-amazon.html#giói-thiệu",
    "title": "62  Cài đặt server RStudio trên Amazon",
    "section": "62.1 Giói thiệu",
    "text": "62.1 Giói thiệu\nNhư chúng ta đều biết, R gặp các vấn đề giới hạn của bộ nhớ. Đặc biệt, với các bạn sử dụng Windows, R còn gặp các vấn đề về lỗi font tiếng Việt (encoding). Thêm vào đó, với các bạn muốn sử dụng R trong Deep Learning với CUDA cũng sẽ gặp rất nhiều khó khăn trong việc cài đặt cũng như các vấn đề về chi phí. Sử dụng RStudio online trên Amazon là cách thức hữu hiệu trong giải quyết các vấn đề sau:\n\nDữ liệu vượt quá dung lượng bộ nhớ máy tính (RAM)\nCó thể làm việc mọi nơi, mọi lúc được\n\nTrong chương này, chúng ta sẽ học cách tạo 1 server Rstudio online trên Amazon trong vòng chưa đến một phút."
  },
  {
    "objectID": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-0-tạo-account-trên-amazon",
    "href": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-0-tạo-account-trên-amazon",
    "title": "62  Cài đặt server RStudio trên Amazon",
    "section": "62.2 Bước 0: Tạo account trên amazon",
    "text": "62.2 Bước 0: Tạo account trên amazon\n\nĐăng ký 1 tài khoản trên Amazon với đường link sau https://aws.amazon.com/\nĐiền thông tin tài khoản thẻ tín dụng để có thể thanh toán với dịch vụ của Amazon"
  },
  {
    "objectID": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-1-sử-dụng-ami-của-rstudio-trên-amazon",
    "href": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-1-sử-dụng-ami-của-rstudio-trên-amazon",
    "title": "62  Cài đặt server RStudio trên Amazon",
    "section": "62.3 Bước 1: Sử dụng AMI của RStudio trên Amazon",
    "text": "62.3 Bước 1: Sử dụng AMI của RStudio trên Amazon\nTruy cập vào trang web sau: [http://www.louisaslett.com/RStudio_AMI]. Trang web này chứa tất cả các RStudio AMI (Amazon Machine Image) của RStudio bản mới nhất"
  },
  {
    "objectID": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-2-lựa-chọn-server-của-amazon.",
    "href": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-2-lựa-chọn-server-của-amazon.",
    "title": "62  Cài đặt server RStudio trên Amazon",
    "section": "62.4 Bước 2: Lựa chọn server của Amazon.",
    "text": "62.4 Bước 2: Lựa chọn server của Amazon.\nThông thường mình sẽ sử dụng dịch vụ Amazon tại Singapore. Tuy nhiên, các bạn có thể lựa chọn tùy ý Server mong muốn."
  },
  {
    "objectID": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-3-chọn-cấu-hình-máy-tính-mong-muốn",
    "href": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-3-chọn-cấu-hình-máy-tính-mong-muốn",
    "title": "62  Cài đặt server RStudio trên Amazon",
    "section": "62.5 Bước 3: Chọn cấu hình máy tính mong muốn",
    "text": "62.5 Bước 3: Chọn cấu hình máy tính mong muốn\nSau khi click vào AMI của Singapore sẽ hiển thị giao diện như sau:\n \n\n\n\n \nỞ bước này, ta phải chọn loại server mong muốn, các bạn có thể sử dụng rất nhiều các cấu hình khác nhau để cài đặt Rstudio server. Để tiết kiệm và sử dụng với mục đích thử nghiệm, mình sẽ dùng máy t2.micro ."
  },
  {
    "objectID": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-4-lựa-chọn-security-group",
    "href": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-4-lựa-chọn-security-group",
    "title": "62  Cài đặt server RStudio trên Amazon",
    "section": "62.6 Bước 4: Lựa chọn Security Group",
    "text": "62.6 Bước 4: Lựa chọn Security Group\nĐể đơn giản, các bạn có thể đến thẳng bước 6. Configure Security Group và chọn Type là HTTP để có thể truy cập vào RStudio online.\n \n\n \n\n\nBước 5: Launch RStudio Server, lưu ý chọn option Proceed without key pair để đơn giản như dưới đây.\n\n \n\n \nSau khi launching xong, ta sẽ có giao diện như hình dưới, các bạn lưu ý vào thẳng console của Amazon (click theo hình ở dưới) để tìm kiếm IP của Rstudio online của mình vừa tạo."
  },
  {
    "objectID": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-6-tìm-địa-chỉ-ip-public-của-máy-tính-vừa-tạo-và-truy-cập",
    "href": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-6-tìm-địa-chỉ-ip-public-của-máy-tính-vừa-tạo-và-truy-cập",
    "title": "62  Cài đặt server RStudio trên Amazon",
    "section": "62.7 Bước 6: Tìm địa chỉ IP Public của máy tính vừa tạo và truy cập",
    "text": "62.7 Bước 6: Tìm địa chỉ IP Public của máy tính vừa tạo và truy cập\nIP của máy tính vừa tạo sẽ có địa chỉ IP4 như hình dưới đây"
  },
  {
    "objectID": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-7-truy-cập-và-sử-dụng-rstudio",
    "href": "p10-01-cai-dat-rstudio-server-tren-amazon.html#bước-7-truy-cập-và-sử-dụng-rstudio",
    "title": "62  Cài đặt server RStudio trên Amazon",
    "section": "62.8 Bước 7: Truy cập và sử dụng RStudio",
    "text": "62.8 Bước 7: Truy cập và sử dụng RStudio\nTruy cập vào địa chỉ IP phía trên. Chúng ta có thể truy cập với mật khẩu và log-in mặc định là user: rstudio và password: rstudio như dưới đây\n \n\n \nSau khi đăng nhập được vào, các bạn có thể đổi mật khẩu, liên kết RStudio với account Dropbox để thuận tiện trong việc phân tích và sao lưu dữ liệu như trong script Welcome.R trong hình dưới đây.\n \n\n \n\nNhư vậy, chúng ta đã vừa tạo được một server Rstudio online của riêng mình. Việc sử dụng RStudio online sẽ rất hữu dụng trong việc vượt qua các hạn chế về cấu hình máy tính cá nhân."
  },
  {
    "objectID": "p10-02-deploy-shiny-app-aws.html#giói-thiệu",
    "href": "p10-02-deploy-shiny-app-aws.html#giói-thiệu",
    "title": "63  Deploy Shiny Apps lên AWS",
    "section": "63.1 Giói thiệu",
    "text": "63.1 Giói thiệu\nXây dựng app trên local có thể được thực hiện rất đơn giản với Shiny. Tuy nhiên, bước tiếp theo cần phải deploy app trên server để nhiều người có thể cùng trung cập. Có 2 cách cơ bản như sau:\n\nDeploy với shinyapps.io: Cách này rất đơn giản để xây prototype, free với app nhỏ, ít người dùng\nDeploy lên Cloud\n\nHiện tại, cách deploy lên AWS với Shiny là đơn giản nhất, ta có thể xây dựng app, deploy app với load balancer rất đơn giản."
  },
  {
    "objectID": "p10-02-deploy-shiny-app-aws.html#các-bước-thực-hiện",
    "href": "p10-02-deploy-shiny-app-aws.html#các-bước-thực-hiện",
    "title": "63  Deploy Shiny Apps lên AWS",
    "section": "63.2 Các bước thực hiện",
    "text": "63.2 Các bước thực hiện\n\nTạo và sử dụng AMI có cả RStudio và Shiny Server (trong chương trước), lưu ý cần enable HTTP4 port 80 để có thể access.\nTạo key để có thể truy cập server từ local. Sau khi tạo xong, ta có thể truy cập vào máy remote như sau\n\nssh -i \"rstudio-shiny-server.pem\" ubuntu@ec2-54-169-222-143.ap-southeast-1.compute.amazonaws.com\n\nChạy R với quyền root & install để cả shiny & rstudio có thể cùng sử dụng. Nếu cài đặt packages trong RStudio, khi đó, ta đang cài đặt package với user rstudio, rấ có thể các package trong Shiny Server vẫn chưa được cài đặt\n\nsudo -i\nR\ninstall.packages(\"install packages\")\n\nChạy sample app shiny\n\nsudo /opt/shiny-server/bin/deploy-example default\n\nCopy app mới tạo\n\nsudo cp -r /home/rstudio/ShinyApps/sample-apps/basic-app/ /srv/shiny-server/sample-apps/\n\nSửa file config, thêm dòng preserve_logs true; trong đầu file config và save lại. Shiny sẽ lưu lại các log khi chạy bị lỗi.\n\nsudo nano /etc/shiny-server/shiny-server.conf\n\nXem log shiny\n\ncd /var/log/shiny-server\nsudo tail ten_file_log\n\nRestart hệ thống\n\nsudo systemctl restart shiny-server"
  },
  {
    "objectID": "p10-02-deploy-shiny-app-aws.html#cách-cài-đặt-python-in-shiny-server",
    "href": "p10-02-deploy-shiny-app-aws.html#cách-cài-đặt-python-in-shiny-server",
    "title": "63  Deploy Shiny Apps lên AWS",
    "section": "63.3 Cách cài đặt Python in shiny server",
    "text": "63.3 Cách cài đặt Python in shiny server\nVào root user\nsudo apt-get install python3-venv\n\napt install python3-pip\n\npip3 install --user virtualenv\n\nvirtualenv python_environment\n\nvirtualenv -p /usr/bin/python3.6 python_environment\n\nsource python_environment/bin/activate"
  },
  {
    "objectID": "p10-02-deploy-shiny-app-aws.html#một-số-câu-lệnh-linux",
    "href": "p10-02-deploy-shiny-app-aws.html#một-số-câu-lệnh-linux",
    "title": "63  Deploy Shiny Apps lên AWS",
    "section": "63.4 Một số câu lệnh Linux",
    "text": "63.4 Một số câu lệnh Linux\n\nls -l: Hiển thị tên file và thư mục\nchmode 777: Change mode, cho phép read, write & execute\nssh -i: Truy cập vào Linux server sau khi có mã .pem\nsudo: Super User Do\ncp -r: Copy & force write\nscp -i \"your-key-pair.pem\" -r username@ec2-ip-compute.amazonaws.com:/home/username/dirtocopy .: Copy file từ instance về local"
  },
  {
    "objectID": "p10-02-deploy-shiny-app-aws.html#tài-liệu-tham-khảo",
    "href": "p10-02-deploy-shiny-app-aws.html#tài-liệu-tham-khảo",
    "title": "63  Deploy Shiny Apps lên AWS",
    "section": "63.5 Tài liệu tham khảo",
    "text": "63.5 Tài liệu tham khảo\n\nhttps://abndistro.com/post/2019/07/06/deploying-a-shiny-app-with-shiny-server-on-an-aws-ec2-instance/\nhttps://towardsdatascience.com/how-to-host-a-r-shiny-app-on-aws-cloud-in-7-simple-steps-5595e7885722\nhttp://freigeist.devmag.net/r/773-deploying-shiny-server-on-amazon-some-troubleshoots-and-solutions.html\nhttps://www.alpha-epsilon.de/r/2018/07/22/an-auto-scaling-shiny-server-on-aws/\nInstall virtual environment: https://gist.github.com/Geoyi/d9fab4f609e9f75941946be45000632b\nUltimate guide to deploy Shiny App: https://www.charlesbordet.com/en/guide-shiny-aws/#5-extra-create-a-nice-domain-name\nScaling Shiny: https://www.alpha-epsilon.de/r/2018/07/22/an-auto-scaling-shiny-server-on-aws/"
  },
  {
    "objectID": "p10-03-web-service-azure.html#log-in-vào-azure",
    "href": "p10-03-web-service-azure.html#log-in-vào-azure",
    "title": "64  Xây dựng web service với Azure",
    "section": "64.1 Log in vào Azure",
    "text": "64.1 Log in vào Azure\n\nlibrary(AzureML)\nlibrary(tidyverse)\n\n#Tạo workspace\nsessionInfo()\nws <- workspace(\n  id = \"2052e86780c342fcb7cdf5a7179b447a\",\n  auth = \"76186c54e53443e38b78226ebb5f2873\"\n)"
  },
  {
    "objectID": "p10-03-web-service-azure.html#tạo-web-service",
    "href": "p10-03-web-service-azure.html#tạo-web-service",
    "title": "64  Xây dựng web service với Azure",
    "section": "64.2 Tạo web service",
    "text": "64.2 Tạo web service\n\nCài đặt Rtools\nĐiều chỉnh Path cho zip tool & khởi động lại R (tắt RStudio và bật lại)\n\n\nSys.which(\"zip\")\nRtools.bin=\"C:\\\\RBuildTools\\\\3.4\\\\bin\"\nsys.path = Sys.getenv(\"PATH\")\nif (Sys.which(\"zip\") == \"\" ) {\n  system(paste(\"setx PATH \\\"\", Rtools.bin, \";\", sys.path, \"\\\"\", sep = \"\"))\n}\n\n\nKết quả sau khi khởi động lại R\n\n\nSys.which(\"zip\")"
  },
  {
    "objectID": "p10-03-web-service-azure.html#xây-dựng-api-đơn-giản",
    "href": "p10-03-web-service-azure.html#xây-dựng-api-đơn-giản",
    "title": "64  Xây dựng web service với Azure",
    "section": "64.3 Xây dựng API đơn giản",
    "text": "64.3 Xây dựng API đơn giản\n\nadd <- function(x, y){\n  x + y\n}\n\napi <- publishWebService(\n  ws,\n  fun = add, \n  name = \"AzureML-vignette-silly\",\n  inputSchema = list(\n    x = \"numeric\", \n    y = \"numeric\"\n  ), \n  outputSchema = list(\n    ans = \"numeric\"\n  )\n)\n\n\napi %>% class\napi %>% names\n\n#Tìm kiếm document\n(helpPageUrl <- api$HelpLocation)\n\n#ServiceID\napi$WebServiceId\n\n#Tìm kiếm thông tin\n\n(webservices <- services(ws, name = \"AzureML-vignette-silly\"))\n\n#Sử dụng webservice\ndf <- data.frame(\n  x = 1:5,\n  y = 6:10\n)\ns <- services(ws, name = \"AzureML-vignette-silly\")\ns <- tail(s, 1) # Sử dụng services mới nhất\nep <- endpoints(ws, s)\n\nconsume(ep, df)\n\n\nXóa webservice\n\n\ndeleteWebService(ws, name = \"AzureML-vignette-silly\")"
  },
  {
    "objectID": "p10-03-web-service-azure.html#xây-dựng-api-web-service-cho-mô-hình-dự-báo",
    "href": "p10-03-web-service-azure.html#xây-dựng-api-web-service-cho-mô-hình-dự-báo",
    "title": "64  Xây dựng web service với Azure",
    "section": "64.4 Xây dựng API web service cho mô hình dự báo",
    "text": "64.4 Xây dựng API web service cho mô hình dự báo\n\nlibrary(MASS)\nlibrary(gbm)\n\ntest <- Boston[1:5, 1:13]\nset.seed(123)\n\ngbm1 <- gbm(medv ~ .,\n            distribution = \"gaussian\",\n            n.trees = 5000,\n            interaction.depth = 8,\n            n.minobsinnode = 1,\n            shrinkage = 0.01,\n            cv.folds = 5,\n            data = Boston,\n            n.cores = 1) # You can set this to n.cores = NULL to use all cores\n\nbest.iter <- gbm.perf(gbm1, method=\"cv\", plot=FALSE)\n\nmypredict <- function(newdata){\n  require(gbm)\n  predict(gbm1, newdata, best.iter)\n}\n\n# Example use of the prediction function\nprint(mypredict(test))\n\nep <- publishWebService(ws = ws, \n                        fun = mypredict, \n                        name = \"AzureML-vignette-gbm\",\n                        inputSchema = test) #Thêm định dạng của dataframe trong input\n\n# Sử dụng service\nprint(consume(ep, test))"
  },
  {
    "objectID": "p10-03-web-service-azure.html#tài-liệu-tham-khảo",
    "href": "p10-03-web-service-azure.html#tài-liệu-tham-khảo",
    "title": "64  Xây dựng web service với Azure",
    "section": "64.5 Tài liệu tham khảo",
    "text": "64.5 Tài liệu tham khảo\n\nHướng dẫn AzureML https://cran.r-project.org/web/packages/AzureML/vignettes/getting_started.html"
  },
  {
    "objectID": "p10-09-apis-voi-plumber.html#cách-xây-dựng-api",
    "href": "p10-09-apis-voi-plumber.html#cách-xây-dựng-api",
    "title": "65  API với Plumber",
    "section": "65.1 Cách xây dựng API",
    "text": "65.1 Cách xây dựng API\nCấu trúc API do 2 phần.\n\nFile script R chứa các hàm\nFile run script\n\nFile plumber.R: Chứa các hàm cần trả APIs\n\n# plumber.R\n\n#' @get /mean\n#' @post /mean \nnormalMean <- function(samples=10){\n  data <- rnorm(samples)\n  mean(data)\n}\n\n#' @post /sum\n#' @get /sum\naddTwo <- function(a=8, b=9){\n  as.numeric(a) + as.numeric(b)\n}\n\nlibrary(plumber)\nlibrary(ggplot2)\n#' @get /histogram_test\n#' @png\nhistogram_test <- function(){\n  mtcars=mtcars\n  b=ggplot(mtcars,aes(mtcars$cyl))\n  print(b+geom_bar())\n}\n\n#' @get /plot\n#' @png\nmy_plot <- function(samples=100){\n  print(hist(rnorm(samples, 3,1)))\n}\n\nlibrary(dygraphs)\nlibrary(htmlwidgets)\n#' @post /my_html\n#' @get /my_html\n#' @serializer htmlwidget\nmy_html <- function(){\n  library(dygraphs)\n  lungDeaths <- cbind(mdeaths, fdeaths)\n  out <- dygraph(lungDeaths)\n  print(out)\n}\n\nFile rum_plumber.R: run server để trả APIs\n\nlibrary(plumber)\nr <- plumb(\"plumber.R\")\nr$run(port=8000)"
  },
  {
    "objectID": "p10-09-apis-voi-plumber.html#cách-sử-dụng-kết-quả-từ-web",
    "href": "p10-09-apis-voi-plumber.html#cách-sử-dụng-kết-quả-từ-web",
    "title": "65  API với Plumber",
    "section": "65.2 Cách sử dụng kết quả từ web",
    "text": "65.2 Cách sử dụng kết quả từ web\n\nhttp://127.0.0.1:8000/sum?a=9&b=10"
  },
  {
    "objectID": "p10-09-apis-voi-plumber.html#case-study",
    "href": "p10-09-apis-voi-plumber.html#case-study",
    "title": "65  API với Plumber",
    "section": "65.3 Case study",
    "text": "65.3 Case study\n\nXem thêm use case sử dụng docker để tạo APIs dự báo mô hình"
  },
  {
    "objectID": "p10-09-apis-voi-plumber.html#tài-liệu-tham-khảo",
    "href": "p10-09-apis-voi-plumber.html#tài-liệu-tham-khảo",
    "title": "65  API với Plumber",
    "section": "65.4 Tài liệu tham khảo",
    "text": "65.4 Tài liệu tham khảo\n\nhttps://www.rplumber.io/"
  },
  {
    "objectID": "p11-01-git.html#giới-thiệu-về-git",
    "href": "p11-01-git.html#giới-thiệu-về-git",
    "title": "66  Sử dụng GIT",
    "section": "66.1 Giới thiệu về Git",
    "text": "66.1 Giới thiệu về Git\nGit là công cụ cho phép quản lý nhiều version của code. Việc sử dụng git thành thạo sẽ giúp gia tăng hiệu quả quản lý version, source code. Luồng làm việc với git có thể chia làm 2 nhóm lớn:\n\nLàm việc trên local - cho phép quản lý và xử lý các version khác nhau của code do chính mình tạo\nLàm việc trên github - cho phép quản lý và tương tác với nhiều thành viên/project khác nhau\nMỗi folder chứa 1 file .git lưu lại toàn bộ lịch sử cả project\nMột project có chưa các file, foder, lịch sử của cả project được gọi là 1 repository\nKhi lưu file trong directory nhưng chưa commit, toàn bộ các file được đưa vào stagging area. Sau khi commit, lịch sử các file mới được lưu lại tại .git"
  },
  {
    "objectID": "p11-01-git.html#git-cơ-bản",
    "href": "p11-01-git.html#git-cơ-bản",
    "title": "66  Sử dụng GIT",
    "section": "66.2 Git cơ bản",
    "text": "66.2 Git cơ bản\n\n66.2.1 Các khái niệm cơ bản\n\nrepository ((repo)): Một cơ sở dữ liệu (database)/folder có chứa tất cả các checkpoint (các điểm thay đổi file) cùng với metadata.\nbranch: Các version khác nhau của 1 repo\nmaster: Branch gốc\nadd: Là việc add file(s) vào repo\ncommit: Là việc xác nhận sự thay đổi thông tin trong repo (sửa file, thêm file mới, etc.)\nremote: Là việc link đến một repo gốc trên một máy tính khác\nmerging: Là việc kết hợp các version từ các branch khác nhau lại thành một\nconflict: Là việc version trên các branch\n\nLưu ý:\n\nKhi chỉ commit, git sẽ yêu cầu nhâp message thay đổi file\nCác phần comment bắt đầu với # sẽ không lưu\n\n\n\n66.2.2 Luồng làm việc của GIT khi làm việc online\n\nfork: Copy một project sang account github của cá nhân\nclone: Download project từ github xuống local\nadd: Thêm file vào project\ncommit: Là việc xác nhận sự thay đổi thông tin trong repo (sửa file, thêm file mới, etc.)\npush: Đẩy version gần nhất được commit từ local lên github\npull: Download các commit mà những thành viên khác đã tạo và tích hợp với bản gốc\n\n\n\n\n66.2.3 Cơ chế lưu trữ thông tin của Git\nThông tin cơ bản: Git sử dụng cấu trúc dữ liệu nhiều lớp để lưu trữ lịch sử thông tin\n\nBlob: Unique version của mỗi file\nTree: theo dõi tên và địa chỉ của mỗi file\nCommit: Lưu trữ các thông tin với mỗi version\n\n\n\n66.2.3.1 Hash\n\nMỗi commit có một ID gọi là hash\nDefault khi sử dụng git show sẽ ra version gần nhất, nếu muốn version tiếp theo sẽ sử dụng HEAD~2, HEAD~3\n\n\n\n66.2.3.2 Branch\n\nGit cho phép tạo các branch, mỗi branch là một folder từ master và hoạt động độc lập với nhau. Branch chỉ ảnh hưởng khi merge lại với nhau\nGit sẽ tạo 1 branch master mặc định\nMột số trường hợp, branch có các conflict\n\n\n#Xem branch\ngit branch #Current branch sẽ hiển thị sau dấu *\n\n# Xem sự khác biệt giữa các branch\ngit diff branch1..branch2\n\n# Switch giữa các branch\ngit checkout branch-name\n\n# Tạo branch\ngit checkout -b branch_name\n\n# Merge branch\ngit merge source destination -m \"message\"\n\n\n\n\n66.2.4 Các câu lệnh cơ bản\n\n# Tạo git project\ngit init \ngit init path_project\n\n# Clone new project vào thư mục dental\ngit clone file:///home/thunk/repo dental\n\n#Kiểm tra các file nào đang ở stagging area\ngit status\n\n# So sánh trạng thài các file\ngit diff file.rename\n\n#So sánh trạng thái tất cả các file\ngit diff\n\n#Add file vào stagging area\ngit add filename\n\n# Add tất cả các file\ngit add .\n\n# So sánh trạng thái của file trong stagging area\ngit diff -r HEAD data/northern.csv\n\nHEAD: version gần nhất\n\n\n#Commit\ngit commit -m \"Message to commit\"\n\n# Kiểm tra log\ngit log\n\ngit log --oneline\n\n# Thoát log: Q + Enter\n\n# Kiểm tra log 1 file\ngit log file_path\n\n\n# Kiểm tra thông tin gần nhất\ngit show\n\n# Kiểm tra thông tin trên mỗi commit\ngit show hash #Không cần thể hiện hết thông tin của hash, chỉ cần vài ký tự đầu\n\n# Kiểm tra sự thay đổi trong version thứ 2\ngit show HEAD~2\n\n# So sánh các version\ngit diff hash1..hash2\ngit diff 71ee..78e4\n\n# Xóa file không track\nls\ngit clean -f\n\n# Undo changes file chưa staged (file mới lưu Ctrl+S nhưng chưa add\n)\ngit checkout -- filename\n\n# Unstage với file đât được staged\ngit reset HEAD filename\n\n# Reset version cũ của một file\ngit checkout hash filename\ngit checkout abd78 report.txt\ngit commit -m \"newversion\"\n\n#Rest toàn bộ file về version gần nhất\ngit reset HEAD .\n\n# Merge một branch với branch đang có\ngit merge branch_name\n\n\nCommit tất cả các file\n\n\ngit commit --message \"Commit message\" -a\n\n\nRevert một file\n\n\ngit checkout SHA \"file to roll back\"\ngit checkout ab0a6de7 \"Github with R.Rmd\"\n\n\nRevert tất cả các file\n\n\ngit checkout SHA *\ngit checkout 2708f7c4 *"
  },
  {
    "objectID": "p11-01-git.html#luồng-làm-việc-cơ-bản-với-git-trên-local",
    "href": "p11-01-git.html#luồng-làm-việc-cơ-bản-với-git-trên-local",
    "title": "66  Sử dụng GIT",
    "section": "66.3 Luồng làm việc cơ bản với git trên local",
    "text": "66.3 Luồng làm việc cơ bản với git trên local\n\n66.3.1 Tạo branch & merge conflict\n\n# Kiểm tra các branch\ngit branch -l\n# Check out và tạo branch mới (ducanh)\ngit checkout -n ducanh\n# Add file\ngit add .\n# Commit\ngit commit -m \"Test\"\n# Check out sang master\ngit checkout master\n# Merge\ngit merge ducanh\n# Resolve conflict nếu cần thiết\ngit add .\ngit commit \"Resolve conflict\"\n\n# Delte branch nhánh\ngit branch -d ducanh\n\n\n\n66.3.2 Revert về version cũ\n\n# Kiểm tra version\ngit log --oneline\n# Roll back 1 file sang version cũ\ngit checkout 19dd151 test.txt\n# Roll back tất cả file\ngit checkout 19dd151 .\n\n# Add lại và commit\ngit add test.txt\n# Commit\ngit commit -m \"Rolling back\"\n# Kiểm tra lại\ngit log --oneline\n\nHướng dẫn sử dụng Git với R\n\nKhi tạo project, chọn version control từ Project option là Git"
  },
  {
    "objectID": "p11-01-git.html#làm-việc-nhóm-trên-github",
    "href": "p11-01-git.html#làm-việc-nhóm-trên-github",
    "title": "66  Sử dụng GIT",
    "section": "66.4 Làm việc nhóm trên Github",
    "text": "66.4 Làm việc nhóm trên Github\n\n66.4.1 Luồng làm việc với central workflow\nVới luồng làm việc tạo central workflow, manager cần thực hiện:\n\nTạo repository trên github\nAdd collaborator với quyền write trên github\n\n\nTrong luồng làm việc này, có một số lưu ý sau:\n\nProject được lưu trữ trên github được gọi là origin, các repository trên máy được gọi là local.\nCollaborator có thể tương tác với project trên git theo 2 cách:\n\nKéo project về máy local git pull origin master\nĐẩy các thay đổi từ local lên git git push origin master\n\nCác luồng xử lý trên local thực hiện như thông thường\n\nLuồng làm việc cơ bản\nCollaborator 1\n\n# Clone\ngit clone repository\n\n# Pull changes made by others\ngit pull origin master\n\n# Tạo sự thay đổi, solve conflict\ngit add .\ngit commit --no-edit\n\n# Push vào master\ngit push origin master\n\n# Push vào branch mới\ngit push origin -u new-chart\n\nCollaborator 2\n\n# Download commit\ngit fetch origin \n\n# Swich sang branch new-chart\ngit checkout new-chart\n\n# Chỉnh sửa code\n\n# Add, commit và merge\ngit add .\ngit commit -m \"Chỉnh sửa chart\"\n\n# Push vào origin\ngit push origin new-chart\n\nMaster\n\n# Pull thay đổi của new-chart\ngit pull origin new-chart\n\n# Merge\ngit checkout master\ngit merge new-chart\n\n# Fix conflict\n\n# Push origin\ngit push orign master\n\n# Delete branch new-chart local\ngit branch -d new-chart \n\n# Delete branch new-chart github\ngit push origin -d new-chart\n\nLưu ý:\n\ngit pull là cách đi ngắn gọn của git fetch và git merge\nLuồng làm việc này phù hợp với làm việc trong team nhỏ\n\n\n\n66.4.2 Luồng làm việc online theo fork\nLuồng làm việc này, mỗi collaborator sẽ fork project về và tạo pull request để yêu cầu thay đổi.\n\nLuồng làm việc tối thiểu\n\nFork project trên Github về account trước.\nClone về local: git clone http://github.com/myaccount/project-xxx\nTạo branch riêng để phát triển: git checkout -b feature-01\nCode loằng ngoằng xong thì commit lên branch riêng đấy: git commit -m \"Feature xong het roi.\"\nGộp vào nhánh source code chính: git checkout master && git merge feature-01\nĐẩy code lên account cá nhân: git push origin master\nVào link project gốc thấy cái nút Pull Request , click vào để điền nội dung merge code vào project chính.\nNgười quản lý project chính, thấy có ông nào đó PR (Pull Request) lên, sẽ kiểm tra code, nếu được thì merge và push lên mainstream.\n\nLưu ý:\n\nLuồng làm việc này phù hợp với các project cộng đồng"
  },
  {
    "objectID": "p11-01-git.html#các-tricks-khi-làm-việc-với-git",
    "href": "p11-01-git.html#các-tricks-khi-làm-việc-với-git",
    "title": "66  Sử dụng GIT",
    "section": "66.5 Các tricks khi làm việc với git",
    "text": "66.5 Các tricks khi làm việc với git\n\n66.5.1 Chặn proxy git\n\ngit config --global http.proxy http://username:password@proxy.server.com:8080 \ngit config --global https.proxy http://username:password@proxy.server.com:8080  \n#If password contain @ replace by %40 \n\n\n git config --global https.proxy http://anhhd3:VPBank2016*@10.128.10.88:8080\n git config --global http.proxy http://anhhd3:VPBank2016*@10.128.10.88:8080\n\n\n\n66.5.2 Tạo gitignore\n.gitignore là file quản lý các dạng file mà ta không muốn đẩy lên github.\n\n# Tạo file git\ntouch gitignore\n\nSau khi tạo .gitignore, ta có thể đưa các định dạng file không muốn đưa lên Github (mặc dù vẫn tồn tại ở local master). Ví dụ các file trong .gitignore như sau.\n*.Rmd\n*.Rda\n*.xlsx\n\n\n66.5.3 Set config bị lỗi\nKhi set config bị lỡi, có thể reset lại như sau\n\ngit config --global --unset credential.helper\n\n\n\n66.5.4 Reset lại global git config\n\ngit config --global --edit\n\n\n\n66.5.5 Revert vs. Reset\n\nRevert: Revert opposite - do đó, KHÔNG dùng khi muốn revert version\nReset: Reset về version cũ, mất toàn bộ lịch sử\n\n\ngit revert 5sdf312\ngit reset 5sdf312\n\n\n\n66.5.6 Set-up local git server\n\nBonobo git server"
  },
  {
    "objectID": "p11-01-git.html#tài-liệu-tham-khảo",
    "href": "p11-01-git.html#tài-liệu-tham-khảo",
    "title": "66  Sử dụng GIT",
    "section": "66.6 Tài liệu tham khảo",
    "text": "66.6 Tài liệu tham khảo\n\nhttp://r-pkgs.had.co.nz/git.html\nhttp://happygitwithr.com/\nhttp://archive.fabacademy.org/archives/2016/doc/gitCheatSheet.html\nhttps://campus.datacamp.com/courses/introduction-to-git-for-data-science/repositories?ex=1\nHọc git online: https://learngitbranching.js.org/"
  },
  {
    "objectID": "p11-02-cmd.html#comment-trong-bat",
    "href": "p11-02-cmd.html#comment-trong-bat",
    "title": "67  CMD và Bash",
    "section": "67.1 Comment trong bat",
    "text": "67.1 Comment trong bat\n\nSử dụng REM\n\n```{bat}\n#| eval: false\nREM phần comment\n```"
  },
  {
    "objectID": "p11-02-cmd.html#chạy-file-script-python-từ-bat",
    "href": "p11-02-cmd.html#chạy-file-script-python-từ-bat",
    "title": "67  CMD và Bash",
    "section": "67.2 Chạy file script python từ bat",
    "text": "67.2 Chạy file script python từ bat\n```{bat}\n#| eval: false\npython fix_md.py\n```"
  },
  {
    "objectID": "p11-02-cmd.html#delete-tất-cả-các-file-và-folder",
    "href": "p11-02-cmd.html#delete-tất-cả-các-file-và-folder",
    "title": "67  CMD và Bash",
    "section": "67.3 Delete tất cả các file và folder",
    "text": "67.3 Delete tất cả các file và folder\n\ndel /S \"..\\sphinx-book\\_source\\*\" /F /Q"
  },
  {
    "objectID": "p11-02-cmd.html#convert-file-với-pandoc",
    "href": "p11-02-cmd.html#convert-file-với-pandoc",
    "title": "67  CMD và Bash",
    "section": "67.4 Convert file với pandoc",
    "text": "67.4 Convert file với pandoc\nCho phép convert các file markdown sang rst và giữa nguyên định dạng tên file\n```{bat}\n#| eval: false\nfor %%i in (*.md) do pandoc -f markdown -t rst %%~ni.md > ../sphinx-book/_source/%%~ni.rst\n```"
  },
  {
    "objectID": "p11-02-cmd.html#copy-nhiều-file-sang-folder-khác",
    "href": "p11-02-cmd.html#copy-nhiều-file-sang-folder-khác",
    "title": "67  CMD và Bash",
    "section": "67.5 Copy nhiều file sang folder khác",
    "text": "67.5 Copy nhiều file sang folder khác\n```{bat}\n#| eval: false\nset \"source=..\\sphinx-book\\_config\"\nset \"destination=..\\sphinx-book\\_source\"\n\nxcopy /s/Y \"%source%\" \"%destination%\" \n```"
  },
  {
    "objectID": "p11-02-cmd.html#xóa-nhiều-file-trong-folder",
    "href": "p11-02-cmd.html#xóa-nhiều-file-trong-folder",
    "title": "67  CMD và Bash",
    "section": "67.6 Xóa nhiều file trong folder",
    "text": "67.6 Xóa nhiều file trong folder\nXóa định dạng file nhất định trong 1 folder\n```{bat}\n#| eval: false\ndel /S \"..\\sphinx-book\\_build\\html\\*.md\" /F /Q\ndel /S \"..\\sphinx-book\\_build\\html\\*.html\" /F /Q\ndel /S \"..\\sphinx-book\\_build\\html\\*.js\" /F /Q\n```"
  },
  {
    "objectID": "p11-02-cmd.html#assign-quyền-trong-1-thư-mục",
    "href": "p11-02-cmd.html#assign-quyền-trong-1-thư-mục",
    "title": "67  CMD và Bash",
    "section": "67.7 Assign quyền trong 1 thư mục",
    "text": "67.7 Assign quyền trong 1 thư mục\nAssign quyền cho 1 user vào 1 thư mục\n```{bat}\n#| eval: false\nicacls \"D:\\01.aadwebsite\" /grant vpbank.com.vn\\truongnh3:(OI)(CI)R\n```"
  },
  {
    "objectID": "p11-02-cmd.html#copy-từ-1-folder-sang-folder-khác-bằng-bat",
    "href": "p11-02-cmd.html#copy-từ-1-folder-sang-folder-khác-bằng-bat",
    "title": "67  CMD và Bash",
    "section": "67.8 Copy từ 1 folder sang folder khác bằng bat",
    "text": "67.8 Copy từ 1 folder sang folder khác bằng bat\n```{bat}\n#| eval: false\nset \"source=D:\\1. DBS\\DBS team documents\\03.Tong hop SQL\\02.SQL website\\_site\"\nset \"destination=D:\\01.aadwebsite\\bi-knowledge\"\nset \"source_email=D:\\1. DBS\\DBS team documents\\03.Tong hop SQL\\02.SQL website\\\"\n\ndel /S \"%destination%\\*\" /F /Q\n\nxcopy /s \"%source%\" \"%destination%\"\n```"
  },
  {
    "objectID": "p11-02-cmd.html#set-working-directory-từ-file-bat",
    "href": "p11-02-cmd.html#set-working-directory-từ-file-bat",
    "title": "67  CMD và Bash",
    "section": "67.9 Set working directory từ file bat",
    "text": "67.9 Set working directory từ file bat\nVấn đề: Khi đặt task scheduler, chạy file CMD sẽ bị đặt về working directory mặc định của windows. Để file bat có thể chạy bình thường cần phải đổi working directory\nGiải pháp:\n\n%dp0: Cho phép lấy link folder của file bat\npushd: Force đổi directory\n\n```{bat}\n#| eval: false\ncd %~dp0\npushd %~dp0\n```"
  },
  {
    "objectID": "p11-03-docker.html#giới-thiệu",
    "href": "p11-03-docker.html#giới-thiệu",
    "title": "68  Sử dụng docker",
    "section": "68.1 Giới thiệu",
    "text": "68.1 Giới thiệu\nDocker cho phép tạo một hệ điều hành con (Linux) trong hệ điều hành đang chạy. Phục vụ các công việc:\n\nĐảm bảo tính tái sử dụng\nKhắc phục các nhược điểm của Windows\nCho phép scale-up khi đưa vào triển khai diện rộng. VD: Đưa các API dự báo từ R vào hoạt động"
  },
  {
    "objectID": "p11-03-docker.html#các-cài-đặt",
    "href": "p11-03-docker.html#các-cài-đặt",
    "title": "68  Sử dụng docker",
    "section": "68.2 Các cài đặt",
    "text": "68.2 Các cài đặt\n\nCác bước cài đặt:\n\nCài đặt docker-for-windows\nCài đặt docker-toolbox-for-windows. Lưu ý: chỉ cài đặt phần Git cho Ubuntu\n\nSau khi cài đặt xong kiểm tra:\n\ndocker version"
  },
  {
    "objectID": "p11-03-docker.html#cài-đặt-r-và-rstudio-trên-docker",
    "href": "p11-03-docker.html#cài-đặt-r-và-rstudio-trên-docker",
    "title": "68  Sử dụng docker",
    "section": "68.3 Cài đặt R và RStudio trên docker",
    "text": "68.3 Cài đặt R và RStudio trên docker\nSử dụng câu lệnh sau\ndocker run --rm -p 8787:8787 rocker/verse\nGiải thích:\n\n--rm: Khi quit container, sẽ quit container. Nếu không, mỗi lần sẽ lưu một version của container\n-p: Để Rstudio chạy trên port 8787\nRStudio Server sẽ chạy trên web với thông tin như sau:\n\nLink: http://localhost:8787\nPassword: rstudio\nUsername: rstudio"
  },
  {
    "objectID": "p11-03-docker.html#các-câu-lệnh-cơ-bản",
    "href": "p11-03-docker.html#các-câu-lệnh-cơ-bản",
    "title": "68  Sử dụng docker",
    "section": "68.4 Các câu lệnh cơ bản",
    "text": "68.4 Các câu lệnh cơ bản\n\n\n\n\n\n\n\n         Câu lệnh\n            Ý nghĩa\n\n\n\n\ndocker info\nThông tin docker\n\n\ndocker images\nLiệt kê các images đang có trong docker\n\n\ndocker ps\nDanh sách các docker đang chạy\n\n\ndocker stop image_name\nDừng image đang chạy\n\n\ndocker exec -it container_id bash\nChạy bash (ubuntu) trong docker\n\n\ndocker rmi container_name\nRemove images in docker\n\n\n\nKiểm tra java đã cài:\n\ndocker exec -it <container_id> bash\njava -version"
  },
  {
    "objectID": "p11-03-docker.html#lưu-ý",
    "href": "p11-03-docker.html#lưu-ý",
    "title": "68  Sử dụng docker",
    "section": "68.5 Lưu ý",
    "text": "68.5 Lưu ý\n\n68.5.1 Lưu package trong rocker\n\nLưu các packages đã cài trong R trong image:\n\nKhởi động powershell khác\ndocker commit -m \"some_comment\" <container_id> <<new_container_name>>\n\n\nVD: docker commit -m \"install h2o & dplyr\" 4a6a528b35da h2o_dplyr\n\nContainer ID phải được kiểm tra từ docker ps\nShare Drive trong Docker:\n\nBước 1: Tạo User mới: Computer Management >> System Tools >> Local Users and Groups >> Users >> Create new user (docker/123456) (set pasword never expired)\nBước 2: Chọn ổ cần share >> Give Access to >> Advanced Sharing >> Advanced Sharing >> Permission >> Add >>> Add thêm user docker >> Full Access\nBước 3: Khởi động lại máy tính\nBước 4: Chạy câu lệnh sau & login bằng user/pwd vừa tạo\n\ndocker run --rm -p 8787:8787 -v //d/docker:/home/rstudio h2o_dplyr\n\n\n\n68.5.2 Copy images sang máy khác\n\nBước 1: Chọn ổ muốn lưu cd D:\\docker\nBước 2: Lưu image: docker save -o **image_name.tar** **image_name**\nBước 3: Load image: docker load --input **image_name.tar**\n\nTa có thể nén lại dạng zip\ndocker save <docker image name> | gzip > <docker image name>.tar.gz\nzcat <docker image name>.tar.gz | docker load\nVí dụ:\n\n#Load về ổ D\nd:\n#Lưu lại dạng tar\ndocker save -o rstudio_ds.tar anhhd/rstudio_ds\n#Load images\nd:\ndocker load -i rstudio_ds.tar\n#Hoặc\ndocker load --input rstudio_ds.tar\n\n\n\n68.5.3 Cài đặt Java/H2O trong ubuntu\n\nInstall plumber: docker pull trestletech/plumber\nChạy chế độ bash trong Ubuntu: docker exec -it <container_id> bash\nCài đặt Java: apt-get install default-jre\nKhởi động R: R\nInstall package H2O như bình thường"
  },
  {
    "objectID": "p11-03-docker.html#publish-image-vào-docker-hub",
    "href": "p11-03-docker.html#publish-image-vào-docker-hub",
    "title": "68  Sử dụng docker",
    "section": "68.6 Publish image vào docker hub",
    "text": "68.6 Publish image vào docker hub\nTạo tài khoản trên docker\n\nBước 1: docker login - điền thông tin login\nBước 2: Tag thông tin image docker tag my_image $DOCKER_ID_USER/my_image\n\nVD: Tạo image là rstudio_ds, cần làm như sau docker tag rstudio_ds anhhd/rstudio_ds\n\nBước 3: Publish lên dockerhub docker push $DOCKER_ID_USER/my_image. VD docker push anhhd/rstudio_ds"
  },
  {
    "objectID": "p11-03-docker.html#tạo-một-image-để-tạo-rest-api-từ-r-model",
    "href": "p11-03-docker.html#tạo-một-image-để-tạo-rest-api-từ-r-model",
    "title": "68  Sử dụng docker",
    "section": "68.7 Tạo một IMAGE để tạo REST API từ R model",
    "text": "68.7 Tạo một IMAGE để tạo REST API từ R model\n\nBước 1: Tạo và lưu model dạng RDA vào folder/ hoặc mojo với H2O\nBước 2: Tạo Function dự báo từ dữ liệu input đầu vào\nBước 3: Sửa lại Dockerfile\nBước 4: Sử dụng powershell hoặc cmd, đổi cd vào thư mục chứa các file vừa tạo. Chạy câu lệnh sau.\n\ndocker build .\nLưu ý: Có dấu chấm\n\nBước 5: tag tên image vào container/image vừa tạo\n\ndocker tag <contaniner_id> <new tag name>\nVí dụ: Tham khảo thư mục test_plumber trong docker"
  },
  {
    "objectID": "p11-03-docker.html#tài-liệu-tham-khảo",
    "href": "p11-03-docker.html#tài-liệu-tham-khảo",
    "title": "68  Sử dụng docker",
    "section": "68.8 Tài liệu tham khảo",
    "text": "68.8 Tài liệu tham khảo\n\nR docker tutorial"
  },
  {
    "objectID": "p11-04-apis.html#giới-thiệu",
    "href": "p11-04-apis.html#giới-thiệu",
    "title": "69  API",
    "section": "69.1 Giới thiệu",
    "text": "69.1 Giới thiệu\nAPIs (Application Programing Interfaces) là cách thức luân chuyển thông tin, yêu cầu giữa người dùng với server và ngược lại. API có thể hiểu đơn giản là 1 phần mềm mini được thực hiện trên 1 máy tính khác (so với máy tính ta đang sử dụng). Phần này giới thiệu 1 số những điểm rất cơ bản để data scientist có thể sử dụng và làm việc với api.\nKhi các phần mềm làm việc với nhau, có 3 cấu phần chính:\n\nserver: Máy tính thực hiện (run) APIs\nAPI\nClient: Phần mềm/máy tính trao đổi thông tin với server thông qua APIs"
  },
  {
    "objectID": "p11-04-apis.html#protocol",
    "href": "p11-04-apis.html#protocol",
    "title": "69  API",
    "section": "69.2 Protocol",
    "text": "69.2 Protocol\nTập hợp các quy tắc mà các máy tính có thể trao đổi thông tin/service với nhau được gọi là protocol. Protocol rất đa dạng, có thể là Blootooth hay HTTP. Trong protocol có hai cấu phần:\n\nRequest: Client truyền thông tin cho server\nResponse: Server trả kết quả cho client\n\nRequest bao gồm 4 cấu phần:\n\nURL: Xác định nguồn lực (locator) của 1 service\nMethod: Khai báo cho server biết cần phải làm gì. Có 4 loại methods chính:\n\nget: Yêu cầu server nhận dữ liệu\npost: Yêu cầu server tạo dữ liệu (thông qua tính toán, query, …). Dữ liệu có thể là ảnh, text, số, bảng, …\nput: Yêu cầu server update dữ liệu\ndelete: Yêu cầu server xóa dữ liệu\n\nHeaders: Chứa metadata cho các request từ client\nBody: Chứa dữ liệu mà client muốn truyền cho server\n\nKhi server thực hiện yêu cầu từ client sẽ trả ra kết quả. Kết quả trả ra chứa status code, là một chuỗi 3 ký tự, thông báo về trạng thái của request;\n\n200: success\n404: Không tìm thấy"
  },
  {
    "objectID": "p11-04-apis.html#data-trong-api",
    "href": "p11-04-apis.html#data-trong-api",
    "title": "69  API",
    "section": "69.3 Data trong API",
    "text": "69.3 Data trong API\nData trong API thường được dùng với 2 dạng:\n\njson\nxml\n\n\n# JSON\n{\n    \"crust\": \"original\",\n    \"toppings\": [\"cheese\", \"pepperoni\", \"garlic\"],\n    \"status\": \"cooking\"\n}\n\n# XML\n\n<order>\n    <crust>original</crust>\n    <toppings>\n        <topping>cheese</topping>\n        <topping>pepperoni</topping>\n        <topping>garlic</topping>\n    </toppings>\n    <status>cooking</status>\n</order>"
  },
  {
    "objectID": "p11-04-apis.html#tài-liệu-tham-khảo",
    "href": "p11-04-apis.html#tài-liệu-tham-khảo",
    "title": "69  API",
    "section": "69.4 Tài liệu tham khảo",
    "text": "69.4 Tài liệu tham khảo\n\nAPI"
  },
  {
    "objectID": "p11-05-html-css.html#html",
    "href": "p11-05-html-css.html#html",
    "title": "70  Cơ bản về HTML và CSS",
    "section": "70.1 HTML",
    "text": "70.1 HTML\n\n70.1.1 HMTL là gì ?\nKhái niệm\nHTML là tên viết tắt của HyperText Markup Language: ngôn ngữ đánh dấu siêu văn bản\n\nHypertext (siêu văn bản): là một đoạn text bất kì nhưng có chứa link đến một nguồn thông tin khác (như một đoạn văn bản khác, một địa chỉ website, hình ảnh, âm thanh…)\n\nHTML là cầu nối giao tiếp giữa người dùng và trình duyệt, giúp trình duyệt hiểu được cách thức hiển thị của trang web.\n**HTML** giống như bộ xương của 1 website, giúp chúng ta xác định được bố cục của website bằng cách đánh dấu bằng các thẻ (Tag)\nMột tài liệu HTML được tạo bởi các HTML element và được quy định bằng các cặp thẻ (tag) (ví dụ <strong>) thường sẽ bao gồm 1 cặp là thẻ mở và thẻ đóng, tuy nhiên cũng có 1 số thẻ không có thẻ đóng .\nMột file html thường được lưu lại dưới đuôi .htm hoặc .html, việc đọc html sẽ do trình duyệt đảm nhận\nVai trò của HTML trong website\n\nVD 1 website bình thường có thể được tạo bởi các ngôn ngữ:\n\nHTML: xây dựng cấu trúc và định dạng các siêu văn bản\nCSS: định dạng các siêu văn bản thô từ HTML thành website có bố cục, màu sắc, ảnh nền…\nJavascript: Tạo các sự kiện tương tác với người dùng (VD: click chuột vào 1 ảnh thì phóng to)\nPHP: Xử lý và trao đổi dữ liệu từ máy chủ đến trình duyệt\nMySQL: quản trị CSDL\n\n\n\n70.1.2 Cấu trúc một đoạn HTML\nCấu trúc cơ bản\n```{html}\n  <tag> Nội dung khai báo </tag>\n```\nNgoài ra trong thẻ có thể chứa thuộc tính, sẽ đặt trong thẻ mở đầu. VD:\n```{html}\n  <form action=\"https://vnexpress.net\"> </form>\n```\nNote: Những ký tự nằm giữa dấu <!-- và dấu --> sẽ được coi là comment và trình duyệt sẽ bỏ qua không đọc.\nCác tag HTML quan trọng\nĐể soạn thảo văn bản HTML, chúng ta dùng Sublime Text:\n\nĐịnh dạng Text:\nPhân đoạn:\n\n```{html}\n<!-- Comment: thẻ p dùng để đánh dấu đoạn văn,\ntách riêng với những đoạn text bình thường-->\n<p>Đoạn văn bản</p>\n \n<!-- Comment: br là thẻ đơn, nên bạn không cần thẻ đóng -->\nXuống dòng với <br>\n \n <!-- Comment: trích dẫn từ nguồn khác -->\n<blockquote>\"Tôi chuẩn man\" ~ Cao Thái Sơn</blockquote>\n```\n\n\n70.1.3 Cấu trúc 1 file HTML cơ bản\nMột file html bao gồm 4 yếu tố chính:\n\nKhai báo loại tài liệu html (DOCTYPE)\nThẻ đóng và mở tài liệu HTML, toàn bộ nôi dung website được bao bọc trong cặp thẻ <html></html>\nThông tin website nằm trong cặp thẻ <head></head> - khai báo thông tin website, tên website, khai báo CSS, các đoạn Javascript và 1 số thông tin khác. Thông tin trong phần này thường sẽ không hiển thị lên trình duyệt.\nNội dung website nằm trong cặp thẻ <body></body> - đây là toàn bộ nội dung của website, phần sẽ hiển thị lên trình duyệt.\n\nVD 1 cây HTML:\n```{html}\n<!DOCTYPE html> \n<html>\n    <head>\n    <title>Đây là tiêu đề </title>\n    <!-- Thông tin sẽ bị ẩn --> \n    </head> \n    \n    <body>\n    <b>in đậm </b>\n    <i>in nghiêng </i>\n    </body>\n</html>\n```\n\n70.1.3.1 Các thẻ khai báo thông tin website cơ bản.\nKhai báo tên tài liệu với cặp thẻ <title>\nCặp thẻ <title></title> có tác dụng khai báo tên tài liệu của bạn đang soạn, thẻ này giúp hiển thị tên của tài liệu trên trình duyệt đồng thời các search engine như Google cũng dựa vào tên này để tìm kiếm tài liệu\nVD chúng ta đặt nội dung trong thẻ title là <title>File thực hành html - anhnt67</title> thì khi lên trình duyệt sẽ hiển thị như sau:\n\nKhai báo metadata với thẻ <meta>\nThẻ này khai báo metadata trong tài liệu html như mô tả, tên tác giả, bảng mã sử dụng. Thẻ này khá đặc biệt vì không có thẻ đóng mà sẽ có dấu /> ở cuối cùng\nThẻ <meta> luôn được khai báo kèm ít nhất là 1 thuộc tính và thuộc tính đó phải có giá trị.\nThuộc tính charset\nKhai báo cho trình duyệt biết bảng mã ký tự siêu văn bản bên trong tài liệu, hầu hết hiện nay đều sử dụng bảng mã UTF-8 cho mọi ngôn ngữ\n\n<meta charset=\"utf-8\" />\n\nThuộc tính name\nPhải khai báo đồng thời 2 thuộc tính name và content, trong đó content thiết lập nội dung cho thuộc tính name. VD:\n\n<meta name = \"auto\" content = \"Nguyễn Tuấn Anh\" />\n\nMột số thuộc tính name sẵn có:\n\nauthor: khai báo tên tác giả của tài liệu\ndescription: nhập mô tả tài liệu\nkeyword: nhập từ khóa tìm kiếm trên các website tìm kiếm.\n\n```{html}\n    <head>\n        <title>File thực hành html - anhnt67</title>\n        <meta charset = \"utf-8\" />\n        <meta name = \"author\" content = \"Nguyễn Tuấn Anh\" />\n        <meta name = \"description\" content = \"File training html - DBS AAD\">\n        <meta name = \"keywords\" content = \"hoc html, DBS aad, training\">\n    <!-- Thông tin sẽ bị ẩn --> \n    </head> \n```\n\n\n70.1.3.2 Các thẻ định dạng chữ viết và văn bản\nTiêu đề và đoạn văn bản\nTiêu đề (Headline) được định nghĩa bằng thẻ  - n từ 1 đến 6, cấp độ giảm dần từ <h1> đến <h6>\n\nSee the Pen xyNZEY by Tuan Anh Nguyen (@atula21592) on CodePen.\n\n\nĐoạn văn bản (Paragraph) được khai báo bằng cặp thẻ <p></p>, văn bản nằm trong đoạn văn bản này sẽ được coi là 1 đoạn văn bản, khi kết thúc 1 đoạn văn bản và sang 1 đoạn văn bản khác thì sẽ được cách nhau với 1 tỷ lệ nhất định. VD\n```{html}\n    <body>\n        <p>Đây là 1 đoạn văn bản, khai báo bằng thẻ <code>&lt;p&gt;</code></p>\n        <p>Đây cũng là 1 đoạn văn bản nhé</p>\n    </body>\n```\nCác thẻ định dạng chữ viết\n\nSee the Pen GYaora by Tuan Anh Nguyen (@atula21592) on CodePen.\n\n\n\n\n\n70.1.4 Tạo danh sách với HTML\nTrong HTML có ba kiểu danh sách (list type) đó là kiểu sắp xếp (ordered list), kiểu không sắp xếp (unordered list) và kiểu danh sách mô tả (description list). Cụ thể:\n\nKiểu sắp xếp (Ordered List): Là kiểu hiển thị một danh sách mà các mục con của nó được sắp xếp theo thứ tự bằng số hoặc chữ cái.\nKiểu không sắp xếp (Unordered List): Là kiểu hiển thị danh sách mà các mục con của nó sẽ không được sắp xếp theo thứ tự mà chỉ được đánh dấu bằng một ký tự đặc trưng.\nKiểu mô tả (Description List): Là kiểu hiển thị danh sách mà các mục con của nó sẽ không được đánh dấu thứ tự, nhưng sẽ có kèm theo một đoạn miêu tả.\n\n```{html}\n\n<!-- Ordered list --> \n        <ol>\n          <li>Mục con 1</li>\n          <li>Mục con 2</li>\n          <li>Mục con 3</li>\n          <li>Mục con 4</li>\n          <li>Mục con 5</li>\n        </ol>\n\n        <p>Một kiểu hiển thị khác của Ordered List</p>\n\n        <ol type=\"I\">\n          <li>Mục con 1</li>\n          <li>Mục con 2</li>\n          <li>Mục con 3</li>\n          <li>Mục con 4</li>\n          <li>Mục con 5</li>\n        </ol>\n\n    <!-- Unordered list -->         \n        <ul>\n          <li>Mục con 1</li>\n          <li>Mục con 2</li>\n          <li>Mục con 3</li>\n          <li>Mục con 4</li>\n          <li>Mục con 5</li>\n        </ul>\n\n        <p>Một kiểu hiển thị khác của Unordered List</p>\n\n        <ul style=\"list-style-type: square\">\n          <li>Mục con 1</li>\n          <li>Mục con 2</li>\n          <li>Mục con 3</li>\n          <li>Mục con 4</li>\n          <li>Mục con 5</li>\n        </ul>\n    \n    <!-- Description list -->       \n        <p>Một website gồm có</p>\n        <dl>\n          <dt>HTML</dt>\n          <dd>- khung xương của website.</dd>\n          \n          <dt>CSS</dt>\n          <dd>- bộ da của website</dd>\n          \n          <dt>Javascript</dt>\n          <dd>- bộ cánh của website</dd>\n          \n          <dt>Ngôn ngữ server-side (ASP.NET, PHP, RUBY,..)</dt>\n          <dd>- linh hồn của website</dd>\n        </dl>\n\n    <!--Xếp chồng danh sách -->\n        <ul>\n          <li>WordPress\n            <ul>\n              <li>WordPress Themes</li>\n              <li>WordPress Plugin</li>\n            </ul>\n          </li>\n          <li>Front-end\n            <ul>\n              <li>HTML</li>\n              <li>CSS</li>\n              <li>Javascript (jQuery, AngularJS,..)</li>\n            </ul>\n          </li>\n    </body>\n\n```\n\n70.1.4.1 Tạo liên kết với html\n```{html}\n<a href=\"http://vnexpress.net\">Báo hay nhất Việt Nam</a><br>\n\n<!-- Mở liên kết trong tab mới -->  \n<a href=\"http://vnexpress.net\" target =\"_blank\">Báo hay nhất Việt Nam</a>\n```\n\n\n\n70.1.5 Chèn ảnh vào html\n```{html}\n<img src=\"C:\\Users\\anhnt67\\OneDrive - VPBank\\HTML-CSS\\Image\\htmlcssjava.png\", alt = \"Đây là ảnh\">\n```"
  },
  {
    "objectID": "p11-05-html-css.html#css",
    "href": "p11-05-html-css.html#css",
    "title": "70  Cơ bản về HTML và CSS",
    "section": "70.2 CSS",
    "text": "70.2 CSS\n\n70.2.1 CSS là gì\nCSS là chữ viết tắt của Cascading Style Sheets , là ngôn ngữ được sử dụng để tìm và định dạng lại các phần tử được tạo ra bởi html. Hiểu đơn giản, nếu HTML đóng vai trò định dạng các phần tử trên website như việc tạo ra các đoạn văn bản, các tiêu đề, bảng,…thì CSS sẽ giúp tạo thêm “phong cách” cho các phần tử HTML đó như đổi màu sắc trang, đổi màu chữ, thay đổi cấu trúc,…\n\nCấu trúc một đoạn CSS\nMột đoạn css bao gồm 4 phần:\n```{css}\nvùng chọn {\n   thuộc tính : giá trị;\n   thuộc tính: giá trị;\n   .....\n}\n```\nĐầu tiên khai báo vùng chọn, sau đó các thuộc tính và giá trị sẽ nằm bên trong cặp dấu ngoặc nhọn {}.\n\n\n70.2.2 Nhúng CSS vào website\nCó 2 cách để nhúng file css vào file html:\n\nInline Styles: Nhúng trực tiếp vào tài liệu HTML thông qua cặp thẻ <style> </style>.\nExternal Styles: Tạo một tập tin .css riêng và nhúng vào tài liệu HTML thông qua cặp thẻ .\n\nTrong đó, Inline Styles thích hợp với việc chèn một số đoạn css ngắn - trình duyệt không mất nhiều thời gian để load file css. Còn External styles thích hợp khi cần chèn nhiều đoạn CSS.\n\n\n70.2.3 Vùng chọn trong CSS\nVùng chọn là khu vực mà bạn muốn áp dụng các quy tắc CSS. Ví dụ bạn muốn tăng kích thước font chữ của các thẻ h1 thì vùng chọn của bạn sẽ là h1.\nVÙng chọn của CSS có thể là bất cứ thứ gì, từ thẻ\n\nđến các thẻ con bên trong nó\nVùng chọn theo tên thẻ\nChọn toàn bộ các tên thẻ được gọi, nhược điểm là toàn bộ các thẻ HTML trong website được chọn trong CSS đều biến đổi theo\n```{css}\n-----Format cho vùng h1\nh1{\n    font-family: Arial;\n    color: yellow\n}\n```\nVùng chọn theo ID\nChúng ta có thể chọn một phần tử cụ thể dựa vào giá trị của thuộc tính id trong thẻ HTML, lưu ý là mỗi phần tử phải mang một id riêng biệt không trùng nhau.\nKhi gọi theo ID trong CSS thì nó phải có # đặt trước tên id để phân biệt với các loại vùng chọn khác\nVD:\n```{css}\n  #post-title {\n    color:\n```\nVùng chọn theo class\nKhác với ID, class có thể dùng nhiều lần cho 1 phần tử. Khi gọi class trong CSS thì dùng cấu trúc .tênclass\nVí dụ:\n```{css}\n.a{\n    text-align: right;\n}\n```\nVùng chọn thứ cấp\nVới vùng chọn này, chúng ta có thể chọn 1 phần tử con trong 1 phần tử html nào đó\nVí dụ:\n```{css}\n#menu li {\n  color: green;\n  list-style-type: square;\n}\n\n```\n\n\n70.2.4 Thuộc tính trong CSS\nCác thuộc tính cho text\nCác thuộc tính cơ bản nhất:\n\ntext-align: Căn lề văn bản.\ntext-decoration: Trang trí văn bản.\ntext-indent: Thêm khoảng trống trước văn bản theo chiều ngang từ trái sang phải.\ntext-shadow: Thêm bóng cho văn bản.\ntext-transform: Tùy chỉnh việc hiển thị chữ in hoa.\n\nCác thuộc tính cho font chữ\n\nĐặt font chữ bằng font-family\nThiết lập chữ viết được hiển thị dưới dạng in nghiêng hoặc bình thường, có 3 giá trị normal, italic, oblique\nIn đậm bằng font-weight\nThay đổi kích thước chữ bằng font-size\nMàu chữ bằng color"
  },
  {
    "objectID": "p11-10-nguyen-ly-thi-giac.html#nguyên-lý-thị-giác",
    "href": "p11-10-nguyen-ly-thi-giac.html#nguyên-lý-thị-giác",
    "title": "71  Nguyên lý thị giác và Business Dashboard",
    "section": "71.1 Nguyên lý thị giác",
    "text": "71.1 Nguyên lý thị giác"
  },
  {
    "objectID": "p11-10-nguyen-ly-thi-giac.html#business-intelli",
    "href": "p11-10-nguyen-ly-thi-giac.html#business-intelli",
    "title": "71  Nguyên lý thị giác và Business Dashboard",
    "section": "71.2 Business Intelli",
    "text": "71.2 Business Intelli\nBusiness Dashboard có 3 thuộc tính cơ bản sau:\n\nBao hàm câu trả lời cho mục tiêu của dashboard, không đòi hỏi người dùng phải suy nghĩ thêm\nThường chứa các biểu đồ, bảng biểu\nKhông chứa tất cả dữ liệu mà chỉ hiển thị những thông tin quan trọng nhất"
  },
  {
    "objectID": "p11-30-tim-hieu-he-thong.html#export-sample-từ-nhiều-bảng-ra-excel",
    "href": "p11-30-tim-hieu-he-thong.html#export-sample-từ-nhiều-bảng-ra-excel",
    "title": "72  Tìm hiểu hệ thống dữ liệu",
    "section": "72.1 Export sample từ nhiều bảng ra excel",
    "text": "72.1 Export sample từ nhiều bảng ra excel\n\n#Step 1: Load library & set wd\n\nsetwd(\"./01-internet-banking\") # Set directory to save files\n\nlibrary(RODBC)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(writexl)\n\nrm(list = ls())\nch <- odbcConnect(\"sql\")\n\n\n#Step 2: Generating table list\ndatabase <- \"M108.BICDATA_HIS\"\n\ntbl_query <- paste0(\"\n         SELECT \n         lst.name AS table_name\n         FROM \", database, \".[sys].[tables] lst\n         \")\ncat(tbl_query)\n\ntable.list <- sqlQuery(ch, tbl_query)\n\n# table.list <- table.list %>% \n#   filter(str_detect(table_name, \"I2b|EB|B2B\"))\n\ntable.list <- table.list %>% \n  filter(str_detect(table_name, \"I2B|EB|B2B\"))\n\n\n\n#Step 3: Create excel file for each table\n\nfor (i in table.list$table_name){\n  tryCatch({\n    print(i)\n    table_to_query <- paste0(database,\".dbo.\", i)\n    query <- paste0(\"select top 1000 * from \", table_to_query)\n    data <- sqlQuery(ch, query)\n    write_xlsx(data, \n              path = paste0(i, \".xlsx\"))\n  },\n  error = function(e){})\n}"
  },
  {
    "objectID": "p11-30-tim-hieu-he-thong.html#import-nhiều-files-vào-access",
    "href": "p11-30-tim-hieu-he-thong.html#import-nhiều-files-vào-access",
    "title": "72  Tìm hiểu hệ thống dữ liệu",
    "section": "72.2 Import nhiều files vào access",
    "text": "72.2 Import nhiều files vào access\n\nSub GetAllFiles()\nDim fd As Object\nDim strFilter As String\nDim lngItems As Long\nDim varFile As Variant\nDim FileName As String\n\nConst msoFileDialogOpen As Long = 3\nConst msoFileDialogViewDetails As Long = 2\n\nSet fd = FileDialog(msoFileDialogOpen)\n\nWith fd\n.AllowMultiSelect = True\n.InitialView = msoFileDialogViewDetails\n.InitialFileName = \"F:\\OneDrive - VPBank\\03.Kien thuc ngan hang\\03-kien-thuc-ngan-hang\\04-du-lieu\\01-internet-banking\\\"\n.Title = \"Select your file(s)\"\n.Filters.Clear\n.Filters.Add \"All Files\", \"*.*\"\n.Show\nEnd With\n\nFor Each varFile In fd.selecteditems\nOn Error Resume Next\nFileName = Mid(varFile, InStrRev(varFile, \"\\\") + 1, InStrRev(varFile, \".\") - InStrRev(varFile, \"\\\") - 1)\n    DoCmd.TransferSpreadsheet acImport, acSpreadsheetTypeExcel9, FileName, varFile, True\nNext\n\nEnd Sub"
  },
  {
    "objectID": "p11-30-tim-hieu-he-thong.html#vba-cho-phép-xuất-bảng-cột-và-ghi-chú-trong-access",
    "href": "p11-30-tim-hieu-he-thong.html#vba-cho-phép-xuất-bảng-cột-và-ghi-chú-trong-access",
    "title": "72  Tìm hiểu hệ thống dữ liệu",
    "section": "72.3 VBA cho phép xuất bảng, cột và ghi chú trong access",
    "text": "72.3 VBA cho phép xuất bảng, cột và ghi chú trong access\n\n\nSub ListTablesAndFields()\n     'Macro Purpose:  Write all table and field names to and Excel file\n     'Source:  vbaexpress.com/kb/getarticle.php?kb_id=707\n     'Updates by Derek - Added column headers, modified base setting for loop to include all fields,\n     '                   added type, size, and description properties to export\n     \n    Dim lTbl As Long\n    Dim lFld As Long\n    Dim dBase As Database\n    Dim xlApp As Object\n    Dim wbExcel As Object\n    Dim lRow As Long\n     \n     'Set current database to a variable adn create a new Excel instance\n    Set dBase = CurrentDb\n    Set xlApp = CreateObject(\"Excel.Application\")\n    Set wbExcel = xlApp.workbooks.Add\n     \n     'Set on error in case there are no tables\n    On Error Resume Next\n     \n    'DJK 2011/01/27 - Added in column headers below\n    lRow = 1\n    With wbExcel.sheets(1)\n        .Range(\"A\" & lRow) = \"Table Name\"\n        .Range(\"B\" & lRow) = \"Field Name\"\n        .Range(\"C\" & lRow) = \"Type\"\n        .Range(\"D\" & lRow) = \"Size\"\n        .Range(\"E\" & lRow) = \"Description\"\n    End With\n    \n     'Loop through all tables\n    For lTbl = 0 To dBase.TableDefs.Count\n         'If the table name is a temporary or system table then ignore it\n        If Left(dBase.TableDefs(lTbl).Name, 1) = \"~\" Or _\n        Left(dBase.TableDefs(lTbl).Name, 4) = \"MSYS\" Then\n             '~ indicates a temporary table\n             'MSYS indicates a system level table\n        Else\n             'Otherwise, loop through each table, writing the table and field names\n             'to the Excel file\n            For lFld = 0 To dBase.TableDefs(lTbl).Fields.Count - 1  'DJK 2011/01/27 - Changed initial base from 1 to 0, and added type, size, and description\n                lRow = lRow + 1\n                With wbExcel.sheets(1)\n                    .Range(\"A\" & lRow) = dBase.TableDefs(lTbl).Name\n                    .Range(\"B\" & lRow) = dBase.TableDefs(lTbl).Fields(lFld).Name\n                    .Range(\"C\" & lRow) = dBase.TableDefs(lTbl).Fields(lFld).Type\n                    .Range(\"D\" & lRow) = dBase.TableDefs(lTbl).Fields(lFld).Size\n                    .Range(\"E\" & lRow) = dBase.TableDefs(lTbl).Fields(lFld).Properties(\"Description\")\n                End With\n            Next lFld\n        End If\n    Next lTbl\n     'Resume error breaks\n    On Error GoTo 0\n     \n     'Set Excel to visible and release it from memory\n    xlApp.Visible = True\n    Set xlApp = Nothing\n    Set wbExcel = Nothing\n     \n     'Release database object from memory\n    Set dBase = Nothing\n     \nEnd Sub"
  },
  {
    "objectID": "p11-40-aws-overview.html#giới-thiệu",
    "href": "p11-40-aws-overview.html#giới-thiệu",
    "title": "73  AWS",
    "section": "73.1 Giới thiệu",
    "text": "73.1 Giới thiệu\nKhi làm việc và xây dựng kiến trúc công nghệ thông tin trên AWS, cần lưu ý các điểm sau:\n\nCompliance: Tuân thủ đúng pháp luật. VD: Dữ liệu khách hàng Amazons phải nằm trong lãnh thổ của Anh, do đó, không thể đặt tại Region của Mỹ\nLatency: Tốc độ truy cập giữa end user và data center, nên đặt gần nhau\nPricing: Giá các dịch vụ khác nhau của AWS\nService Avalability: Tính khả dụng của các dịch vụ khác nhau của AWS\n\nĐể sử dụng các dịch vụ của AWS, có 3 cách:\n\nManagement Console: Tương tác vào web console của AWS\nCommand Line Interface - CLI: Sử dụng syntax của AWS khi có ssh\nSDK: Tương tác qua SDK của các ngôn ngữ lập trình thông dụng như Python, Ruby,…\n\n\nAWS xây dựng mô hình chia sẻ trách nhiệm trong an ninh mạng (Share Responsibility Model), trong đó, AWS chịu trách nhiệm về hardware, service… thông qua hoạt động ảo hoá (virtualization). Người dùng chịu trách nhiệm về data, bảo mật truy cập software,…\n\nRoot user là quyền admin vào AWS và có quyền cao nhất. Khi sử dụng với quyền root, cần dùng multi factor security.\nAmazon IAM (Identity and Access Management)"
  },
  {
    "objectID": "p11-90-other.html#biến-local-host-thành-website",
    "href": "p11-90-other.html#biến-local-host-thành-website",
    "title": "74  Các tricks khác",
    "section": "74.1 Biến local host thành website",
    "text": "74.1 Biến local host thành website\nVấn đề: Tạo website local tại ổ D:/website và muốn share để các thành viên khác có thể truy cập khi gõ địa chỉ IP máy tính trên browser\nGiải quyết: Sử dụng Internet Information Services Manager\n\nB1: Bật IIS\nB2: Chuột phải vào Sites >> Add Website >> Add physical path\nB3: Restart lại website\n\nLưu ý: Có thể sửa port cho website trở thành port 8080"
  },
  {
    "objectID": "p11-90-other.html#gửi-email-từ-file-vbs",
    "href": "p11-90-other.html#gửi-email-từ-file-vbs",
    "title": "74  Các tricks khác",
    "section": "74.2 Gửi email từ file vbs",
    "text": "74.2 Gửi email từ file vbs\n\nMở notepad++, tạo file vbs (Visual Basic Script file)\n\nDim olApp,olns,olMail,olMailItem \n\nSet olApp = CreateObject(\"Outlook.Application\")\nSet olns = olApp.GetNamespace(\"MAPI\")\n\nSet olMail = olApp.CreateItem(olMailItem)\nolMail.To = \"dbs_analytics@vpbank.com.vn;thanhnm3@vpbank.com.vn;truongnh3@vpbank.com.vn\"\nolMail.Subject = \"Website updated\"\nolMail.Body = \"Dear all,\" & vbCrLf & \"Our website has updated!\"  & vbCrLf & \"Link: http://hon-bicc-pc0035/\"\n' olMail.Attachments.Add(\"Add attachment location here\")\nolMail.Send\n    \nSet olns = Nothing\nSet olMail = Nothing\nSet olApp = Nothing"
  },
  {
    "objectID": "p11-90-other.html#đợt-thời-gian-chờ-trước-khi-tắt-máy",
    "href": "p11-90-other.html#đợt-thời-gian-chờ-trước-khi-tắt-máy",
    "title": "74  Các tricks khác",
    "section": "74.3 Đợt thời gian chờ trước khi tắt máy",
    "text": "74.3 Đợt thời gian chờ trước khi tắt máy\n```{bat}\nREM 3000s\nShutdown -s -t 3000 \n```"
  },
  {
    "objectID": "p13-01-casestudy-truc-quan-hoa.html#xây-dựng-phễu-bán-hàng-theo-từng-nhóm",
    "href": "p13-01-casestudy-truc-quan-hoa.html#xây-dựng-phễu-bán-hàng-theo-từng-nhóm",
    "title": "75  Trực quan hóa dữ liệu",
    "section": "75.1 Xây dựng phễu bán hàng theo từng nhóm",
    "text": "75.1 Xây dựng phễu bán hàng theo từng nhóm\nTrong quá trình phân tích bán hàng, phếu bán hàng (sale funnel) là một kỹ thuật rất hữu dụng để trực quan hóa kết quả kinh doanh theo từng nhóm. Tuy nhiên, hiện ít có biểu đồ nào thể hiện được phễu bán hàng một cách hiệu quả trên R.\nTrong mục này, tác giả sẽ hướng dẫn một ví dụ thực tiễn trực quan hóa phễu bán hàng một cách hiệu quả.\nXem ví dụ điển hình về phễu bán hàng dưới đây\n\ndata <- read.table(textConnection(\n          c(\"step;segment1;segment2;segment3;total\n          1_visit;1806;11663;12641;26110\n          2_register;1143;6476;5372;12991\n          3_login;1806;11663;2694;16163\n          4_subscribe;21;3322;2694;6037\n          5_paid;259;422;41;722\")),\n        header = T, sep = \";\")\n# Dữ liệu\ndata\n\n                   step segment1 segment2 segment3 total\n1               1_visit     1806    11663    12641 26110\n2            2_register     1143     6476     5372 12991\n3               3_login     1806    11663     2694 16163\n4           4_subscribe       21     3322     2694  6037\n5                5_paid      259      422       41   722\n\n\nTrong tập dữ liệu trên, ta sẽ mô phỏng dữ liệu phếu bán hàng của 3 phân khúc khách hàng trên một trang thương mại điện tử mà trong đó, khách hàng sẽ đi qua năm bước khác nhau:\n\nGhé thăm website (visit)\nĐăng ký (register)\nĐăng nhập (login)\nĐăng ký cập nhật các thông tin sản phẩm (subscribe)\nMua hàng và trả tiền thành công (paid)\n\nĐể tạo một biểu đồ phễu bán hàng, ta sẽ thực hiện 3 bước lớn sau.\n\nTạo theme cho biểu đồ\nTạo các biểu đồ con cho phễu bán hàng\nKết hợp các biểu đồ để tạo thành phễu bán hàng hoàn chỉnh\n\n\n# Gọi library\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(forcats)\nlibrary(ggthemes)\n\n# Tạo theme trông cho chart\nfunnel_theme <- theme(axis.title = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.text.x = element_blank(),\n        legend.position = \"none\",\n        panel.grid = element_blank()\n        )\n\n# Phân rã dữ liệu\ndf <- data %>% melt(id.vars = \"step\")\n\n# Tạo biểu đồ chính\np1 <- df %>% \n  mutate(step = fct_rev(step)) %>% \n  filter(variable != \"total\") %>% \n  ggplot(aes(step, value)) +\n  geom_bar(aes(fill = variable), stat = \"identity\") +\n  facet_grid(~variable, scale = \"free\") +\n  coord_flip() + \n  geom_text(aes(label = value),\n            position = position_stack(vjust = .5)) +\n  scale_fill_tableau() +\n  theme_minimal() +\n  scale_y_sqrt() +\n  funnel_theme +\n  theme(plot.margin=grid::unit(c(0,0,0,0), \"mm\")) +\n  theme(\n  axis.text.y = element_blank(),\n  strip.text = element_text(size = 14, \n                            face = \"bold\")) +\n  theme(\n    panel.spacing = unit(0, \"mm\")) +\n  annotate(\"rect\", xmin = 0.5, xmax = 1.5, ymin = 0, ymax = Inf,\n           alpha = .2) +\n  annotate(\"rect\", xmin = 2.5, xmax = 3.5, ymin = 0, ymax = Inf,\n           alpha = .2) +\n  annotate(\"rect\", xmin = 4.5, xmax = 5.5, ymin = 0, ymax = Inf,\n           alpha = .2) +\n  theme(axis.text.y = element_blank())\np1\n\n\n\n\n\nTạo thêm phần label tổng theo từng segment\n\n\ndf %>%\n  mutate(step = fct_rev(step)) %>%\n  filter(variable == \"total\") %>%\n  ggplot(aes(step, 0)) +\n  geom_label(aes(label = value),\n             col = \"white\",\n             fill = \"darkred\",\n             size = 4) +\n  coord_flip() +\n  facet_wrap(~variable) +\n  theme_minimal() +\n  theme(axis.text = element_blank()) +\n  funnel_theme +\n  theme(\n    strip.text.x = element_blank()\n  ) -> p2\np2\n\n\n\n\n\nTạo thêm thứ tự các bước trong phễu bán hàng để dễ theo dõi hơn\n\n\ndf2 <- data.frame(step = data$step,\n                  value = 1:5)\ndf2 %>% \n  mutate(step = fct_rev(step)) %>% \n  ggplot(aes(step, 1)) +\n  geom_hline(yintercept = 1) +\n  geom_point(size = 10, col = \"darkgreen\") +\n  geom_text(aes(label = value), \n            col = \"white\") +\n  coord_flip() +\n  theme_minimal() +\n  funnel_theme +\n  theme(\n    axis.text = element_text(size = 14)\n  ) -> p3\np3\n\n\n\n\n\nCuối cùng, ta có thể tạo ghép các biểu đồ rời rạc để tạo thành phễu bán hàng hoàn chỉnh. Việc kết hợp các biểu đồ trên ggplot2 có thể hoàn thành một cách đơn giản với ggplot2\n\n\n#devtools::install_github(\"thomasp85/patchwork\")\nlibrary(patchwork)\np3 + \n  labs(title = \"Sale funnel for 3 segments\") +\n  p1 + p2 + \n  plot_layout(nrow = 1, widths = c(1, 8, 1))\n\n\n\n\nNhư vậy, chúng ta đã hoàn thành phễu bán hàng rất chuyên nghiệp với ggplot2. Phễu bán hàng này đặc biệt hiệu quả khi cùng lúc phải so sánh nhiều phân khúc khách hàng khác nhau trên toàn bộ chuỗi bán hàng."
  },
  {
    "objectID": "p13-01-casestudy-truc-quan-hoa.html#vẽ-biểu-đồ-warterfall-cho-aciveinactive-users",
    "href": "p13-01-casestudy-truc-quan-hoa.html#vẽ-biểu-đồ-warterfall-cho-aciveinactive-users",
    "title": "75  Trực quan hóa dữ liệu",
    "section": "75.2 Vẽ biểu đồ warterfall cho acive/inactive users",
    "text": "75.2 Vẽ biểu đồ warterfall cho acive/inactive users\nTrong kỷ nguyên số, chỉ số active user (tạm dịch: người dùng thường xuyên hoạt động) là chỉ số đặc biệt quan trọng với bất kỳ website/ app nào. Công thức tính chỉ số người dùng thường xuyên hoạt động tại khoảng thời gian t được tính như sau:\n\\[active_{t} = active_{t-1} + new_{t} - churn_{t}\\]\nVí dụ về waterfall chart được lấy từ ví dụ của Tableau tại đường link: [https://public.tableau.com/views/CH24_BBOD_ChurnTurnover/SubscriberChurnAnalysis]\nTrong case study này, chúng ta sẽ tìm cách xây dựng một biểu đồ waterfall chart tương tự\n\n# Load library\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(lubridate)\nlibrary(grid)\nlibrary(gridExtra)\n\n# Tạo dữ liệu giả lập\nset.seed(123)\ndata <- data.frame(date = seq(1, 372, by = 31) %>% as_date)\ndata <- data %>% \n  mutate(new = abs(rnorm(12, 100, 10)) %>% round(0)) %>% \n  mutate(churn = abs(rnorm(12, 50, 30)) %>% round(0)) %>% \n  mutate(net = new - churn)  %>% \n  mutate(eop = cumsum(net)) %>% \n  select(-net)\n\ndata\n\n         date new churn eop\n1  1970-01-02  94    62  32\n2  1970-02-02  98    53  77\n3  1970-03-05 116    33 160\n4  1970-04-05 101   104 157\n5  1970-05-06 101    65 193\n6  1970-06-06 117     9 301\n7  1970-07-07 105    71 335\n8  1970-08-07  87    36 386\n9  1970-09-07  93    18 461\n10 1970-10-08  96    43 514\n11 1970-11-08 112    19 607\n12 1970-12-09 104    28 683\n\n\nTrong ví dụ này, dữ liệu được tạo ngẫu nhiên sao cho số lượng active user cuối kỳ (eop - end of period) bằng với số cuối kỳ trước, thêm số lượng mới và trừ đi lượng khách hàng rời bỏ (churn).\nĐể tạo waterfall chart, ta có thể sử dụng geom_segment trong ggplot2\n\n# Xác định độ rộng của segment\nstep <- 0.4*(max(data$date) - min(data$date))/(nrow(data) - 1)\n\n# Xác định ymax\ndata <- data %>% \n  mutate(ymax = eop + churn)\n\n# Xác định ymin\ndf <- data %>% \n  melt(id.vars = c(\"date\", \"eop\", \"ymax\")) %>% \n  mutate(ymin = ymax - value) %>% \n  rename(group = variable)\n\n# Xác định xmin và xmax\ndf <- df %>% \n  mutate(xmin = case_when(\n    group == \"new\" ~ date - step,\n    TRUE ~ date \n  )) %>% \n  mutate(xmax = case_when(\n    group == \"new\" ~ date,\n    TRUE ~ date + step\n  ))\n\n\n# Create waterfall chart\np1 <- df %>% \n  arrange(date) %>% \n  ggplot() +\n  geom_rect(aes(xmin = xmin,\n                xmax = xmax,\n                ymin = ymin,\n                ymax = ymax,\n                fill = group))\np1\n\n\n\n\nNhư vậy, ta đã tạo xong biểu đồ water-fall đơn giản. Ở bước tiếp theo, chúng ta cần điều chỉnh lại các thành phần cho biểu đồ.\n\n# Tạo dữ liệu cho biểu đồ đường\ndf2 <- df %>% select(date, eop) %>% distinct()\n\n# Điều chỉnh theme\np2 <- p1  + \n  geom_line(aes(date, eop), col = \"dodgerblue4\", size = 1) +\n  geom_point(aes(date, eop), col = \"dodgerblue4\", size = 2.5) +\n  geom_text(aes(date, eop, label = eop), vjust = 1.2, \n            hjust = -0.1) +\n  scale_fill_manual(values = c(\"grey60\", \"coral2\")) +\n  theme_minimal() +\n  theme(\n    axis.line = element_line(color = \"gray40\", size = 0.5),\n    legend.position = \"top\") +\n  scale_x_date(breaks = data$date,\n               date_labels = \"%b\") +\n  theme(panel.grid.minor.x = element_blank(),\n        legend.title = element_blank()) +\n  ggtitle(\"Overview of active users\") +\n  xlab(\"Date\") + \n  ylab(\"Number of active users\")\np2\n\n\n\n\nBước tiếp theo, ta cần xây dựng biểu đồ bar đơn giản để có thể đưa vào góc phần tư bên trái của biểu đồ vừa tạo.\n\np3 <- df %>% \n  mutate(value = case_when(\n    group == \"churn\" ~ -1 * value,\n    TRUE ~ value\n  )) %>% \n  ggplot(aes(date, value)) +\n  geom_bar(aes(fill = group), stat = \"identity\") +\n  scale_fill_manual(values = c(\"grey60\", \"coral2\")) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    axis.text.x = element_text(angle = 90)\n  ) +\n  scale_x_date(breaks = data$date,\n               date_labels = \"%b\") \np3\n\n\n\n\nCuối cùng, ta có thể nhóm hai biểu đồ trên với grid & gridExtra.\n\ngrid.newpage()\n\n# Xác định vị trí cho biểu đồ chính\nposition_1 <- viewport(width = 1, height = 1, \n                       x = 0.5, y = 0.5)  \n\n# Vị trí cho biểu đồ phụ\nposition_2 <- viewport(width = 0.35, height = 0.25, \n                       x = 0.25, y = 0.75)  \n\nprint(p2, vp = position_1)\nprint(p3, vp = position_2)"
  },
  {
    "objectID": "p13-01-casestudy-truc-quan-hoa.html#xây-dựng-biểu-đồ-lollipop-chart",
    "href": "p13-01-casestudy-truc-quan-hoa.html#xây-dựng-biểu-đồ-lollipop-chart",
    "title": "75  Trực quan hóa dữ liệu",
    "section": "75.3 Xây dựng biểu đồ lollipop chart",
    "text": "75.3 Xây dựng biểu đồ lollipop chart\nTrong trực quan hóa dữ liệu, lollipop chart tuy không phải là một trong những biểu đồ phổ biến nhưng lại rất hiệu quả khi muốn thể hiện sự dịch thay đổi của một chỉ số giữa hai điểm thời gian. Trong case study này, ta xây dựng biểu đồ lollipop chart với ggplot2.\n\n# Load library\nlibrary(tidyverse)\nlibrary(gapminder)\n\nĐể xây dựng biểu đồ, ta sử dụng dữ liệu gapminder từ tập dữ liệu gapminder. Mục tiêu là xây dựng biểu đồ thể hiện được sự thay đổi GDP/đầu người của các nước châu Âu trong năm 1952 so với năm 1977.\n\ndata <- gapminder %>% \n  filter(continent == \"Europe\") %>% \n  filter(year %in% c(1952, 1977)) %>% \n  select(country, year, gdpPercap) %>% \n  spread(year, gdpPercap) %>% \n  rename(y1952 = `1952`,\n         y1977 = `1977`)\ndata %>% head(10)\n\n# A tibble: 10 × 3\n   country                y1952  y1977\n   <fct>                  <dbl>  <dbl>\n 1 Albania                1601.  3533.\n 2 Austria                6137. 19749.\n 3 Belgium                8343. 19118.\n 4 Bosnia and Herzegovina  974.  3528.\n 5 Bulgaria               2444.  7612.\n 6 Croatia                3119. 11305.\n 7 Czech Republic         6876. 14800.\n 8 Denmark                9692. 20423.\n 9 Finland                6425. 15605.\n10 France                 7030. 18293.\n\n\nBiểu đồ lollipop có thể xây dựng dựa trên geom_point và geom_segment như sau.\nBiểu đồ đầu tiên\n\ndata %>% \n  ggplot(aes(x  = country)) +\n  # Tạo đường nối giữa hai điểm\n  geom_segment(aes(y = y1952, yend = y1977,\n               x = country, xend = country), size = 1,\n               col = \"grey50\") +\n  # Tạo điểm đầu\n  geom_point(aes(country, y1952, color = \"1952\"), size = 3.5) +\n  # Tạo điểm cuối\n  geom_point(aes(country, y1977, \n             color = \"1977\"), size = 3.5) +\n  coord_flip()\n\n\n\n\nTuy nhiên, với biểu đồ trên, ta thấy xuất hiện hai lỗi cơ bản sau:\n\nThứ nhất, thứ tự các các quan sát đang để dạng mặc định. Do đó, kết quả trực quan hóa chỉ mang tính thông tin mà chưa có yêu tố kể chuyện (story telling). Ta có thể giải quyết bằng cách sắp xếp lại factor theo thứ tự từ thấp đến cao.\nThứ hai, biểu đồ thể hiện theo chiều ngang. Do đó, các thông tin không cần thiết có thể được loại bỏ để biểu đồ gọn gàng và mạch lạc hơn.\n\nTa có thể chỉnh lại biểu đồ như sau.\n\n# Tinh chỉnh theme cho biểu đồ\nmy_theme <- function(...) {\n  theme_bw() + \n    theme(plot.background = element_rect(fill = \"white\")) + \n    theme(panel.grid.minor = element_blank()) + \n    theme(panel.grid.major.y = element_blank()) + \n    theme(panel.grid.major.x = element_line()) + \n    theme(axis.ticks = element_blank()) + \n    theme(panel.border = element_blank()) + \n    theme(text = element_text(size = 13, color = \"black\")) + \n    theme(plot.subtitle = element_text(color = \"gray20\", size = 10, face = \"italic\")) + \n    theme(legend.title = element_text(size = 10, color = \"gray20\")) + \n    theme(legend.position = \"top\")\n} \n\n# Tạo biểu đồ mới\np1 <- data %>% \n  mutate(country = fct_reorder(country, y1977)) %>% \n  ggplot(aes(x  = country)) +\n  geom_segment(aes(y = y1952, yend = y1977,\n               x = country, xend = country), size = 1,\n               col = \"grey50\",\n               alpha = 0.7) +\n  geom_point(aes(country, y1952, color = \"1952\"), \n             size = 3.5, alpha = 0.7) +\n  geom_point(aes(country, y1977, color = \"1977\"), \n             size = 3.5, alpha = 0.7) +\n  coord_flip() +\n  my_theme() +\n  scale_y_continuous(breaks = seq(0, 30000, by = 5000), \n                     limits = c(800, 27000)) +\n  scale_color_manual(\n    name = \"Year\",\n    labels = c(\"1952\", \"1977\"),\n    values = c(\"darkblue\", \"darkred\")\n  ) +\n  labs(x = NULL, y = NULL,\n       title = \"An example of lollipop chart\",\n       subtitle = \"Changes of GDP per capita in Europe\",\n       caption = \"Created by RAnalytics.vn\")\np1\n\n\n\n\nBonus: Để nhấn mạnh hơn sự thay đổi của GDP per capita, ta có thể vẽ thêm các đường nối các điểm trong biểu đồ như sau.\n\np2 <- p1 + geom_line(aes(as.numeric(country), y1977), \n               col = \"darkred\")\np2"
  },
  {
    "objectID": "p02-02-bien-doi-du-lieu-dplyr.html#các-kỹ-thuật-nâng-cao-trong-dplyr",
    "href": "p02-02-bien-doi-du-lieu-dplyr.html#các-kỹ-thuật-nâng-cao-trong-dplyr",
    "title": "4  Ngữ pháp của biến đổi dữ liệu với DPLYR",
    "section": "4.4 Các kỹ thuật nâng cao trong dplyr",
    "text": "4.4 Các kỹ thuật nâng cao trong dplyr\nBên cạnh các nhóm hàm cơ bản đã trình bày ở phần trên, dplyr còn có một số hàm nâng cao khác đặc biệt hữu dụng trong quá trình biến đổi, tổng hợp dữ liệu, bao gồm case_when, mutate_at & summarise_at\n\n4.4.1 Điều kiện phân nhóm với case_when\nTrong quá trình phân tích và xử lý dữ liệu, chúng ta thường phải tạo thêm các trường mới hoặc tính toán dữ liệu dựa vào từng điều kiện khác nhau để đưa ra giá trị của trường hoặc cách tính cho dữ liệu. Ví dụ, khi ta muốn tính thưởng cho KH thì sẽ phải dùng nhiều công thức khác nhau như KH thuộc VIP sẽ nhân 1 tỷ lệ, KH thuộc nhóm trung bình sẽ có 1 tỷ lệ khác, hay KH thông thường thì sẽ 1 tỷ lệ khác….\nTrong dplyr, hàm case_when() xử lý các trường hợp trên rất nhanh chóng.\n\ndata %&gt;% mutate(new_var = case_when(\n                   condition_1 ~ \"value_1\",\n                   condition_2 ~ \"value_2\",...,\n                   TRUE ~ \"value_n\"\n                 ))\n\nTa xem ví dụ sau:\n\ndf &lt;- data.frame(number = 1:10) \n\ndf %&gt;% mutate(nhom = case_when(\n  number &lt;= 5 ~ \"nhom_1\", # nhóm 1: số từ 1 đến 5\n  number &gt; 5 & number &lt;= 8 ~ \"nhom_2\", # nhóm 2: số từ 6 đến 8\n  TRUE ~ \"nhom_3\" # các số còn lại\n  ))\n\n   number   nhom\n1       1 nhom_1\n2       2 nhom_1\n3       3 nhom_1\n4       4 nhom_1\n5       5 nhom_1\n6       6 nhom_2\n7       7 nhom_2\n8       8 nhom_2\n9       9 nhom_3\n10     10 nhom_3\n\n\n\n\n4.4.2 Tạo thêm biến mới theo điều kiện với mutate_if & mutate_at\nKhi phân tích, ta có thể tạo thêm biến mới khi các biến trong dataframe thỏa mãn điều kiện nào đó.\n\ndata %&gt;% mutate_if(condition, function)\n\n\ndf &lt;- data.frame(\n  id = 1:5,\n  gender = c(\"F\", \"M\", \"M\", \"F\", \"F\"),\n  income = c(4,5,3,6,7)\n)\ndf %&gt;% summary\n\n       id       gender              income \n Min.   :1   Length:5           Min.   :3  \n 1st Qu.:2   Class :character   1st Qu.:4  \n Median :3   Mode  :character   Median :5  \n Mean   :3                      Mean   :5  \n 3rd Qu.:4                      3rd Qu.:6  \n Max.   :5                      Max.   :7  \n\n# Biến đổi các biến factor thành character\ndf %&gt;% mutate_if(is.factor, \n                 as.character) %&gt;% \n  summary\n\n       id       gender              income \n Min.   :1   Length:5           Min.   :3  \n 1st Qu.:2   Class :character   1st Qu.:4  \n Median :3   Mode  :character   Median :5  \n Mean   :3                      Mean   :5  \n 3rd Qu.:4                      3rd Qu.:6  \n Max.   :5                      Max.   :7  \n\n\nNgoài ra, ta có thể tự tạo các hàm mới và áp dụng với mutate_if. Xem ví dụ sau.\n\nmy_func &lt;- function(x){x*100}\n# Nhân các biến numeric lên 100 lần\ndf %&gt;% \n  mutate_if(is.numeric, my_func)\n\n   id gender income\n1 100      F    400\n2 200      M    500\n3 300      M    300\n4 400      F    600\n5 500      F    700\n\n\nĐối với mutate_at, ta cũng có thể thực hiện tương tự. Cấu trúc tổng quát của mutate_at như sau.\n\ndata %&gt;% \n  mutate_at(vars(var1, var2, ...), function())\n\n\ndf \n\n  id gender income\n1  1      F      4\n2  2      M      5\n3  3      M      3\n4  4      F      6\n5  5      F      7\n\n# Nhân biến income lên 100 lần\ndf %&gt;% mutate_at(vars(income), my_func)\n\n  id gender income\n1  1      F    400\n2  2      M    500\n3  3      M    300\n4  4      F    600\n5  5      F    700\n\n\n\n\n4.4.3 Tổng hợp dữ liệu theo điều kiện với summarise_at và summarise_if\nTương tự như mutate_at và mutate_if, ta có thể tổng hợp nhanh dữ liệu theo điều kiện.\nCấu trúc tổng quát của summarise_at\n\ndata %&gt;% \n     group_by(var) (không bắt buộc)\n     summarise_at(vars(variables), funs(functions))\n\nXem ví dụ sau\n\nmtcars %&gt;% \n  group_by(am) %&gt;% \n  summarise_at(vars(mpg, disp),\n               funs(mean, max, median))\n\n# A tibble: 2 × 7\n     am mpg_mean disp_mean mpg_max disp_max mpg_median disp_median\n  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1     0     17.1      290.    24.4      472       17.3        276.\n2     1     24.4      144.    33.9      351       22.8        120.\n\n\nTương tự, ta có cấu trúc tổng quát của summarise_if\n\ndata %&gt;% \n   group_by(var) (không bắt buộc)\n   summarise_if(condition, funs(functions))\n\n\niris %&gt;% \n  select(Species, Sepal.Length, Sepal.Width) %&gt;% \n  group_by(Species) %&gt;% \n  summarise_if(is.numeric,\n               funs(mean, median))\n\n# A tibble: 3 × 5\n  Species    Sepal.Length_mean Sepal.Width_mean Sepal.Length_median\n  &lt;fct&gt;                  &lt;dbl&gt;            &lt;dbl&gt;               &lt;dbl&gt;\n1 setosa                  5.01             3.43                 5  \n2 versicolor              5.94             2.77                 5.9\n3 virginica               6.59             2.97                 6.5\n# ℹ 1 more variable: Sepal.Width_median &lt;dbl&gt;\n\n\n\n\n4.4.4 Tổng hợp dữ liệu theo điều kiện\nBên cạnh việc tổng hợp dữ liệu thông thường, ta có thể tổng hợp theo các điều kiện phức tạp như sau.\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  summarise(x1 = mean(Sepal.Length[Petal.Width &gt;= 1.5]),\n            x2 = mean(Sepal.Width[Petal.Width &lt;= 1.9]),\n            # count n by length\n            n1 = length(Sepal.Width[Petal.Width &lt;= 1.9]))\n\n# A tibble: 3 × 4\n  Species        x1    x2    n1\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 setosa     NaN     3.43    50\n2 versicolor   6.19  2.77    50\n3 virginica    6.60  2.79    21\n\n\nTrong ví dụ trên, ta tổng hợp dữ liệu như sau:\n\nTính x1 là giá trị trung bình Sepal.Length với các quan sát có Petal.Width &gt;= 1.5\nTính x2 là giá tri trung bình Sepal.Width với các quan sát có Petal.Width &lt;= 1.9\nTính n1 là số lượng quan sát có Petal.Width &lt;= 1.9\n\n\n\n4.4.5 Tính toán theo hàng\nTrong quá trình tính toán dữ liệu, đôi khi ta phải tính toán theo hàng, không phải côt. Khi đó, cần chuyển cấu trúc theo dạng `rowwise``\n\nlibrary(dplyr)\ndf &lt;- data.frame(x = 1:2, y = 3:4, z = 5:6)\n# Đẻ thông thường không hoạt động\ndf %&gt;% \n  mutate(sum = sum(x, y, z))\n\n  x y z sum\n1 1 3 5  21\n2 2 4 6  21\n\n# Sử dụng rowwise\ndf %&gt;% \n  rowwise() %&gt;% \n  mutate(sum = sum(x, y, z),\n         mean = mean(x:z))\n\n# A tibble: 2 × 5\n# Rowwise: \n      x     y     z   sum  mean\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     3     5     9     3\n2     2     4     6    12     4"
  },
  {
    "objectID": "p02-06-lap-trinh-ham.html#đơn-giản-hóa-hàm-với",
    "href": "p02-06-lap-trinh-ham.html#đơn-giản-hóa-hàm-với",
    "title": "8  Lập trình hàm",
    "section": "8.7 Đơn giản hóa hàm với {{}}",
    "text": "8.7 Đơn giản hóa hàm với {{}}\nVới tidyverse ở các phiên bản mới (sau 2020) đã cho phép đơn giản hóa quá trình viết hàm bằng cách sử dụng {{}} thay thế cho enquo và !!.\nXem ví dụ sau\n\nlibrary(dplyr)\nmy_func1 &lt;- function(data, x, y){\n    result &lt;- data %&gt;% \n        group_by({{y}}) %&gt;% \n        summarise(\n            avg = mean({{x}}),\n            q50 = quantile({{x}}, 0.5))\n    return(result)\n}\nmy_func1(mtcars, mpg, am)\n\n# A tibble: 2 × 3\n     am   avg   q50\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  17.1  17.3\n2     1  24.4  22.8\n\n\n\nTrường hợp nhiều biến\n\n\nmy_func2 &lt;- function(data, x, ...){\n    result &lt;- data %&gt;% \n        group_by(...) %&gt;% \n        summarise(\n            avg = mean({{x}}),\n            q50 = quantile({{x}}, 0.5))\n    return(result)\n}\nmy_func2(mtcars, mpg, am, vs)\n\n# A tibble: 4 × 4\n# Groups:   am [2]\n     am    vs   avg   q50\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0  15.0  15.2\n2     0     1  20.7  21.4\n3     1     0  19.8  20.4\n4     1     1  28.4  30.4\n\n\n\nTrường hợp rename với tên biến là một thành phần trong biến cũ, vẫn phải dùng kết hợp := và quo_name\n\n\nmy_func3 &lt;- function(data, x){\n  new_name &lt;- paste0(quo_name(quo({{x}})), \"_new\")\n  data %&gt;% \n    mutate(!!new_name := {{x}})\n}\nmy_func3(mtcars %&gt;% select(mpg), mpg) %&gt;% head\n\n                   mpg mpg_new\nMazda RX4         21.0    21.0\nMazda RX4 Wag     21.0    21.0\nDatsun 710        22.8    22.8\nHornet 4 Drive    21.4    21.4\nHornet Sportabout 18.7    18.7\nValiant           18.1    18.1"
  },
  {
    "objectID": "p03-75-mlops-vetiver.html#lưu-trữ-object-với-pins",
    "href": "p03-75-mlops-vetiver.html#lưu-trữ-object-với-pins",
    "title": "35  MLOPs với vetiver",
    "section": "35.1 Lưu trữ object với pins",
    "text": "35.1 Lưu trữ object với pins\nÝ tưởng của pins rất đơn giản, pins sẽ có 3 nhóm câu lệnh đơn giản:\n\nboard_XXX: Tạo một board, là nơi chưa các object sẽ được lưu trữ. Trong các tổ chức sử dụng mạng LAN là nơi chia sẻ dữ liệu, có thể dùng board_folder.\npin_write: Cho phép write/save một object vào board để có thể chia sẻ\npin_read: Cho phép đọc một object từ một board nhất định.\n\nXem ví dụ về workflow dưới đây giữa 2 người phân tích A & B\n\nVới người A\n\nlibrary(pins)\n# Step 0: Tạo board - cho phép lưu trữ version các object\nboard &lt;- board_folder(\"./board\", version = T)\n\n# Step 1: Lưu object mtcars - version 1\nboard %&gt;% \n  pin_write(head(mtcars), \"mtcars\")\n\n# Step 2: Lưu object mtcars - version 2\nboard %&gt;% \n  pin_write(tail(mtcars), \"mtcars\")\n\n# Tìm kiếm version của mtcars\nboard %&gt;% \n  pin_versions(\"mtcars\")\n\n\nVới người B:\n\nlibrary(pins)\n# Step 0: Tạo kết nối đến board đã có sẵn\nboard &lt;- board_folder(\"./board\", version = T)\n\n# Step 1: Đọc object mtcars đã có sẵn\nboard %&gt;% \n  pin_read(\"mtcars\", version = \"xxx\")"
  },
  {
    "objectID": "p03-75-mlops-vetiver.html#mlops-với-vetiver",
    "href": "p03-75-mlops-vetiver.html#mlops-với-vetiver",
    "title": "35  MLOPs với vetiver",
    "section": "35.2 MLOPS với vetiver",
    "text": "35.2 MLOPS với vetiver\nVới MLOPS, vetiver sẽ cho phép thực hiện:\n\nLưu trữ & versioning mô hình\nDeploy model dạng API từ pin board\nKhởi tạo docker file phục vụ deploy API trên môi trường production với docker"
  },
  {
    "objectID": "p03-75-mlops-vetiver.html#tạo-mô-hình-đơn-giản",
    "href": "p03-75-mlops-vetiver.html#tạo-mô-hình-đơn-giản",
    "title": "35  MLOPs với vetiver",
    "section": "35.3 Tạo mô hình đơn giản",
    "text": "35.3 Tạo mô hình đơn giản\nXây dựng & lưu trữ 2 mô hình mtcars\n\nlibrary(tidymodels)\nlibrary(vetiver)\nlibrary(tidyverse)\nlibrary(pins)\n\n# Step 0: Tạo board (tạo board_temp cho notebook - thực tế cần tạo board_folder)\nmodel_board &lt;- board_temp(versioned = TRUE)\n\n# Step 1: Tạo & lưu trữ mô hình LM\ncar_mod &lt;- workflow(mpg ~ ., linear_reg()) %&gt;%\n  fit(mtcars)\n\nv &lt;- vetiver_model(car_mod, \"cars_mpg\")\n\nmodel_board %&gt;% vetiver_pin_write(v)\n\n# Các meta data có thể lấy ra từ list của v\n\n# Step 2: Xây dựng mô hình mới với vetiver\ncar_mod &lt;- workflow(mpg ~ ., decision_tree(mode = \"regression\")) %&gt;%\n  fit(mtcars)\n\nv &lt;- vetiver_model(car_mod, \"cars_mpg\")\n\nmodel_board %&gt;% vetiver_pin_write(v)\n\n# Step 3: Kiểm tra các version\nmodel_board %&gt;% pin_versions(\"cars_mpg\")\n\n\nTạo ra API\n\n\nlibrary(plumber)\nv &lt;- model_board %&gt;% vetiver_pin_read(\"cars_mpg\")\n\n# Model API từ plumber route\npr() %&gt;%\n  vetiver_api(v)\n\n\nRun API với port 8080\n\n\npr() %&gt;%\n  vetiver_api(v) %&gt;% \n  pr_run(port = 8080)"
  },
  {
    "objectID": "p03-75-mlops-vetiver.html#tạo-docker-file",
    "href": "p03-75-mlops-vetiver.html#tạo-docker-file",
    "title": "35  MLOPs với vetiver",
    "section": "35.4 Tạo docker file",
    "text": "35.4 Tạo docker file\nvetiver hỗ trợ tạo nhanh docker file, bao gồm 3 cấu phần sau:\n\nvetiver_renv.lock: Chứa các dependency của môi trường R\nplumber.R: Script đọc và serve model từ pin board\nDockerfile: File docker, cho phép đọc các dependency và tạo ra Docker Image với Docker Compose\n\n\nvetiver_prepare_docker(model_board, \"cars_mpg\")"
  },
  {
    "objectID": "p03-75-mlops-vetiver.html#tài-liệu-tham-khảo",
    "href": "p03-75-mlops-vetiver.html#tài-liệu-tham-khảo",
    "title": "35  MLOPs với vetiver",
    "section": "35.5 Tài liệu tham khảo",
    "text": "35.5 Tài liệu tham khảo\n\nhttps://vetiver.rstudio.com\nhttps://pins.rstudio.com"
  },
  {
    "objectID": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#quarto",
    "href": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#quarto",
    "title": "3  Xây dựng báo cáo với Rmarkdown",
    "section": "3.3 Quarto",
    "text": "3.3 Quarto\nquarto là bước phát triển tiếp theo của rmarkdown, cho phép sử dụng nhiều engine khác nhau phân tích số liệu trên cùng một tài liệu. Quarto dùng tương tự như rmarkdown nhưng có thể lựa chọn python, sql hay R khi chạy code và cho phép có các phần tùy chỉnh về layout linh hoạt & dễ dàng hơn rất nhiều so với rmarkdown.\nĐể chuyển đổi từ rmarkdown sang quarto (.qmd), ta chỉ việc đổi tên định dạng từ .rmd sang .qmd\nĐối với các file mới, ta có thể làm như sau: File &gt;&gt; New File &gt;&gt; Quarto Document\n\n§\n\nCác metadata hay dùng khi khai báo quarto document\n---\ntitle: \"Untitled\"\ndescription: \"Single IDE for Python and R\"\nauthor: \"Anh Hoang Duc\"\nformat:\n  html:\n    mainfont: Cambria\n    embed-fonts: true\n    toc: true\n    toc_number: yes\n    toc-location: left\n    number-sections: true\n    toc-title: \"Mục lục\"\n    fontcolor: \"black\"\n    code-fold: true\n    code-tools: true\n    code-line-numbers: true\nself-contained: true\ntitle-block-banner: \"#27445C\"\njupyter: python3\n---\n\n3.3.1 Các tips quarto hay dùng\n\n3.3.1.1 Callout\nCho phép ghi chú, đánh dấu các nội dung quan trọng.\n\nCodeKết quả\n\n\n:::{.callout-warning}\n## Lưu ý\nĐây là nội dung quan trọng\n:::\n\n\n\n\n\n\n\n\nLưu ý\n\n\n\nĐây là nội dung quan trọng\n\n\n\n\n\n\n§\n\n\n\n3.3.1.2 Code block\nTương tự như rmarkdown, quarto cho phép thực hiện các option như asis = T, echo = F ở code block. Bên cạnh đó, quarto cho phép thực hiện các option bằng cách điền các comment của code như sau\n```{r}\n#| echo: false\nhead(mtcars)\n```\n\n\n3.3.1.3 Đặt nhiều biểu đồ cạnh nhau\nĐây là tính năng rất hay cho phép vẽ full chiều ngang của biểu đồ. Ta thực hiện như sau\n:::{layout-ncol=2 .column-page}\n```{r}\n#| echo: false\nboxplot(iris$Sepal.Length, col = \"darkblue\")\n```\n```{r}\n#| echo: false\nboxplot(iris$Sepal.Length, col = \"darkred\")\n```\n:::\nKết quả hiển thị như sau\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.1.4 Tabset\nTabset cho phép tạo các tab chứa các nội dung khác nhau\n:::{.panel-tabset}\n## Tab 1\n## Tab 2\n## Tab 3\n:::\nKết quả hiển thị như sau\n\nTab 1Tab 2Tab 3"
  },
  {
    "objectID": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#kết-quả-1",
    "href": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#kết-quả-1",
    "title": "3  Xây dựng báo cáo với Rmarkdown",
    "section": "3.5 Kết quả",
    "text": "3.5 Kết quả"
  },
  {
    "objectID": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#code-1",
    "href": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#code-1",
    "title": "3  Xây dựng báo cáo với Rmarkdown",
    "section": "3.4 Code",
    "text": "3.4 Code\n:::{layout-ncol=2 .column-page}\n```{r}\n#| echo: false\nboxplot(iris$Sepal.Length, col = \"darkblue\")\n```\n```{r}\n#| echo: false\nboxplot(iris$Sepal.Length, col = \"darkred\")\n```\n:::"
  },
  {
    "objectID": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#tab-1",
    "href": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#tab-1",
    "title": "3  Xây dựng báo cáo với Rmarkdown",
    "section": "3.4 Tab 1",
    "text": "3.4 Tab 1"
  },
  {
    "objectID": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#tab-2",
    "href": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#tab-2",
    "title": "3  Xây dựng báo cáo với Rmarkdown",
    "section": "3.5 Tab 2",
    "text": "3.5 Tab 2"
  },
  {
    "objectID": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#tab-3",
    "href": "p02-70-xay-dung-bao-cao-voi-rmarkdown.html#tab-3",
    "title": "3  Xây dựng báo cáo với Rmarkdown",
    "section": "3.6 Tab 3",
    "text": "3.6 Tab 3"
  },
  {
    "objectID": "index.html#lưu-ý",
    "href": "index.html#lưu-ý",
    "title": "Khoa học dữ liệu với R",
    "section": "1.5 Lưu ý",
    "text": "1.5 Lưu ý\nĐối với các bạn muốn tập trung phát triển Insights, nên tập trung đọc các chương sau:\nCấp độ 1 - Phân tích khám phá - bắt buộc phải nắm vững (chương 1-7)\n\nGiới thiệu cơ bản về R (chương 1)\nXây dựng báo cáo với Rmarkdown\nBiến đổi dữ liệu với DPLYR\nPhân rã & xoay chiều dữ liệu\nNgữ pháp biểu đồ với GGPLOT2\nCác chỉ số thống kê cơ bản\n\nCấp độ 2 - Phân tích khám phá - Kỹ thuật nâng cao - nhưng vẫn cần bắt buộc phải nắm vững (chương 8-10)\n\nXử lý dữ liệu text\nLập trình với purrr\nLập trình hàm\n\nCấp độ 3 - cơ bản về các thuật toán & nguyên lý thường sử dụng:\n\nNguyên lý dự báo & cơ bản học máy (chương 18)\nMô hình hồi quy tuyến tính (chương 19)\nMô hình logistics (chương 20)\nCây quyết định & họ cây quyết định (chương 21-22)\nPhân cụm với kmeans (chương 38)\nBasket Analysis (chương 39)\n\nNâng cao\n\nSurvival Analysis (chương 54)\nQuantile Regression (chương 59)\nChuỗi thời gian (chương 41)\n\nCác chương và nội dung khác dùng để tham khảo đọc thêm nếu cần. Tuy nhiên, với việc nắm vững các kỹ thuật trên đã giải quyết 95%+ các bài toàn thực tế trên các lĩnh vực khác nhau và giúp các bạn có thể dịch chuyển nhanh chóng sang các tầng insights nâng cao"
  },
  {
    "objectID": "p02-02-bien-doi-du-lieu-dplyr.html#lưu-ý",
    "href": "p02-02-bien-doi-du-lieu-dplyr.html#lưu-ý",
    "title": "4  Ngữ pháp của biến đổi dữ liệu với DPLYR",
    "section": "4.2 Lưu ý",
    "text": "4.2 Lưu ý\nTừ phiên bản R mới (4.x), đã có toán tử pipe mặc định |&gt;"
  },
  {
    "objectID": "p02-02-bien-doi-du-lieu-dplyr.html#các-vấn-đề-khác",
    "href": "p02-02-bien-doi-du-lieu-dplyr.html#các-vấn-đề-khác",
    "title": "4  Ngữ pháp của biến đổi dữ liệu với DPLYR",
    "section": "4.5 Các vấn đề khác",
    "text": "4.5 Các vấn đề khác\n\n4.5.1 Chuyển đổi giữa dplyr & sql\nTrong quá trình học DPLYR hoặc SQL, có thể sử dụng các packages sau để chuyển đổi qua lại giữa các ngôn ngữ:\n\ntidyquery để chuyển đổi từ SQL sang dplyr\n\ndbplyr để chuyển đổi từ dplyr sang SQL\n\n\nChuyển đổi dplyr sang SQL\n\nlibrary(dplyr)\nlibrary(dbplyr)\n\n# Copy dataframe sang SQLite\niris_df &lt;- iris %&gt;% memdb_frame()\n\niris_df %&gt;% \n  group_by(Species) %&gt;% \n  summarise(no = n(),\n            mean_sepal = mean(Sepal.Length)) %&gt;% \n  show_query()\n\n&lt;SQL&gt;\nSELECT `Species`, COUNT(*) AS `no`, AVG(`Sepal.Length`) AS `mean_sepal`\nFROM `dbplyr_001`\nGROUP BY `Species`\n\n\n\nChuyển đổi từ code SQL sang dplyr\n\nlibrary(tidyquery)\n\ncode &lt;- \"select avg(`Sepal.Length`) as avg_sepal_length\n        , count(*) as NO\n        from iris\n        group by `Species`\"\n\ncode %&gt;% query\n\n# A tibble: 3 × 2\n  avg_sepal_length    NO\n             &lt;dbl&gt; &lt;int&gt;\n1             5.01    50\n2             5.94    50\n3             6.59    50\n\ncode %&gt;% show_dplyr()\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(mean(Sepal.Length, na.rm = TRUE), dplyr::n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(avg_sepal_length = `mean(Sepal.Length, na.rm = TRUE)`, NO = `dplyr::n()`) %&gt;%\n  select(avg_sepal_length, NO)\n\n\n\n\n4.5.2 Kết nối dplyr với database\nKhi làm việc thực tế, ta cần load data từ các cơ sở dữ liệu ra các phần mềm phân tích hoặc biến đổi phân tích & ghi kết quả ngược lại vào database.\n\n4.5.2.1 Cách 1: Sử dụng RODBC\nCách này khá đơn giản tuy nhiên chỉ hỗ trợ các câu lệnh truy vấn và lưu cả bảng và database mà không cho phép thực hiện các câu lệnh phức tạp hơn như Update, Delete. Cách thức thực hiện như sau.\n\nBước 1: Vào Control Panel &gt;&gt; Administrative Tool &gt;&gt; ODBC Data Source &gt;&gt; Add &gt;&gt; Chọn Server &gt;&gt; Lựa chọn login vào Server &gt;&gt; Test Data &gt;&gt; Test Data Source\nBước 2: Khởi tạo connection với SQL từ R\n\n\n#Cài đặt connection với SQLServer\nlibrary(RODBC)\nch &lt;- odbcConnect(\"sql\", uid = \"sa\", pwd = \"123\")\n#Kết nối SQL với R\ndf &lt;- sqlQuery(ch, \"\n         SELECT top 100 * from learnsql.dbo.account\") \n\n\n\n4.5.2.2 Cách 2: Sử dụng DBI, odbc\nCách sử dụng với DBI và odbc ưu việt hơn so với sử dụng RODBC. Cách tiếp cận này thậm chí cho phép translate từ dplyr sang SQL\n\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(DBI)\nlibrary(odbc)\nsessionInfo()\n# Step 1: Tạo connection với odbc\n\ncon &lt;- dbConnect(odbc::odbc(),\n                 Driver    = \"SQL Server\", \n                 Server    = \"localhost\",\n                 Database  = \"learningsql\",\n                 UID       = \"sa\",\n                 PWD       = 123456,\n                 Port      = 1433)\n\n#Xem danh sách các bảng trong DB\n\ndbListTables(con, schema_name = \"dbo\")\n\n#Xem danh sách các cột trong 1 bảng\n\ndbListFields(con, \"tbl_customer_dim\")\n\n#Preview 1 bảng\ncon %&gt;% tbl(\"tbl_customer_dim\")\n\n#Đếm số dòng với tally\n\ncon %&gt;% tbl(\"tbl_customer_dim\") %&gt;% tally\n\n#Convert sang SQL Query\n\ncon %&gt;% tbl(\"tbl_customer_dim\") %&gt;% tally %&gt;% show_query()\n\ncon %&gt;% tbl(\"tbl_customer_dim\") %&gt;%\n  group_by(SEGMENT) %&gt;% \n  summarise(n = n()) %&gt;% \n  show_query()\n\ncon %&gt;% tbl(\"tbl_customer_dim\") %&gt;%\n  group_by(SEGMENT) %&gt;% \n  mutate(n = dense_rank(Volumn)) %&gt;% \n  show_query()\n\ntbl(con, \"tbl_customer_dim\") %&gt;% \n  left_join(tbl(con, \"DimDate\"), by = c(\"Date\" = \"DateKey\"))  -&gt; df\n \n# Write dataframe vào bảng - là df2_database\ndbWriteTable(con, \"df2_database\", df2)\n\ndbSendQuery(con, \"select top 10 * from tbl_customer_dim\") %&gt;% \n  dbFetch()\n\ndbGetQuery(con, \"select top 10 * from tbl_customer_dim\")  %&gt;% \n  tbl\n\nCác câu lệnh biến đổi dữ liệu\n\ndbWriteTable(con, \"iris\", iris)\n\n\nquery &lt;- \"\nupdate analytics.dbo.iris\nset y = 5\n\ndelete from analytics.dbo.iris\nwhere cut = 'Ideal'\n\"\n\ndbSendQuery(con, query)\n\nLưu ý:\n\nKhi tạo string query trong, nên dùng dâu \"\"\nDâu '' trong SQL có thể dùng trong dấu \"\" mà không gặp vấn đề gì"
  },
  {
    "objectID": "p02-02-bien-doi-du-lieu-dplyr.html#tài-liệu-tham-khảo",
    "href": "p02-02-bien-doi-du-lieu-dplyr.html#tài-liệu-tham-khảo",
    "title": "4  Ngữ pháp của biến đổi dữ liệu với DPLYR",
    "section": "4.6 Tài liệu tham khảo",
    "text": "4.6 Tài liệu tham khảo\n\nhttps://dplyr.tidyverse.org/\nhttps://r4ds.had.co.nz/\nhttps://dbplyr.tidyverse.org/"
  }
]